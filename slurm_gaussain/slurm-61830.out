2018-06-15 19:35:26.036773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:35:26.037038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
2018-06-15 19:35:32.565916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
CHEKPOINT DIR: uniform
model has been loaded from mnist_classifier.pkl
---------
Variables: name (type shape) [size]
---------
Variable:0 (float32_ref 5x5x1x32) [800, bytes: 3200]
Variable_1:0 (float32_ref 32) [32, bytes: 128]
Variable_2:0 (float32_ref 5x5x32x64) [51200, bytes: 204800]
Variable_3:0 (float32_ref 64) [64, bytes: 256]
Variable_4:0 (float32_ref 3136x1024) [3211264, bytes: 12845056]
Variable_5:0 (float32_ref 1024) [1024, bytes: 4096]
Variable_6:0 (float32_ref 1024x10) [10240, bytes: 40960]
Variable_7:0 (float32_ref 10) [10, bytes: 40]
discriminator/d_conv1/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]
discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]
discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]
discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]
generator/g_fc1/Matrix:0 (float32_ref 74x1024) [75776, bytes: 303104]
generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]
generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]
generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]
classifier/c_fc1/Matrix:0 (float32_ref 1024x64) [65536, bytes: 262144]
classifier/c_fc1/bias:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/beta:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/gamma:0 (float32_ref 64) [64, bytes: 256]
classifier/c_fc2/Matrix:0 (float32_ref 64x12) [768, bytes: 3072]
classifier/c_fc2/bias:0 (float32_ref 12) [12, bytes: 48]
Total size of variables: 16552792
Total bytes of variables: 66211168
 [*] Reading checkpoints...
 [*] Success to read MultiModalInfoGAN.model-43721
 [*] Load SUCCESS


SAMPLES SIZE=4680,LABELS=299520


SAVED TRAINING SET generated_training_set_mnist_UniformSample_mu_0_sigma_0.15_czcc_czrc_rzcc_rzrc
 [*] Training finished!
 [*] Testing finished!
2018-06-15 19:36:21.663491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:36:21.663731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
model has been loaded from mnist_classifier.pkl
argmax:[7 7 7 ..., 7 7 7]
step 0: accuracy:0.0, confidence:0.9975712895393372, loss:14.716750144958496
Assinging:8
[   0   39    0    0    0    0    0 9961]
argmax:[5 5 5 ..., 5 5 5]
step 0: accuracy:0.0, confidence:0.9875743389129639, loss:18.34659767150879
Assinging:6
[   0    0    0    0    0 9883    0    0  117]
argmax:[9 9 9 ..., 9 9 9]
step 0: accuracy:0.9961000084877014, confidence:0.9937304854393005, loss:0.016177073121070862
Assinging:10
[   0   39    0    0    0    0    0    0    0 9961]
argmax:[1 1 1 ..., 1 1 1]
step 0: accuracy:0.0, confidence:0.9996072053909302, loss:14.714092254638672
Assinging:2
[    0 10000]
argmax:[3 3 3 ..., 3 3 3]
step 0: accuracy:0.0, confidence:0.9994745850563049, loss:11.704439163208008
Assinging:4
[    0     0     0 10000]
argmax:[6 6 6 ..., 6 6 6]
step 0: accuracy:0.0, confidence:0.9994109272956848, loss:22.491304397583008
Assinging:7
[    0     0     0     0     0     0 10000]
argmax:[2 2 2 ..., 2 2 2]
step 0: accuracy:0.0, confidence:0.9987232685089111, loss:16.472150802612305
Assinging:3
[    0     0 10000]
argmax:[8 8 8 ..., 8 8 8]
step 0: accuracy:0.0, confidence:0.9983187317848206, loss:15.355284690856934
Assinging:9
[    0     0     0     0     0     0     0     0 10000]
argmax:[0 0 0 ..., 0 0 0]
step 0: accuracy:0.0, confidence:0.9998655319213867, loss:17.982446670532227
Assinging:1
[10000]
argmax:[4 4 4 ..., 4 4 4]
step 0: accuracy:0.0, confidence:0.9926775097846985, loss:15.980181694030762
Assinging:5
[    0     0     0     0 10000]
2018-06-15 19:36:39.813218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:36:39.813458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
step 0: accuracy:0.10700000077486038, confidence:0.5953738689422607, loss:5.246886730194092
epoch0: step0/4680
step 0: accuracy:0.09200000017881393, confidence:1.0, loss:68.24635314941406
epoch0: step500/4680
step 0: accuracy:0.09099999815225601, confidence:1.0, loss:38.399253845214844
epoch0: step1000/4680
step 0: accuracy:0.10000000149011612, confidence:0.9999072551727295, loss:12.477225303649902
epoch0: step1500/4680
step 0: accuracy:0.1340000033378601, confidence:0.9999063611030579, loss:10.049019813537598
epoch0: step2000/4680
step 0: accuracy:0.07999999821186066, confidence:0.9958944320678711, loss:7.916233062744141
epoch0: step2500/4680
step 0: accuracy:0.11299999803304672, confidence:0.9958533048629761, loss:7.409372329711914
epoch0: step3000/4680
step 0: accuracy:0.1080000028014183, confidence:0.9990143775939941, loss:8.441883087158203
epoch0: step3500/4680
step 0: accuracy:0.10599999874830246, confidence:0.9989072680473328, loss:8.810905456542969
epoch0: step4000/4680
step 0: accuracy:0.08500000089406967, confidence:0.9991642236709595, loss:9.179821968078613
epoch0: step4500/4680
step 0: accuracy:0.08299999684095383, confidence:0.9993754029273987, loss:9.497682571411133
epoch1: step0/4680
step 500: accuracy:0.10899999737739563, confidence:0.8820627331733704, loss:4.021402835845947
epoch1: step500/4680
step 1000: accuracy:0.0820000022649765, confidence:0.9987305402755737, loss:9.642415046691895
epoch1: step1000/4680
step 1500: accuracy:0.09300000220537186, confidence:0.9589846134185791, loss:5.214296340942383
epoch1: step1500/4680
step 2000: accuracy:0.14000000059604645, confidence:0.9975953102111816, loss:7.054528713226318
epoch1: step2000/4680
step 2500: accuracy:0.09799999743700027, confidence:0.9993318915367126, loss:8.782187461853027
epoch1: step2500/4680
step 3000: accuracy:0.10300000011920929, confidence:0.9999168515205383, loss:11.034615516662598
epoch1: step3000/4680
step 3500: accuracy:0.10499999672174454, confidence:0.9993452429771423, loss:9.694761276245117
epoch1: step3500/4680
step 4000: accuracy:0.08500000089406967, confidence:0.9992614984512329, loss:9.539970397949219
epoch1: step4000/4680
step 4500: accuracy:0.09000000357627869, confidence:0.99886155128479, loss:8.899056434631348
epoch1: step4500/4680
step 0: accuracy:0.10100000351667404, confidence:0.9998247027397156, loss:10.496315002441406
epoch2: step0/4680
step 1000: accuracy:0.0949999988079071, confidence:0.9998001456260681, loss:9.859174728393555
epoch2: step500/4680
step 2000: accuracy:0.07599999755620956, confidence:0.9996500015258789, loss:10.547109603881836
epoch2: step1000/4680
step 3000: accuracy:0.0820000022649765, confidence:0.999971330165863, loss:14.028253555297852
epoch2: step1500/4680
step 4000: accuracy:0.11699999868869781, confidence:0.9986023902893066, loss:7.963041305541992
epoch2: step2000/4680
step 5000: accuracy:0.09799999743700027, confidence:0.9994860887527466, loss:9.102892875671387
epoch2: step2500/4680
step 6000: accuracy:0.08500000089406967, confidence:0.9983546137809753, loss:7.940229892730713
epoch2: step3000/4680
step 7000: accuracy:0.09399999678134918, confidence:0.9967811703681946, loss:7.156836986541748
epoch2: step3500/4680
step 8000: accuracy:0.0860000029206276, confidence:0.9985161423683167, loss:8.344752311706543
epoch2: step4000/4680
step 9000: accuracy:0.09700000286102295, confidence:0.9802653789520264, loss:5.915053844451904
epoch2: step4500/4680
step 0: accuracy:0.09799999743700027, confidence:0.9946879744529724, loss:7.259753704071045
epoch3: step0/4680
step 1500: accuracy:0.09399999678134918, confidence:0.7304838299751282, loss:3.153538703918457
epoch3: step500/4680
step 3000: accuracy:0.1940000057220459, confidence:0.868045449256897, loss:3.8071439266204834
epoch3: step1000/4680
step 4500: accuracy:0.10400000214576721, confidence:0.9947118759155273, loss:8.061826705932617
epoch3: step1500/4680
step 6000: accuracy:0.1289999932050705, confidence:0.9533617496490479, loss:4.738707065582275
epoch3: step2000/4680
step 7500: accuracy:0.09700000286102295, confidence:0.9840245246887207, loss:6.1399736404418945
epoch3: step2500/4680
step 9000: accuracy:0.10499999672174454, confidence:0.9515320062637329, loss:5.11879825592041
epoch3: step3000/4680
step 10500: accuracy:0.08699999749660492, confidence:0.992897093296051, loss:6.628134727478027
epoch3: step3500/4680
step 12000: accuracy:0.09200000017881393, confidence:0.9899981617927551, loss:6.457136154174805
epoch3: step4000/4680
step 13500: accuracy:0.10599999874830246, confidence:0.9588223695755005, loss:5.679443359375
epoch3: step4500/4680
step 0: accuracy:0.10000000149011612, confidence:0.9968295693397522, loss:8.150758743286133
epoch4: step0/4680
step 2000: accuracy:0.08299999684095383, confidence:0.9993022680282593, loss:10.296183586120605
epoch4: step500/4680
step 4000: accuracy:0.09799999743700027, confidence:0.9381736516952515, loss:5.084129810333252
epoch4: step1000/4680
step 6000: accuracy:0.1340000033378601, confidence:0.9527904987335205, loss:5.887012958526611
epoch4: step1500/4680
step 8000: accuracy:0.1080000028014183, confidence:0.9326475858688354, loss:4.769749641418457
epoch4: step2000/4680
step 10000: accuracy:0.08699999749660492, confidence:0.9940506219863892, loss:7.730650424957275
epoch4: step2500/4680
step 12000: accuracy:0.10499999672174454, confidence:0.9229711294174194, loss:5.120683670043945
epoch4: step3000/4680
step 14000: accuracy:0.08900000154972076, confidence:0.9905991554260254, loss:7.254880428314209
epoch4: step3500/4680
step 16000: accuracy:0.09300000220537186, confidence:0.9890161156654358, loss:6.8901591300964355
epoch4: step4000/4680
step 18000: accuracy:0.12700000405311584, confidence:0.8980391621589661, loss:4.835937023162842
epoch4: step4500/4680
step 0: accuracy:0.09600000083446503, confidence:0.9811593890190125, loss:6.762307167053223
epoch5: step0/4680
step 2500: accuracy:0.12200000137090683, confidence:0.906083345413208, loss:5.822692394256592
epoch5: step500/4680
step 5000: accuracy:0.17100000381469727, confidence:0.9100103378295898, loss:7.291775226593018
epoch5: step1000/4680
step 7500: accuracy:0.10000000149011612, confidence:0.9993389844894409, loss:11.540868759155273
epoch5: step1500/4680
step 10000: accuracy:0.12300000339746475, confidence:0.9912794232368469, loss:6.877577304840088
epoch5: step2000/4680
step 12500: accuracy:0.09099999815225601, confidence:0.9958671927452087, loss:8.908244132995605
epoch5: step2500/4680
step 15000: accuracy:0.10100000351667404, confidence:0.9545482993125916, loss:6.16321325302124
epoch5: step3000/4680
step 17500: accuracy:0.1340000033378601, confidence:0.9728883504867554, loss:6.115739345550537
epoch5: step3500/4680
step 20000: accuracy:0.09700000286102295, confidence:0.9893554449081421, loss:7.187959671020508
epoch5: step4000/4680
step 22500: accuracy:0.13300000131130219, confidence:0.8725932836532593, loss:4.678625106811523
epoch5: step4500/4680
step 0: accuracy:0.10199999809265137, confidence:0.9484326243400574, loss:6.177445888519287
epoch6: step0/4680
step 3000: accuracy:0.1589999943971634, confidence:0.9159752130508423, loss:6.125034332275391
epoch6: step500/4680
step 6000: accuracy:0.18400000035762787, confidence:0.9074545502662659, loss:5.14312219619751
epoch6: step1000/4680
step 9000: accuracy:0.164000004529953, confidence:0.9320434331893921, loss:5.52666711807251
epoch6: step1500/4680
step 12000: accuracy:0.16599999368190765, confidence:0.8464098572731018, loss:4.059341907501221
epoch6: step2000/4680
step 15000: accuracy:0.13300000131130219, confidence:0.9926022887229919, loss:8.842351913452148
epoch6: step2500/4680
step 18000: accuracy:0.15700000524520874, confidence:0.8819205164909363, loss:5.763479232788086
epoch6: step3000/4680
step 21000: accuracy:0.1420000046491623, confidence:0.9367649555206299, loss:7.0233917236328125
epoch6: step3500/4680
step 24000: accuracy:0.17599999904632568, confidence:0.9275921583175659, loss:5.270219326019287
epoch6: step4000/4680
step 27000: accuracy:0.20100000500679016, confidence:0.8420671224594116, loss:4.104950428009033
epoch6: step4500/4680
step 0: accuracy:0.1720000058412552, confidence:0.8700346350669861, loss:4.735752105712891
epoch7: step0/4680
step 3500: accuracy:0.2529999911785126, confidence:0.8332594037055969, loss:4.428450107574463
epoch7: step500/4680
step 7000: accuracy:0.20100000500679016, confidence:0.9347994327545166, loss:7.434208393096924
epoch7: step1000/4680
step 10500: accuracy:0.18400000035762787, confidence:0.8651205897331238, loss:5.487882614135742
epoch7: step1500/4680
step 14000: accuracy:0.19300000369548798, confidence:0.8011127710342407, loss:3.822694778442383
epoch7: step2000/4680
step 17500: accuracy:0.10599999874830246, confidence:0.9949105381965637, loss:7.852397441864014
epoch7: step2500/4680
step 21000: accuracy:0.18299999833106995, confidence:0.8536241054534912, loss:4.546210765838623
epoch7: step3000/4680
step 24500: accuracy:0.1860000044107437, confidence:0.9182398319244385, loss:4.981884956359863
epoch7: step3500/4680
step 28000: accuracy:0.11900000274181366, confidence:0.9150760173797607, loss:4.641768932342529
epoch7: step4000/4680
step 31500: accuracy:0.20399999618530273, confidence:0.8035513162612915, loss:3.2304863929748535
epoch7: step4500/4680
step 0: accuracy:0.16500000655651093, confidence:0.8310647010803223, loss:4.099374771118164
epoch8: step0/4680
step 4000: accuracy:0.25099998712539673, confidence:0.8183594942092896, loss:3.6159677505493164
epoch8: step500/4680
step 8000: accuracy:0.20000000298023224, confidence:0.9313912391662598, loss:5.360539436340332
epoch8: step1000/4680
step 12000: accuracy:0.17000000178813934, confidence:0.805593729019165, loss:4.319177627563477
epoch8: step1500/4680
step 16000: accuracy:0.27399998903274536, confidence:0.7852784991264343, loss:3.472323179244995
epoch8: step2000/4680
step 20000: accuracy:0.12200000137090683, confidence:0.9790754318237305, loss:7.742982387542725
epoch8: step2500/4680
step 24000: accuracy:0.15399999916553497, confidence:0.8920634984970093, loss:6.095121383666992
epoch8: step3000/4680
step 28000: accuracy:0.12600000202655792, confidence:0.9430870413780212, loss:5.566131114959717
epoch8: step3500/4680
step 32000: accuracy:0.15199999511241913, confidence:0.9077336192131042, loss:4.711964130401611
epoch8: step4000/4680
step 36000: accuracy:0.24300000071525574, confidence:0.8296560049057007, loss:3.4620187282562256
epoch8: step4500/4680
step 0: accuracy:0.21699999272823334, confidence:0.8293746709823608, loss:3.9171934127807617
epoch9: step0/4680
step 4500: accuracy:0.2370000034570694, confidence:0.8744292855262756, loss:5.269942283630371
epoch9: step500/4680
step 9000: accuracy:0.18700000643730164, confidence:0.9244266152381897, loss:7.275481224060059
epoch9: step1000/4680
step 13500: accuracy:0.2590000033378601, confidence:0.8095089793205261, loss:3.2666075229644775
epoch9: step1500/4680
step 18000: accuracy:0.34299999475479126, confidence:0.7147742509841919, loss:2.4509410858154297
epoch9: step2000/4680
step 22500: accuracy:0.11299999803304672, confidence:0.9733992218971252, loss:6.161079406738281
epoch9: step2500/4680
step 27000: accuracy:0.19300000369548798, confidence:0.8518299460411072, loss:4.246686935424805
epoch9: step3000/4680
step 31500: accuracy:0.18799999356269836, confidence:0.9138337969779968, loss:4.947703838348389
epoch9: step3500/4680
step 36000: accuracy:0.13600000739097595, confidence:0.9351418614387512, loss:4.799083232879639
epoch9: step4000/4680
step 40500: accuracy:0.2370000034570694, confidence:0.7946345210075378, loss:2.955159902572632
epoch9: step4500/4680
step 0: accuracy:0.18700000643730164, confidence:0.8460122346878052, loss:4.016690254211426
epoch10: step0/4680
step 5000: accuracy:0.2980000078678131, confidence:0.73115473985672, loss:2.240959644317627
epoch10: step500/4680
step 10000: accuracy:0.19099999964237213, confidence:0.8824819326400757, loss:4.635921955108643
epoch10: step1000/4680
step 15000: accuracy:0.24300000071525574, confidence:0.8190910220146179, loss:4.562674045562744
epoch10: step1500/4680
step 20000: accuracy:0.3490000069141388, confidence:0.7602919936180115, loss:2.6875455379486084
epoch10: step2000/4680
step 25000: accuracy:0.11100000143051147, confidence:0.9769749641418457, loss:6.6042609214782715
epoch10: step2500/4680
step 30000: accuracy:0.22100000083446503, confidence:0.8388199210166931, loss:4.255423069000244
epoch10: step3000/4680
step 35000: accuracy:0.17499999701976776, confidence:0.8976874947547913, loss:4.886112213134766
epoch10: step3500/4680
step 40000: accuracy:0.14900000393390656, confidence:0.9193772673606873, loss:4.712151527404785
epoch10: step4000/4680
step 45000: accuracy:0.25099998712539673, confidence:0.8008711934089661, loss:3.009817361831665
epoch10: step4500/4680
step 0: accuracy:0.23199999332427979, confidence:0.8494962453842163, loss:3.8566277027130127
epoch11: step0/4680
step 5500: accuracy:0.27399998903274536, confidence:0.8221158385276794, loss:3.7862207889556885
epoch11: step500/4680
step 11000: accuracy:0.18700000643730164, confidence:0.9333276152610779, loss:5.7050251960754395
epoch11: step1000/4680
step 16500: accuracy:0.23199999332427979, confidence:0.8079283833503723, loss:3.4100289344787598
epoch11: step1500/4680
step 22000: accuracy:0.35600000619888306, confidence:0.7342355847358704, loss:2.3859612941741943
epoch11: step2000/4680
step 27500: accuracy:0.13099999725818634, confidence:0.9553539156913757, loss:5.867526531219482
epoch11: step2500/4680
step 33000: accuracy:0.20800000429153442, confidence:0.851997971534729, loss:4.526984691619873
epoch11: step3000/4680
step 38500: accuracy:0.22300000488758087, confidence:0.8709205389022827, loss:4.03895902633667
epoch11: step3500/4680
step 44000: accuracy:0.11900000274181366, confidence:0.9379081130027771, loss:5.710951328277588
epoch11: step4000/4680
step 49500: accuracy:0.31200000643730164, confidence:0.8116645216941833, loss:2.841214656829834
epoch11: step4500/4680
step 0: accuracy:0.24799999594688416, confidence:0.8363175392150879, loss:3.655512809753418
epoch12: step0/4680
step 6000: accuracy:0.3179999887943268, confidence:0.7364747524261475, loss:2.287447929382324
epoch12: step500/4680
step 12000: accuracy:0.23399999737739563, confidence:0.9156994819641113, loss:4.731202602386475
epoch12: step1000/4680
step 18000: accuracy:0.24199999868869781, confidence:0.8410286903381348, loss:4.493431091308594
epoch12: step1500/4680
step 24000: accuracy:0.3160000145435333, confidence:0.7632803916931152, loss:2.8878729343414307
epoch12: step2000/4680
step 30000: accuracy:0.12099999934434891, confidence:0.9577726125717163, loss:5.9550604820251465
epoch12: step2500/4680
step 36000: accuracy:0.22699999809265137, confidence:0.8531206250190735, loss:4.156092166900635
epoch12: step3000/4680
step 42000: accuracy:0.19599999487400055, confidence:0.8726432919502258, loss:4.276806831359863
epoch12: step3500/4680
step 48000: accuracy:0.1589999943971634, confidence:0.9217086434364319, loss:4.4581685066223145
epoch12: step4000/4680
step 54000: accuracy:0.2800000011920929, confidence:0.8130591511726379, loss:2.938936948776245
epoch12: step4500/4680
step 0: accuracy:0.1679999977350235, confidence:0.8943350315093994, loss:4.698485851287842
epoch13: step0/4680
step 6500: accuracy:0.2770000100135803, confidence:0.8153876066207886, loss:3.1414616107940674
epoch13: step500/4680
step 13000: accuracy:0.19900000095367432, confidence:0.9003870487213135, loss:4.332147598266602
epoch13: step1000/4680
step 19500: accuracy:0.257999986410141, confidence:0.8700814843177795, loss:4.545414447784424
epoch13: step1500/4680
step 26000: accuracy:0.4429999887943268, confidence:0.7754323482513428, loss:2.3448715209960938
epoch13: step2000/4680
step 32500: accuracy:0.13099999725818634, confidence:0.9613201022148132, loss:6.408691883087158
epoch13: step2500/4680
step 39000: accuracy:0.2070000022649765, confidence:0.8772928714752197, loss:5.047922134399414
epoch13: step3000/4680
step 45500: accuracy:0.23199999332427979, confidence:0.8907044529914856, loss:4.077045440673828
epoch13: step3500/4680
step 52000: accuracy:0.15600000321865082, confidence:0.9192542433738708, loss:4.924388408660889
epoch13: step4000/4680
step 58500: accuracy:0.3190000057220459, confidence:0.8010373711585999, loss:2.6998791694641113
epoch13: step4500/4680
step 0: accuracy:0.25999999046325684, confidence:0.8343300819396973, loss:3.626030206680298
epoch14: step0/4680
step 7000: accuracy:0.28200000524520874, confidence:0.7763459086418152, loss:2.8956689834594727
epoch14: step500/4680
step 14000: accuracy:0.23499999940395355, confidence:0.9230161905288696, loss:5.002011299133301
epoch14: step1000/4680
step 21000: accuracy:0.23899999260902405, confidence:0.8402515649795532, loss:3.9025301933288574
epoch14: step1500/4680
step 28000: accuracy:0.2280000001192093, confidence:0.8051841855049133, loss:3.2385714054107666
epoch14: step2000/4680
step 35000: accuracy:0.10300000011920929, confidence:0.9402512311935425, loss:5.363102436065674
epoch14: step2500/4680
step 42000: accuracy:0.2280000001192093, confidence:0.8441386222839355, loss:3.892159938812256
epoch14: step3000/4680
step 49000: accuracy:0.2160000056028366, confidence:0.8726288080215454, loss:4.12167501449585
epoch14: step3500/4680
step 56000: accuracy:0.1509999930858612, confidence:0.8970232009887695, loss:4.324997901916504
epoch14: step4000/4680
step 63000: accuracy:0.33799999952316284, confidence:0.7801414132118225, loss:2.507786273956299
epoch14: step4500/4680
step 0: accuracy:0.2590000033378601, confidence:0.8258817791938782, loss:3.50113844871521
epoch15: step0/4680
step 7500: accuracy:0.31200000643730164, confidence:0.8033586740493774, loss:3.0363807678222656
epoch15: step500/4680
step 15000: accuracy:0.21199999749660492, confidence:0.9286788702011108, loss:5.031169414520264
epoch15: step1000/4680
step 22500: accuracy:0.24199999868869781, confidence:0.8304402828216553, loss:4.59945011138916
epoch15: step1500/4680
step 30000: accuracy:0.39800000190734863, confidence:0.7814087867736816, loss:2.6189658641815186
epoch15: step2000/4680
step 37500: accuracy:0.1459999978542328, confidence:0.9549819827079773, loss:6.5768818855285645
epoch15: step2500/4680
step 45000: accuracy:0.17100000381469727, confidence:0.9102137684822083, loss:5.832843780517578
epoch15: step3000/4680
step 52500: accuracy:0.22300000488758087, confidence:0.8659733533859253, loss:3.7838821411132812
epoch15: step3500/4680
step 60000: accuracy:0.20100000500679016, confidence:0.894751787185669, loss:3.988419771194458
epoch15: step4000/4680
step 67500: accuracy:0.33500000834465027, confidence:0.8000801205635071, loss:2.724957227706909
epoch15: step4500/4680
step 0: accuracy:0.28999999165534973, confidence:0.8264335989952087, loss:3.189263343811035
epoch16: step0/4680
step 8000: accuracy:0.3179999887943268, confidence:0.8340982794761658, loss:3.0821914672851562
epoch16: step500/4680
step 16000: accuracy:0.210999995470047, confidence:0.9147939682006836, loss:4.418050765991211
epoch16: step1000/4680
step 24000: accuracy:0.210999995470047, confidence:0.8558260202407837, loss:3.9486405849456787
epoch16: step1500/4680
step 32000: accuracy:0.31299999356269836, confidence:0.7570102214813232, loss:2.4405834674835205
epoch16: step2000/4680
step 40000: accuracy:0.12800000607967377, confidence:0.912710428237915, loss:4.7772674560546875
epoch16: step2500/4680
step 48000: accuracy:0.2070000022649765, confidence:0.8639820218086243, loss:4.158626556396484
epoch16: step3000/4680
step 56000: accuracy:0.22200000286102295, confidence:0.8679913878440857, loss:4.115161895751953
epoch16: step3500/4680
step 64000: accuracy:0.1289999932050705, confidence:0.9156989455223083, loss:4.547773361206055
epoch16: step4000/4680
step 72000: accuracy:0.25, confidence:0.7749668955802917, loss:2.7905616760253906
epoch16: step4500/4680
step 0: accuracy:0.2160000056028366, confidence:0.8303094506263733, loss:3.7294864654541016
epoch17: step0/4680
step 8500: accuracy:0.3490000069141388, confidence:0.7499686479568481, loss:2.2555246353149414
epoch17: step500/4680
step 17000: accuracy:0.24699999392032623, confidence:0.8742515444755554, loss:3.3760359287261963
epoch17: step1000/4680
step 25500: accuracy:0.29600000381469727, confidence:0.8294962048530579, loss:3.75298810005188
epoch17: step1500/4680
step 34000: accuracy:0.3970000147819519, confidence:0.7790924906730652, loss:2.57334041595459
epoch17: step2000/4680
step 42500: accuracy:0.12700000405311584, confidence:0.9749833941459656, loss:7.041474342346191
epoch17: step2500/4680
step 51000: accuracy:0.20100000500679016, confidence:0.8953821659088135, loss:5.074731826782227
epoch17: step3000/4680
step 59500: accuracy:0.20499999821186066, confidence:0.8683449625968933, loss:4.130277156829834
epoch17: step3500/4680
step 68000: accuracy:0.19900000095367432, confidence:0.8998137712478638, loss:4.020420551300049
epoch17: step4000/4680
step 76500: accuracy:0.2290000021457672, confidence:0.8608295321464539, loss:3.738095760345459
epoch17: step4500/4680
step 0: accuracy:0.24300000071525574, confidence:0.8648756742477417, loss:3.7931463718414307
epoch18: step0/4680
step 9000: accuracy:0.25, confidence:0.892883837223053, loss:4.373976230621338
epoch18: step500/4680
step 18000: accuracy:0.19900000095367432, confidence:0.9158147573471069, loss:4.9377946853637695
epoch18: step1000/4680
step 27000: accuracy:0.2590000033378601, confidence:0.8447816371917725, loss:3.27799391746521
epoch18: step1500/4680
step 36000: accuracy:0.3179999887943268, confidence:0.7362090349197388, loss:2.1567864418029785
epoch18: step2000/4680
step 45000: accuracy:0.1550000011920929, confidence:0.9047878384590149, loss:4.656837463378906
epoch18: step2500/4680
step 54000: accuracy:0.23600000143051147, confidence:0.8549485802650452, loss:3.9576058387756348
epoch18: step3000/4680
step 63000: accuracy:0.19499999284744263, confidence:0.8611544966697693, loss:4.172608375549316
epoch18: step3500/4680
step 72000: accuracy:0.14100000262260437, confidence:0.9066767692565918, loss:4.601076126098633
epoch18: step4000/4680
step 81000: accuracy:0.27900001406669617, confidence:0.7988874912261963, loss:2.8446104526519775
epoch18: step4500/4680
step 0: accuracy:0.22100000083446503, confidence:0.8408312201499939, loss:3.6965503692626953
epoch19: step0/4680
step 9500: accuracy:0.35199999809265137, confidence:0.7153158187866211, loss:1.985292673110962
epoch19: step500/4680
step 19000: accuracy:0.22599999606609344, confidence:0.8933329582214355, loss:3.7785632610321045
epoch19: step1000/4680
step 28500: accuracy:0.3009999990463257, confidence:0.7793738842010498, loss:3.038555145263672
epoch19: step1500/4680
step 38000: accuracy:0.41100001335144043, confidence:0.7774387001991272, loss:2.5482535362243652
epoch19: step2000/4680
step 47500: accuracy:0.17000000178813934, confidence:0.9441721439361572, loss:5.780889987945557
epoch19: step2500/4680
step 57000: accuracy:0.20999999344348907, confidence:0.8717755079269409, loss:4.481562614440918
epoch19: step3000/4680
step 66500: accuracy:0.23000000417232513, confidence:0.8779820203781128, loss:4.2027411460876465
epoch19: step3500/4680
step 76000: accuracy:0.14800000190734863, confidence:0.9134324789047241, loss:4.683772563934326
epoch19: step4000/4680
step 85500: accuracy:0.3089999854564667, confidence:0.8054547309875488, loss:2.8266074657440186
epoch19: step4500/4680
step 0: accuracy:0.2770000100135803, confidence:0.8325070738792419, loss:3.506516933441162
epoch20: step0/4680
step 10000: accuracy:0.32600000500679016, confidence:0.842873215675354, loss:3.6701626777648926
epoch20: step500/4680
step 20000: accuracy:0.1889999955892563, confidence:0.9231263995170593, loss:5.254154205322266
epoch20: step1000/4680
step 30000: accuracy:0.2919999957084656, confidence:0.7856114506721497, loss:2.8008334636688232
epoch20: step1500/4680
step 40000: accuracy:0.3919999897480011, confidence:0.7453320026397705, loss:2.2684924602508545
epoch20: step2000/4680
step 50000: accuracy:0.1720000058412552, confidence:0.8959082961082458, loss:4.331982612609863
epoch20: step2500/4680
step 60000: accuracy:0.2770000100135803, confidence:0.8710538148880005, loss:3.792585849761963
epoch20: step3000/4680
step 70000: accuracy:0.3009999990463257, confidence:0.8494330048561096, loss:3.325087070465088
epoch20: step3500/4680
step 80000: accuracy:0.1289999932050705, confidence:0.9265754222869873, loss:5.173752307891846
epoch20: step4000/4680
step 90000: accuracy:0.3580000102519989, confidence:0.8110253214836121, loss:2.596738576889038
epoch20: step4500/4680
step 0: accuracy:0.24199999868869781, confidence:0.8605329990386963, loss:3.757452964782715
epoch21: step0/4680
step 10500: accuracy:0.382999986410141, confidence:0.7281036972999573, loss:1.9461708068847656
epoch21: step500/4680
step 21000: accuracy:0.2540000081062317, confidence:0.8764836192131042, loss:3.725203514099121
epoch21: step1000/4680
step 31500: accuracy:0.31200000643730164, confidence:0.8177542686462402, loss:3.446474313735962
epoch21: step1500/4680
step 42000: accuracy:0.3790000081062317, confidence:0.7519750595092773, loss:2.3681931495666504
epoch21: step2000/4680
step 52500: accuracy:0.13899999856948853, confidence:0.9276224374771118, loss:5.351475238800049
epoch21: step2500/4680
step 63000: accuracy:0.2280000001192093, confidence:0.8557260632514954, loss:4.0541534423828125
epoch21: step3000/4680
step 73500: accuracy:0.2029999941587448, confidence:0.8805038928985596, loss:4.082215309143066
epoch21: step3500/4680
step 84000: accuracy:0.1379999965429306, confidence:0.9216089248657227, loss:4.605064392089844
epoch21: step4000/4680
step 94500: accuracy:0.33500000834465027, confidence:0.7915714979171753, loss:2.518869161605835
epoch21: step4500/4680
step 0: accuracy:0.24899999797344208, confidence:0.8306996822357178, loss:3.570378303527832
epoch22: step0/4680
step 11000: accuracy:0.3019999861717224, confidence:0.7852233052253723, loss:3.101726531982422
epoch22: step500/4680
step 22000: accuracy:0.19200000166893005, confidence:0.9210418462753296, loss:4.924458980560303
epoch22: step1000/4680
step 33000: accuracy:0.3140000104904175, confidence:0.7749212384223938, loss:2.831118583679199
epoch22: step1500/4680
step 44000: accuracy:0.3490000069141388, confidence:0.7384012341499329, loss:2.3222930431365967
epoch22: step2000/4680
step 55000: accuracy:0.21699999272823334, confidence:0.8778006434440613, loss:3.927263021469116
epoch22: step2500/4680
step 66000: accuracy:0.23399999737739563, confidence:0.8516895771026611, loss:4.13316011428833
epoch22: step3000/4680
step 77000: accuracy:0.26499998569488525, confidence:0.8653345704078674, loss:3.7104909420013428
epoch22: step3500/4680
step 88000: accuracy:0.10899999737739563, confidence:0.9345216751098633, loss:5.4896135330200195
epoch22: step4000/4680
step 99000: accuracy:0.335999995470047, confidence:0.8234854340553284, loss:2.709423780441284
epoch22: step4500/4680
step 0: accuracy:0.27000001072883606, confidence:0.8651685118675232, loss:3.5959653854370117
epoch23: step0/4680
step 11500: accuracy:0.335999995470047, confidence:0.7637546062469482, loss:2.5747199058532715
epoch23: step500/4680
step 23000: accuracy:0.20200000703334808, confidence:0.8796033263206482, loss:4.228907108306885
epoch23: step1000/4680
step 34500: accuracy:0.20900000631809235, confidence:0.8423236012458801, loss:5.569002628326416
epoch23: step1500/4680
step 46000: accuracy:0.30300000309944153, confidence:0.7885650992393494, loss:3.1728620529174805
epoch23: step2000/4680
step 57500: accuracy:0.19300000369548798, confidence:0.9073082804679871, loss:4.663839817047119
epoch23: step2500/4680
step 69000: accuracy:0.2680000066757202, confidence:0.8747141361236572, loss:3.906298875808716
epoch23: step3000/4680
step 80500: accuracy:0.24500000476837158, confidence:0.8749299049377441, loss:3.844341278076172
epoch23: step3500/4680
step 92000: accuracy:0.17499999701976776, confidence:0.901101291179657, loss:4.2402496337890625
epoch23: step4000/4680
step 103500: accuracy:0.3400000035762787, confidence:0.7844994068145752, loss:2.5288286209106445
epoch23: step4500/4680
step 0: accuracy:0.29899999499320984, confidence:0.8303236961364746, loss:3.2769858837127686
epoch24: step0/4680
step 12000: accuracy:0.3400000035762787, confidence:0.803808331489563, loss:2.938643217086792
epoch24: step500/4680
step 24000: accuracy:0.19900000095367432, confidence:0.9045771360397339, loss:4.244593143463135
epoch24: step1000/4680
step 36000: accuracy:0.3930000066757202, confidence:0.7837443947792053, loss:2.1954288482666016
epoch24: step1500/4680
step 48000: accuracy:0.42500001192092896, confidence:0.7574618458747864, loss:2.008544683456421
epoch24: step2000/4680
step 60000: accuracy:0.17900000512599945, confidence:0.9069609642028809, loss:4.916140556335449
epoch24: step2500/4680
step 72000: accuracy:0.24199999868869781, confidence:0.8724177479743958, loss:4.8091301918029785
epoch24: step3000/4680
step 84000: accuracy:0.25, confidence:0.8562527894973755, loss:3.451059579849243
epoch24: step3500/4680
step 96000: accuracy:0.1420000046491623, confidence:0.916392982006073, loss:4.856100082397461
epoch24: step4000/4680
step 108000: accuracy:0.328000009059906, confidence:0.7955127954483032, loss:2.590394973754883
epoch24: step4500/4680
step 0: accuracy:0.27900001406669617, confidence:0.8386859893798828, loss:3.336575984954834
epoch25: step0/4680
step 12500: accuracy:0.46299999952316284, confidence:0.7436274290084839, loss:1.7051922082901
epoch25: step500/4680
step 25000: accuracy:0.2720000147819519, confidence:0.8400839567184448, loss:3.345383405685425
epoch25: step1000/4680
step 37500: accuracy:0.4230000078678131, confidence:0.7557306885719299, loss:2.0918967723846436
epoch25: step1500/4680
step 50000: accuracy:0.38999998569488525, confidence:0.7535962462425232, loss:2.1705970764160156
epoch25: step2000/4680
step 62500: accuracy:0.26499998569488525, confidence:0.8494654297828674, loss:3.356257915496826
epoch25: step2500/4680
step 75000: accuracy:0.2720000147819519, confidence:0.8316763639450073, loss:3.259967803955078
epoch25: step3000/4680
step 87500: accuracy:0.28299999237060547, confidence:0.8526536822319031, loss:3.3085200786590576
epoch25: step3500/4680
step 100000: accuracy:0.17900000512599945, confidence:0.8888437151908875, loss:4.027426242828369
epoch25: step4000/4680
step 112500: accuracy:0.38600000739097595, confidence:0.7851666808128357, loss:2.210660696029663
epoch25: step4500/4680
step 0: accuracy:0.28299999237060547, confidence:0.8343231081962585, loss:3.3301045894622803
epoch26: step0/4680
step 13000: accuracy:0.35199999809265137, confidence:0.8282870054244995, loss:3.0219013690948486
epoch26: step500/4680
step 26000: accuracy:0.20600000023841858, confidence:0.9139711856842041, loss:4.391287326812744
epoch26: step1000/4680
step 39000: accuracy:0.37700000405311584, confidence:0.76558917760849, loss:2.5154144763946533
epoch26: step1500/4680
step 52000: accuracy:0.46299999952316284, confidence:0.771374523639679, loss:2.049738883972168
epoch26: step2000/4680
step 65000: accuracy:0.17000000178813934, confidence:0.934535801410675, loss:5.525627613067627
epoch26: step2500/4680
step 78000: accuracy:0.25600001215934753, confidence:0.8713556528091431, loss:4.398318767547607
epoch26: step3000/4680
step 91000: accuracy:0.2409999966621399, confidence:0.8553709983825684, loss:3.438506841659546
epoch26: step3500/4680
step 104000: accuracy:0.16500000655651093, confidence:0.8816044330596924, loss:4.350162982940674
epoch26: step4000/4680
step 117000: accuracy:0.3529999852180481, confidence:0.801791787147522, loss:2.3836936950683594
epoch26: step4500/4680
step 0: accuracy:0.2409999966621399, confidence:0.8506699204444885, loss:3.64797306060791
epoch27: step0/4680
step 13500: accuracy:0.3720000088214874, confidence:0.7711195349693298, loss:2.4527111053466797
epoch27: step500/4680
step 27000: accuracy:0.257999986410141, confidence:0.8713545799255371, loss:3.414185047149658
epoch27: step1000/4680
step 40500: accuracy:0.24799999594688416, confidence:0.8714171051979065, loss:3.860337257385254
epoch27: step1500/4680
step 54000: accuracy:0.3140000104904175, confidence:0.778659462928772, loss:2.7396912574768066
epoch27: step2000/4680
step 67500: accuracy:0.26100000739097595, confidence:0.8704548478126526, loss:3.2221872806549072
epoch27: step2500/4680
step 81000: accuracy:0.2770000100135803, confidence:0.8532967567443848, loss:3.500725269317627
epoch27: step3000/4680
step 94500: accuracy:0.22499999403953552, confidence:0.8776510953903198, loss:4.138695240020752
epoch27: step3500/4680
step 108000: accuracy:0.15399999916553497, confidence:0.9089903831481934, loss:4.415048122406006
epoch27: step4000/4680
step 121500: accuracy:0.35199999809265137, confidence:0.8063821792602539, loss:2.402207136154175
epoch27: step4500/4680
step 0: accuracy:0.257999986410141, confidence:0.8508051633834839, loss:3.3506696224212646
epoch28: step0/4680
step 14000: accuracy:0.3050000071525574, confidence:0.8443313837051392, loss:3.620041608810425
epoch28: step500/4680
step 28000: accuracy:0.1850000023841858, confidence:0.9177903532981873, loss:4.644739151000977
epoch28: step1000/4680
step 42000: accuracy:0.41499999165534973, confidence:0.7789009213447571, loss:2.3497817516326904
epoch28: step1500/4680
step 56000: accuracy:0.4359999895095825, confidence:0.7685918211936951, loss:2.153956651687622
epoch28: step2000/4680
step 70000: accuracy:0.13899999856948853, confidence:0.9205572605133057, loss:5.394184589385986
epoch28: step2500/4680
step 84000: accuracy:0.26600000262260437, confidence:0.8693528175354004, loss:4.127023696899414
epoch28: step3000/4680
step 98000: accuracy:0.24199999868869781, confidence:0.8577512502670288, loss:3.6961755752563477
epoch28: step3500/4680
step 112000: accuracy:0.20600000023841858, confidence:0.8934474587440491, loss:4.068556785583496
epoch28: step4000/4680
step 126000: accuracy:0.33399999141693115, confidence:0.8007172346115112, loss:2.5660483837127686
epoch28: step4500/4680
step 0: accuracy:0.2680000066757202, confidence:0.8334290981292725, loss:3.425612211227417
epoch29: step0/4680
step 14500: accuracy:0.45100000500679016, confidence:0.7244715690612793, loss:1.6497960090637207
epoch29: step500/4680
step 29000: accuracy:0.2240000069141388, confidence:0.8922256231307983, loss:4.119003772735596
epoch29: step1000/4680
step 43500: accuracy:0.3919999897480011, confidence:0.7702010869979858, loss:2.2350354194641113
epoch29: step1500/4680
step 58000: accuracy:0.43700000643730164, confidence:0.7487914562225342, loss:1.9323136806488037
epoch29: step2000/4680
step 72500: accuracy:0.29499998688697815, confidence:0.8467249870300293, loss:3.006155490875244
epoch29: step2500/4680
step 87000: accuracy:0.27000001072883606, confidence:0.8577077388763428, loss:3.773383855819702
epoch29: step3000/4680
step 101500: accuracy:0.2029999941587448, confidence:0.8905970454216003, loss:4.447362899780273
epoch29: step3500/4680
step 116000: accuracy:0.16300000250339508, confidence:0.9029962420463562, loss:4.433311939239502
epoch29: step4000/4680
step 130500: accuracy:0.37400001287460327, confidence:0.7958962321281433, loss:2.3753483295440674
epoch29: step4500/4680
2018-06-15 19:47:06.896157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:47:06.896388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
2018-06-15 19:47:09.135741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
CHEKPOINT DIR: uniform
model has been loaded from mnist_classifier.pkl
---------
Variables: name (type shape) [size]
---------
Variable:0 (float32_ref 5x5x1x32) [800, bytes: 3200]
Variable_1:0 (float32_ref 32) [32, bytes: 128]
Variable_2:0 (float32_ref 5x5x32x64) [51200, bytes: 204800]
Variable_3:0 (float32_ref 64) [64, bytes: 256]
Variable_4:0 (float32_ref 3136x1024) [3211264, bytes: 12845056]
Variable_5:0 (float32_ref 1024) [1024, bytes: 4096]
Variable_6:0 (float32_ref 1024x10) [10240, bytes: 40960]
Variable_7:0 (float32_ref 10) [10, bytes: 40]
discriminator/d_conv1/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]
discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]
discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]
discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]
generator/g_fc1/Matrix:0 (float32_ref 74x1024) [75776, bytes: 303104]
generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]
generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]
generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]
classifier/c_fc1/Matrix:0 (float32_ref 1024x64) [65536, bytes: 262144]
classifier/c_fc1/bias:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/beta:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/gamma:0 (float32_ref 64) [64, bytes: 256]
classifier/c_fc2/Matrix:0 (float32_ref 64x12) [768, bytes: 3072]
classifier/c_fc2/bias:0 (float32_ref 12) [12, bytes: 48]
Total size of variables: 16552792
Total bytes of variables: 66211168
 [*] Reading checkpoints...
 [*] Success to read MultiModalInfoGAN.model-43721
 [*] Load SUCCESS


SAMPLES SIZE=4680,LABELS=299520


SAVED TRAINING SET generated_training_set_mnist_UniformSample_mu_0_sigma_0.15_czcc_rzcc_czrc_rzrc
 [*] Training finished!
 [*] Testing finished!
2018-06-15 19:47:55.833247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:47:55.834284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
model has been loaded from mnist_classifier.pkl
argmax:[0 0 0 ..., 0 0 0]
step 0: accuracy:0.0, confidence:0.9998022317886353, loss:16.037172317504883
Assinging:1
[10000]
argmax:[5 5 5 ..., 5 5 5]
step 0: accuracy:0.0, confidence:0.9976364970207214, loss:15.872801780700684
Assinging:6
[   0    0    0    4    0 9959   36    0    1]
argmax:[1 1 1 ..., 1 1 1]
step 0: accuracy:0.0, confidence:0.99969083070755, loss:15.047454833984375
Assinging:2
[    0 10000]
argmax:[8 8 8 ..., 8 8 8]
step 0: accuracy:0.0, confidence:0.9997566342353821, loss:17.477663040161133
Assinging:9
[    0     0     0     0     0     0     0     0 10000]
argmax:[3 3 3 ..., 3 3 3]
step 0: accuracy:0.0, confidence:0.9999160170555115, loss:15.061266899108887
Assinging:4
[    0     0     0 10000]
argmax:[4 4 4 ..., 4 4 4]
step 0: accuracy:0.0008999999845400453, confidence:0.9990870952606201, loss:16.04001235961914
Assinging:5
[   1    0    0    0 9990    0    0    0    0    9]
argmax:[9 9 9 ..., 9 9 9]
step 0: accuracy:0.9998999834060669, confidence:0.9993384480476379, loss:0.0007068284321576357
Assinging:10
[   0    0    0    0    0    0    0    1    0 9999]
argmax:[6 6 6 ..., 6 6 6]
step 0: accuracy:0.0, confidence:0.9995357394218445, loss:21.198984146118164
Assinging:7
[   1    0    0    0    0    0 9999]
argmax:[2 2 2 ..., 2 2 2]
step 0: accuracy:0.0, confidence:0.9985897541046143, loss:17.366241455078125
Assinging:3
[   0    0 9997    0    0    0    0    0    3]
argmax:[7 7 7 ..., 7 7 7]
step 0: accuracy:0.0, confidence:0.9999059438705444, loss:14.124401092529297
Assinging:8
[    0     0     0     0     0     0     0 10000]
2018-06-15 19:48:17.636915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:48:17.637174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
step 0: accuracy:0.10700000077486038, confidence:0.9873235821723938, loss:12.76987361907959
epoch0: step0/4680
step 0: accuracy:0.10199999809265137, confidence:1.0, loss:50.23698425292969
epoch0: step500/4680
step 0: accuracy:0.09099999815225601, confidence:1.0, loss:36.61463165283203
epoch0: step1000/4680
step 0: accuracy:0.1120000034570694, confidence:0.9999818205833435, loss:13.406907081604004
epoch0: step1500/4680
step 0: accuracy:0.08299999684095383, confidence:0.9999796152114868, loss:13.362186431884766
epoch0: step2000/4680
step 0: accuracy:0.07999999821186066, confidence:0.9980804920196533, loss:8.393959999084473
epoch0: step2500/4680
step 0: accuracy:0.07900000363588333, confidence:0.9999522566795349, loss:12.63155746459961
epoch0: step3000/4680
step 0: accuracy:0.11299999803304672, confidence:0.9995299577713013, loss:11.536913871765137
epoch0: step3500/4680
step 0: accuracy:0.0949999988079071, confidence:0.9987825751304626, loss:9.032005310058594
epoch0: step4000/4680
step 0: accuracy:0.09700000286102295, confidence:0.9967746734619141, loss:8.23688793182373
epoch0: step4500/4680
step 0: accuracy:0.0860000029206276, confidence:0.9990869164466858, loss:9.645208358764648
epoch1: step0/4680
step 500: accuracy:0.09000000357627869, confidence:0.9422438740730286, loss:4.82076358795166
epoch1: step500/4680
step 1000: accuracy:0.0820000022649765, confidence:0.9978904128074646, loss:8.509899139404297
epoch1: step1000/4680
step 1500: accuracy:0.11999999731779099, confidence:0.9972951412200928, loss:8.327880859375
epoch1: step1500/4680
step 2000: accuracy:0.10899999737739563, confidence:0.9985767602920532, loss:8.403209686279297
epoch1: step2000/4680
step 2500: accuracy:0.09799999743700027, confidence:0.9985508918762207, loss:8.65608024597168
epoch1: step2500/4680
step 3000: accuracy:0.10400000214576721, confidence:0.9991655945777893, loss:8.912581443786621
epoch1: step3000/4680
step 3500: accuracy:0.11599999666213989, confidence:0.9995046257972717, loss:9.417701721191406
epoch1: step3500/4680
step 4000: accuracy:0.08799999952316284, confidence:0.9999430179595947, loss:11.860374450683594
epoch1: step4000/4680
step 4500: accuracy:0.09200000017881393, confidence:0.9986385703086853, loss:8.874476432800293
epoch1: step4500/4680
step 0: accuracy:0.10899999737739563, confidence:0.9995433688163757, loss:9.830648422241211
epoch2: step0/4680
step 1000: accuracy:0.11999999731779099, confidence:0.9915336966514587, loss:8.106240272521973
epoch2: step500/4680
step 2000: accuracy:0.07599999755620956, confidence:0.9964616894721985, loss:8.500490188598633
epoch2: step1000/4680
step 3000: accuracy:0.10499999672174454, confidence:0.9959580898284912, loss:7.110641002655029
epoch2: step1500/4680
step 4000: accuracy:0.0949999988079071, confidence:0.9997180104255676, loss:10.342618942260742
epoch2: step2000/4680
step 5000: accuracy:0.09799999743700027, confidence:0.9934307336807251, loss:7.204063892364502
epoch2: step2500/4680
step 6000: accuracy:0.0949999988079071, confidence:0.9993937611579895, loss:9.828353881835938
epoch2: step3000/4680
step 7000: accuracy:0.09000000357627869, confidence:0.9986966848373413, loss:8.45706558227539
epoch2: step3500/4680
step 8000: accuracy:0.1080000028014183, confidence:0.9996467232704163, loss:9.264193534851074
epoch2: step4000/4680
step 9000: accuracy:0.0949999988079071, confidence:0.9983626008033752, loss:8.3006010055542
epoch2: step4500/4680
step 0: accuracy:0.11599999666213989, confidence:0.9993754625320435, loss:8.987539291381836
epoch3: step0/4680
step 1500: accuracy:0.10400000214576721, confidence:0.969764232635498, loss:6.1838507652282715
epoch3: step500/4680
step 3000: accuracy:0.11900000274181366, confidence:0.99568772315979, loss:9.651467323303223
epoch3: step1000/4680
step 4500: accuracy:0.09700000286102295, confidence:0.9976276159286499, loss:7.911797523498535
epoch3: step1500/4680
step 6000: accuracy:0.0860000029206276, confidence:0.9951614737510681, loss:6.976739406585693
epoch3: step2000/4680
step 7500: accuracy:0.09700000286102295, confidence:0.9990283250808716, loss:9.498693466186523
epoch3: step2500/4680
step 9000: accuracy:0.0989999994635582, confidence:0.9995304942131042, loss:9.225662231445312
epoch3: step3000/4680
step 10500: accuracy:0.0989999994635582, confidence:0.999294638633728, loss:8.534356117248535
epoch3: step3500/4680
step 12000: accuracy:0.10499999672174454, confidence:0.9996246695518494, loss:9.547501564025879
epoch3: step4000/4680
step 13500: accuracy:0.09799999743700027, confidence:0.997886598110199, loss:8.05419921875
epoch3: step4500/4680
step 0: accuracy:0.09300000220537186, confidence:0.9990309476852417, loss:8.758625984191895
epoch4: step0/4680
step 2000: accuracy:0.10499999672174454, confidence:0.8860909342765808, loss:4.395230293273926
epoch4: step500/4680
step 4000: accuracy:0.09099999815225601, confidence:0.8936298489570618, loss:4.6717729568481445
epoch4: step1000/4680
step 6000: accuracy:0.10000000149011612, confidence:0.9937469363212585, loss:7.131557941436768
epoch4: step1500/4680
step 8000: accuracy:0.09399999678134918, confidence:0.9996880292892456, loss:10.149599075317383
epoch4: step2000/4680
step 10000: accuracy:0.08699999749660492, confidence:0.9977179169654846, loss:8.059208869934082
epoch4: step2500/4680
step 12000: accuracy:0.09600000083446503, confidence:0.9991825222969055, loss:8.710878372192383
epoch4: step3000/4680
step 14000: accuracy:0.09799999743700027, confidence:0.9965090155601501, loss:7.042333602905273
epoch4: step3500/4680
step 16000: accuracy:0.10599999874830246, confidence:0.9972379207611084, loss:7.469832420349121
epoch4: step4000/4680
step 18000: accuracy:0.1080000028014183, confidence:0.9778795838356018, loss:5.760544776916504
epoch4: step4500/4680
step 0: accuracy:0.08500000089406967, confidence:0.9929180145263672, loss:6.984010219573975
epoch5: step0/4680
step 2500: accuracy:0.1979999989271164, confidence:0.8508400917053223, loss:4.231001377105713
epoch5: step500/4680
step 5000: accuracy:0.17399999499320984, confidence:0.7461983561515808, loss:2.998720645904541
epoch5: step1000/4680
step 7500: accuracy:0.14499999582767487, confidence:0.9011881351470947, loss:4.6752119064331055
epoch5: step1500/4680
step 10000: accuracy:0.09700000286102295, confidence:0.9520511031150818, loss:4.731070518493652
epoch5: step2000/4680
step 12500: accuracy:0.09099999815225601, confidence:0.9631141424179077, loss:4.924083709716797
epoch5: step2500/4680
step 15000: accuracy:0.0860000029206276, confidence:0.9960988759994507, loss:7.329267978668213
epoch5: step3000/4680
step 17500: accuracy:0.10899999737739563, confidence:0.9765413403511047, loss:5.217376232147217
epoch5: step3500/4680
step 20000: accuracy:0.09700000286102295, confidence:0.9714874029159546, loss:5.524173259735107
epoch5: step4000/4680
step 22500: accuracy:0.0989999994635582, confidence:0.9323801398277283, loss:4.900832176208496
epoch5: step4500/4680
step 0: accuracy:0.11500000208616257, confidence:0.9835954308509827, loss:6.134560585021973
epoch6: step0/4680
step 3000: accuracy:0.20900000631809235, confidence:0.8740566372871399, loss:4.775542259216309
epoch6: step500/4680
step 6000: accuracy:0.24199999868869781, confidence:0.7327332496643066, loss:2.613754987716675
epoch6: step1000/4680
step 9000: accuracy:0.1979999989271164, confidence:0.7899348139762878, loss:3.279003858566284
epoch6: step1500/4680
step 12000: accuracy:0.1589999943971634, confidence:0.8827752470970154, loss:3.726628303527832
epoch6: step2000/4680
step 15000: accuracy:0.13500000536441803, confidence:0.9259666204452515, loss:4.051684379577637
epoch6: step2500/4680
step 18000: accuracy:0.09399999678134918, confidence:0.9691194891929626, loss:5.579445838928223
epoch6: step3000/4680
step 21000: accuracy:0.10199999809265137, confidence:0.9453561305999756, loss:4.792680740356445
epoch6: step3500/4680
step 24000: accuracy:0.1340000033378601, confidence:0.9208734035491943, loss:4.559513568878174
epoch6: step4000/4680
step 27000: accuracy:0.13600000739097595, confidence:0.8508591055870056, loss:4.066708564758301
epoch6: step4500/4680
step 0: accuracy:0.11900000274181366, confidence:0.9585480093955994, loss:5.416043281555176
epoch7: step0/4680
step 3500: accuracy:0.2029999941587448, confidence:0.8400388360023499, loss:3.7335166931152344
epoch7: step500/4680
step 7000: accuracy:0.28299999237060547, confidence:0.7343271970748901, loss:2.4094505310058594
epoch7: step1000/4680
step 10500: accuracy:0.25600001215934753, confidence:0.7528069019317627, loss:2.594818115234375
epoch7: step1500/4680
step 14000: accuracy:0.20000000298023224, confidence:0.8185815215110779, loss:3.345210552215576
epoch7: step2000/4680
step 17500: accuracy:0.19900000095367432, confidence:0.846817135810852, loss:3.237076997756958
epoch7: step2500/4680
step 21000: accuracy:0.15000000596046448, confidence:0.9190213680267334, loss:5.092360973358154
epoch7: step3000/4680
step 24500: accuracy:0.10400000214576721, confidence:0.9019216299057007, loss:4.244943618774414
epoch7: step3500/4680
step 28000: accuracy:0.14300000667572021, confidence:0.8988638520240784, loss:3.987165927886963
epoch7: step4000/4680
step 31500: accuracy:0.1940000057220459, confidence:0.8315771222114563, loss:3.939160108566284
epoch7: step4500/4680
step 0: accuracy:0.10700000077486038, confidence:0.94739830493927, loss:5.548080921173096
epoch8: step0/4680
step 4000: accuracy:0.23000000417232513, confidence:0.8597102165222168, loss:3.809840440750122
epoch8: step500/4680
step 8000: accuracy:0.27300000190734863, confidence:0.7778812050819397, loss:2.6879169940948486
epoch8: step1000/4680
step 12000: accuracy:0.28299999237060547, confidence:0.7477716207504272, loss:2.4970786571502686
epoch8: step1500/4680
step 16000: accuracy:0.27399998903274536, confidence:0.7810433506965637, loss:2.6620633602142334
epoch8: step2000/4680
step 20000: accuracy:0.2150000035762787, confidence:0.8279135823249817, loss:3.0626795291900635
epoch8: step2500/4680
step 24000: accuracy:0.18700000643730164, confidence:0.8702191114425659, loss:4.19150447845459
epoch8: step3000/4680
step 28000: accuracy:0.15000000596046448, confidence:0.8677434921264648, loss:4.142599582672119
epoch8: step3500/4680
step 32000: accuracy:0.14800000190734863, confidence:0.905906617641449, loss:4.088399887084961
epoch8: step4000/4680
step 36000: accuracy:0.2280000001192093, confidence:0.8376076817512512, loss:3.442939043045044
epoch8: step4500/4680
step 0: accuracy:0.12800000607967377, confidence:0.911990225315094, loss:4.747360706329346
epoch9: step0/4680
step 4500: accuracy:0.2160000056028366, confidence:0.8372287154197693, loss:3.890976667404175
epoch9: step500/4680
step 9000: accuracy:0.33399999141693115, confidence:0.7529014348983765, loss:2.363084554672241
epoch9: step1000/4680
step 13500: accuracy:0.33000001311302185, confidence:0.766227662563324, loss:2.4292140007019043
epoch9: step1500/4680
step 18000: accuracy:0.3370000123977661, confidence:0.7786909937858582, loss:2.5841562747955322
epoch9: step2000/4680
step 22500: accuracy:0.2370000034570694, confidence:0.777293860912323, loss:2.672816514968872
epoch9: step2500/4680
step 27000: accuracy:0.21799999475479126, confidence:0.836777925491333, loss:3.8409574031829834
epoch9: step3000/4680
step 31500: accuracy:0.24500000476837158, confidence:0.8195781111717224, loss:3.5047032833099365
epoch9: step3500/4680
step 36000: accuracy:0.1979999989271164, confidence:0.8898460268974304, loss:3.658658266067505
epoch9: step4000/4680
step 40500: accuracy:0.2150000035762787, confidence:0.8552624583244324, loss:4.170815467834473
epoch9: step4500/4680
step 0: accuracy:0.14900000393390656, confidence:0.9408919811248779, loss:5.379777431488037
epoch10: step0/4680
step 5000: accuracy:0.3959999978542328, confidence:0.7920217514038086, loss:2.130932331085205
epoch10: step500/4680
step 10000: accuracy:0.3869999945163727, confidence:0.7268989682197571, loss:1.9266762733459473
epoch10: step1000/4680
step 15000: accuracy:0.3409999907016754, confidence:0.7625019550323486, loss:2.24780535697937
epoch10: step1500/4680
step 20000: accuracy:0.31299999356269836, confidence:0.7698149681091309, loss:2.2697348594665527
epoch10: step2000/4680
step 25000: accuracy:0.25200000405311584, confidence:0.8043332099914551, loss:2.82922625541687
epoch10: step2500/4680
step 30000: accuracy:0.2460000067949295, confidence:0.845913827419281, loss:3.640089273452759
epoch10: step3000/4680
step 35000: accuracy:0.2619999945163727, confidence:0.8107559680938721, loss:3.486844062805176
epoch10: step3500/4680
step 40000: accuracy:0.19900000095367432, confidence:0.8719043135643005, loss:3.6217572689056396
epoch10: step4000/4680
step 45000: accuracy:0.2759999930858612, confidence:0.820331871509552, loss:3.0497231483459473
epoch10: step4500/4680
step 0: accuracy:0.16599999368190765, confidence:0.8963180780410767, loss:4.5491204261779785
epoch11: step0/4680
step 5500: accuracy:0.21400000154972076, confidence:0.8261871933937073, loss:3.2832577228546143
epoch11: step500/4680
step 11000: accuracy:0.3540000021457672, confidence:0.7014458179473877, loss:1.9727787971496582
epoch11: step1000/4680
step 16500: accuracy:0.34599998593330383, confidence:0.7484530806541443, loss:2.1877243518829346
epoch11: step1500/4680
step 22000: accuracy:0.3330000042915344, confidence:0.7679982781410217, loss:2.6623899936676025
epoch11: step2000/4680
step 27500: accuracy:0.34299999475479126, confidence:0.7473370432853699, loss:2.2758657932281494
epoch11: step2500/4680
step 33000: accuracy:0.28600001335144043, confidence:0.8194393515586853, loss:3.070981979370117
epoch11: step3000/4680
step 38500: accuracy:0.33500000834465027, confidence:0.8080812096595764, loss:3.01628041267395
epoch11: step3500/4680
step 44000: accuracy:0.22100000083446503, confidence:0.8714371919631958, loss:3.7740092277526855
epoch11: step4000/4680
step 49500: accuracy:0.21199999749660492, confidence:0.8526971340179443, loss:3.841278553009033
epoch11: step4500/4680
step 0: accuracy:0.13600000739097595, confidence:0.9314143657684326, loss:5.087125301361084
epoch12: step0/4680
step 6000: accuracy:0.4359999895095825, confidence:0.7876561284065247, loss:2.052703619003296
epoch12: step500/4680
step 12000: accuracy:0.44600000977516174, confidence:0.7596731781959534, loss:1.86857271194458
epoch12: step1000/4680
step 18000: accuracy:0.36899998784065247, confidence:0.816203236579895, loss:2.3358216285705566
epoch12: step1500/4680
step 24000: accuracy:0.2460000067949295, confidence:0.827727198600769, loss:2.4784045219421387
epoch12: step2000/4680
step 30000: accuracy:0.23100000619888306, confidence:0.8019331693649292, loss:2.902095317840576
epoch12: step2500/4680
step 36000: accuracy:0.20600000023841858, confidence:0.8841461539268494, loss:3.9304473400115967
epoch12: step3000/4680
step 42000: accuracy:0.14399999380111694, confidence:0.9315425157546997, loss:4.94493293762207
epoch12: step3500/4680
step 48000: accuracy:0.1469999998807907, confidence:0.9195410013198853, loss:4.72929048538208
epoch12: step4000/4680
step 54000: accuracy:0.20000000298023224, confidence:0.877498209476471, loss:4.069282054901123
epoch12: step4500/4680
step 0: accuracy:0.10300000011920929, confidence:0.9542378783226013, loss:5.578830718994141
epoch13: step0/4680
step 6500: accuracy:0.23600000143051147, confidence:0.8245863914489746, loss:3.218980312347412
epoch13: step500/4680
step 13000: accuracy:0.335999995470047, confidence:0.7140220403671265, loss:2.024968147277832
epoch13: step1000/4680
step 19500: accuracy:0.30000001192092896, confidence:0.7771129608154297, loss:2.508232831954956
epoch13: step1500/4680
step 26000: accuracy:0.328000009059906, confidence:0.7934001088142395, loss:2.8593013286590576
epoch13: step2000/4680
step 32500: accuracy:0.27900001406669617, confidence:0.7862093448638916, loss:2.72263240814209
epoch13: step2500/4680
step 39000: accuracy:0.2840000092983246, confidence:0.8419238328933716, loss:3.4823226928710938
epoch13: step3000/4680
step 45500: accuracy:0.35899999737739563, confidence:0.7854105234146118, loss:2.8579039573669434
epoch13: step3500/4680
step 52000: accuracy:0.2070000022649765, confidence:0.9040544629096985, loss:4.791985511779785
epoch13: step4000/4680
step 58500: accuracy:0.210999995470047, confidence:0.874284029006958, loss:3.6656246185302734
epoch13: step4500/4680
step 0: accuracy:0.11699999868869781, confidence:0.9433261752128601, loss:5.1443963050842285
epoch14: step0/4680
step 7000: accuracy:0.453000009059906, confidence:0.7740006446838379, loss:1.905805230140686
epoch14: step500/4680
step 14000: accuracy:0.4880000054836273, confidence:0.7434809803962708, loss:1.7136870622634888
epoch14: step1000/4680
step 21000: accuracy:0.40400001406669617, confidence:0.7949069738388062, loss:2.2080576419830322
epoch14: step1500/4680
step 28000: accuracy:0.20600000023841858, confidence:0.8791980147361755, loss:2.987663507461548
epoch14: step2000/4680
step 35000: accuracy:0.28700000047683716, confidence:0.7856243252754211, loss:2.588498830795288
epoch14: step2500/4680
step 42000: accuracy:0.22499999403953552, confidence:0.8518785238265991, loss:3.3672244548797607
epoch14: step3000/4680
step 49000: accuracy:0.18799999356269836, confidence:0.8509432077407837, loss:3.6620888710021973
epoch14: step3500/4680
step 56000: accuracy:0.22499999403953552, confidence:0.8820163607597351, loss:3.6705057621002197
epoch14: step4000/4680
step 63000: accuracy:0.25099998712539673, confidence:0.8235491514205933, loss:3.6936099529266357
epoch14: step4500/4680
step 0: accuracy:0.17599999904632568, confidence:0.9138497710227966, loss:5.145698070526123
epoch15: step0/4680
step 7500: accuracy:0.31200000643730164, confidence:0.796775758266449, loss:2.7221179008483887
epoch15: step500/4680
step 15000: accuracy:0.5, confidence:0.7101109623908997, loss:1.5854582786560059
epoch15: step1000/4680
step 22500: accuracy:0.414000004529953, confidence:0.7572007179260254, loss:2.0157971382141113
epoch15: step1500/4680
step 30000: accuracy:0.2879999876022339, confidence:0.8206301927566528, loss:3.0331053733825684
epoch15: step2000/4680
step 37500: accuracy:0.34200000762939453, confidence:0.805743396282196, loss:2.5158488750457764
epoch15: step2500/4680
step 45000: accuracy:0.23800000548362732, confidence:0.8395352363586426, loss:3.6424202919006348
epoch15: step3000/4680
step 52500: accuracy:0.3869999945163727, confidence:0.7970693111419678, loss:2.544971227645874
epoch15: step3500/4680
step 60000: accuracy:0.22599999606609344, confidence:0.8924102187156677, loss:3.9729318618774414
epoch15: step4000/4680
step 67500: accuracy:0.2619999945163727, confidence:0.8381600379943848, loss:3.069307565689087
epoch15: step4500/4680
step 0: accuracy:0.1589999943971634, confidence:0.9170523285865784, loss:4.52070951461792
epoch16: step0/4680
step 8000: accuracy:0.49399998784065247, confidence:0.8085641860961914, loss:1.9727600812911987
epoch16: step500/4680
step 16000: accuracy:0.43700000643730164, confidence:0.7604931592941284, loss:2.0175023078918457
epoch16: step1000/4680
step 24000: accuracy:0.3479999899864197, confidence:0.8009510040283203, loss:2.382584810256958
epoch16: step1500/4680
step 32000: accuracy:0.23000000417232513, confidence:0.8942970633506775, loss:3.346930980682373
epoch16: step2000/4680
step 40000: accuracy:0.2750000059604645, confidence:0.8170644044876099, loss:2.789497137069702
epoch16: step2500/4680
step 48000: accuracy:0.2750000059604645, confidence:0.8509937524795532, loss:3.219515323638916
epoch16: step3000/4680
step 56000: accuracy:0.3009999990463257, confidence:0.7789702415466309, loss:2.822099447250366
epoch16: step3500/4680
step 64000: accuracy:0.24400000274181366, confidence:0.8736464977264404, loss:3.787245035171509
epoch16: step4000/4680
step 72000: accuracy:0.24799999594688416, confidence:0.8343452215194702, loss:3.5489158630371094
epoch16: step4500/4680
step 0: accuracy:0.19099999964237213, confidence:0.9247061610221863, loss:4.964145660400391
epoch17: step0/4680
step 8500: accuracy:0.3799999952316284, confidence:0.7535407543182373, loss:2.0102736949920654
epoch17: step500/4680
step 17000: accuracy:0.49799999594688416, confidence:0.7287819385528564, loss:1.7232543230056763
epoch17: step1000/4680
step 25500: accuracy:0.43700000643730164, confidence:0.7696108222007751, loss:1.9020171165466309
epoch17: step1500/4680
step 34000: accuracy:0.27900001406669617, confidence:0.8728798627853394, loss:3.889371156692505
epoch17: step2000/4680
step 42500: accuracy:0.34200000762939453, confidence:0.8001958131790161, loss:2.535975456237793
epoch17: step2500/4680
step 51000: accuracy:0.25699999928474426, confidence:0.8391143679618835, loss:3.46012020111084
epoch17: step3000/4680
step 59500: accuracy:0.375, confidence:0.8050283193588257, loss:2.5550174713134766
epoch17: step3500/4680
step 68000: accuracy:0.257999986410141, confidence:0.878840446472168, loss:3.5266683101654053
epoch17: step4000/4680
step 76500: accuracy:0.27300000190734863, confidence:0.8376096487045288, loss:2.932137966156006
epoch17: step4500/4680
step 0: accuracy:0.16500000655651093, confidence:0.9097561240196228, loss:4.686773777008057
epoch18: step0/4680
step 9000: accuracy:0.4399999976158142, confidence:0.7955915927886963, loss:1.966379165649414
epoch18: step500/4680
step 18000: accuracy:0.4620000123977661, confidence:0.7462331652641296, loss:1.8478645086288452
epoch18: step1000/4680
step 27000: accuracy:0.4480000138282776, confidence:0.7867076396942139, loss:1.981831669807434
epoch18: step1500/4680
step 36000: accuracy:0.22300000488758087, confidence:0.876484751701355, loss:3.1622354984283447
epoch18: step2000/4680
step 45000: accuracy:0.29899999499320984, confidence:0.7980329990386963, loss:2.617943286895752
epoch18: step2500/4680
step 54000: accuracy:0.22100000083446503, confidence:0.8595739603042603, loss:3.788531541824341
epoch18: step3000/4680
step 63000: accuracy:0.3089999854564667, confidence:0.7880633473396301, loss:2.716402769088745
epoch18: step3500/4680
step 72000: accuracy:0.24899999797344208, confidence:0.8512895703315735, loss:3.3356385231018066
epoch18: step4000/4680
step 81000: accuracy:0.27900001406669617, confidence:0.8447534441947937, loss:3.3815927505493164
epoch18: step4500/4680
step 0: accuracy:0.1679999977350235, confidence:0.9020139575004578, loss:4.846269607543945
epoch19: step0/4680
step 9500: accuracy:0.3580000102519989, confidence:0.7795800566673279, loss:2.3591997623443604
epoch19: step500/4680
step 19000: accuracy:0.31299999356269836, confidence:0.8350915908813477, loss:2.70706844329834
epoch19: step1000/4680
step 28500: accuracy:0.35600000619888306, confidence:0.8040529489517212, loss:2.43798828125
epoch19: step1500/4680
step 38000: accuracy:0.24799999594688416, confidence:0.8651372194290161, loss:3.729438066482544
epoch19: step2000/4680
step 47500: accuracy:0.37299999594688416, confidence:0.7984262108802795, loss:2.399853229522705
epoch19: step2500/4680
step 57000: accuracy:0.2770000100135803, confidence:0.8616625070571899, loss:3.41018009185791
epoch19: step3000/4680
step 66500: accuracy:0.3619999885559082, confidence:0.8046185374259949, loss:2.6328916549682617
epoch19: step3500/4680
step 76000: accuracy:0.257999986410141, confidence:0.8929480910301208, loss:3.9149880409240723
epoch19: step4000/4680
step 85500: accuracy:0.26899999380111694, confidence:0.8668503761291504, loss:3.2402234077453613
epoch19: step4500/4680
step 0: accuracy:0.15399999916553497, confidence:0.9217991232872009, loss:4.76832389831543
epoch20: step0/4680
step 10000: accuracy:0.42800000309944153, confidence:0.807560920715332, loss:2.1644983291625977
epoch20: step500/4680
step 20000: accuracy:0.42800000309944153, confidence:0.7716854810714722, loss:2.184920310974121
epoch20: step1000/4680
step 30000: accuracy:0.4429999887943268, confidence:0.7951387166976929, loss:1.9771326780319214
epoch20: step1500/4680
step 40000: accuracy:0.24799999594688416, confidence:0.8751702308654785, loss:3.1858558654785156
epoch20: step2000/4680
step 50000: accuracy:0.35100001096725464, confidence:0.7940345406532288, loss:2.4331064224243164
epoch20: step2500/4680
step 60000: accuracy:0.25699999928474426, confidence:0.8627575039863586, loss:3.460573196411133
epoch20: step3000/4680
step 70000: accuracy:0.29899999499320984, confidence:0.787886917591095, loss:2.8851051330566406
epoch20: step3500/4680
step 80000: accuracy:0.2619999945163727, confidence:0.8922280073165894, loss:3.625661611557007
epoch20: step4000/4680
step 90000: accuracy:0.26100000739097595, confidence:0.83021080493927, loss:3.5507149696350098
epoch20: step4500/4680
step 0: accuracy:0.18299999833106995, confidence:0.8952767848968506, loss:4.692923069000244
epoch21: step0/4680
step 10500: accuracy:0.4269999861717224, confidence:0.7705007791519165, loss:2.010561943054199
epoch21: step500/4680
step 21000: accuracy:0.5289999842643738, confidence:0.7556418776512146, loss:1.586325764656067
epoch21: step1000/4680
step 31500: accuracy:0.38499999046325684, confidence:0.8229915499687195, loss:2.3906147480010986
epoch21: step1500/4680
step 42000: accuracy:0.2070000022649765, confidence:0.9123908877372742, loss:5.103682518005371
epoch21: step2000/4680
step 52500: accuracy:0.3889999985694885, confidence:0.8081840872764587, loss:2.453436851501465
epoch21: step2500/4680
step 63000: accuracy:0.27000001072883606, confidence:0.8797349333763123, loss:3.5797343254089355
epoch21: step3000/4680
step 73500: accuracy:0.4169999957084656, confidence:0.7919014692306519, loss:2.432338237762451
epoch21: step3500/4680
step 84000: accuracy:0.25600001215934753, confidence:0.8936884999275208, loss:3.873452663421631
epoch21: step4000/4680
step 94500: accuracy:0.2770000100135803, confidence:0.8632824420928955, loss:3.218322515487671
epoch21: step4500/4680
step 0: accuracy:0.17000000178813934, confidence:0.9193311333656311, loss:4.6939544677734375
epoch22: step0/4680
step 11000: accuracy:0.4560000002384186, confidence:0.7903903722763062, loss:1.936491847038269
epoch22: step500/4680
step 22000: accuracy:0.38100001215934753, confidence:0.7692878246307373, loss:2.303950309753418
epoch22: step1000/4680
step 33000: accuracy:0.40299999713897705, confidence:0.8020299673080444, loss:2.1590256690979004
epoch22: step1500/4680
step 44000: accuracy:0.28600001335144043, confidence:0.8685041069984436, loss:3.034003496170044
epoch22: step2000/4680
step 55000: accuracy:0.31299999356269836, confidence:0.8026029467582703, loss:2.6009597778320312
epoch22: step2500/4680
step 66000: accuracy:0.25200000405311584, confidence:0.8677763342857361, loss:4.066149711608887
epoch22: step3000/4680
step 77000: accuracy:0.30300000309944153, confidence:0.7888274192810059, loss:2.840137243270874
epoch22: step3500/4680
step 88000: accuracy:0.2460000067949295, confidence:0.871941089630127, loss:3.7384626865386963
epoch22: step4000/4680
step 99000: accuracy:0.28600001335144043, confidence:0.8239197134971619, loss:3.376875162124634
epoch22: step4500/4680
step 0: accuracy:0.17900000512599945, confidence:0.9011015892028809, loss:5.056619167327881
epoch23: step0/4680
step 11500: accuracy:0.4429999887943268, confidence:0.7665045857429504, loss:1.933221459388733
epoch23: step500/4680
step 23000: accuracy:0.4959999918937683, confidence:0.751008152961731, loss:1.763771414756775
epoch23: step1000/4680
step 34500: accuracy:0.4169999957084656, confidence:0.8041784763336182, loss:2.385010004043579
epoch23: step1500/4680
step 46000: accuracy:0.2549999952316284, confidence:0.8905032873153687, loss:4.288275241851807
epoch23: step2000/4680
step 57500: accuracy:0.39800000190734863, confidence:0.8063048720359802, loss:2.449174642562866
epoch23: step2500/4680
step 69000: accuracy:0.2619999945163727, confidence:0.8724742531776428, loss:3.646850109100342
epoch23: step3000/4680
step 80500: accuracy:0.3540000021457672, confidence:0.8394867181777954, loss:3.1620945930480957
epoch23: step3500/4680
step 92000: accuracy:0.2529999911785126, confidence:0.8784091472625732, loss:3.4150657653808594
epoch23: step4000/4680
step 103500: accuracy:0.28299999237060547, confidence:0.8482787609100342, loss:3.292789936065674
epoch23: step4500/4680
step 0: accuracy:0.1850000023841858, confidence:0.9060013890266418, loss:4.74443244934082
epoch24: step0/4680
step 12000: accuracy:0.4309999942779541, confidence:0.8272429704666138, loss:2.402935743331909
epoch24: step500/4680
step 24000: accuracy:0.4779999852180481, confidence:0.7528809905052185, loss:1.9451531171798706
epoch24: step1000/4680
step 36000: accuracy:0.4490000009536743, confidence:0.7942072749137878, loss:2.0574121475219727
epoch24: step1500/4680
step 48000: accuracy:0.2150000035762787, confidence:0.8895753622055054, loss:3.726630926132202
epoch24: step2000/4680
step 60000: accuracy:0.3479999899864197, confidence:0.7814285159111023, loss:2.390700578689575
epoch24: step2500/4680
step 72000: accuracy:0.25099998712539673, confidence:0.8625372052192688, loss:3.9139487743377686
epoch24: step3000/4680
step 84000: accuracy:0.3370000123977661, confidence:0.7849997282028198, loss:2.767910957336426
epoch24: step3500/4680
step 96000: accuracy:0.26100000739097595, confidence:0.8802557587623596, loss:4.299012660980225
epoch24: step4000/4680
step 108000: accuracy:0.2879999876022339, confidence:0.8404043912887573, loss:3.6221444606781006
epoch24: step4500/4680
step 0: accuracy:0.17299999296665192, confidence:0.9142252802848816, loss:5.234598159790039
epoch25: step0/4680
step 12500: accuracy:0.527999997138977, confidence:0.7530524134635925, loss:1.5748025178909302
epoch25: step500/4680
step 25000: accuracy:0.492000013589859, confidence:0.7526519894599915, loss:1.7081726789474487
epoch25: step1000/4680
step 37500: accuracy:0.4449999928474426, confidence:0.7918593287467957, loss:2.023545026779175
epoch25: step1500/4680
step 50000: accuracy:0.21899999678134918, confidence:0.9259443283081055, loss:5.5335187911987305
epoch25: step2000/4680
step 62500: accuracy:0.382999986410141, confidence:0.7871172428131104, loss:2.3727242946624756
epoch25: step2500/4680
step 75000: accuracy:0.1509999930858612, confidence:0.9268659353256226, loss:5.049500942230225
epoch25: step3000/4680
step 87500: accuracy:0.39100000262260437, confidence:0.8020092248916626, loss:2.6416079998016357
epoch25: step3500/4680
step 100000: accuracy:0.27300000190734863, confidence:0.8795275688171387, loss:3.5345191955566406
epoch25: step4000/4680
step 112500: accuracy:0.3160000145435333, confidence:0.8285017013549805, loss:3.1962969303131104
epoch25: step4500/4680
step 0: accuracy:0.22100000083446503, confidence:0.8931525945663452, loss:4.3035664558410645
epoch26: step0/4680
step 13000: accuracy:0.382999986410141, confidence:0.8473649621009827, loss:2.793858289718628
epoch26: step500/4680
step 26000: accuracy:0.550000011920929, confidence:0.7781871557235718, loss:1.7130496501922607
epoch26: step1000/4680
step 39000: accuracy:0.4339999854564667, confidence:0.8065530061721802, loss:2.067443370819092
epoch26: step1500/4680
step 52000: accuracy:0.2329999953508377, confidence:0.9039539694786072, loss:4.01864767074585
epoch26: step2000/4680
step 65000: accuracy:0.3370000123977661, confidence:0.814178466796875, loss:2.676884174346924
epoch26: step2500/4680
step 78000: accuracy:0.23899999260902405, confidence:0.863232433795929, loss:4.020937442779541
epoch26: step3000/4680
step 91000: accuracy:0.36399999260902405, confidence:0.7848437428474426, loss:2.933204412460327
epoch26: step3500/4680
step 104000: accuracy:0.210999995470047, confidence:0.9065329432487488, loss:5.165152549743652
epoch26: step4000/4680
step 117000: accuracy:0.2460000067949295, confidence:0.836222767829895, loss:3.427644729614258
epoch26: step4500/4680
step 0: accuracy:0.15000000596046448, confidence:0.921246349811554, loss:4.916858196258545
epoch27: step0/4680
step 13500: accuracy:0.5429999828338623, confidence:0.7443042397499084, loss:1.5250532627105713
epoch27: step500/4680
step 27000: accuracy:0.5019999742507935, confidence:0.7463171482086182, loss:1.6404542922973633
epoch27: step1000/4680
step 40500: accuracy:0.39500001072883606, confidence:0.8051371574401855, loss:2.479884386062622
epoch27: step1500/4680
step 54000: accuracy:0.23399999737739563, confidence:0.8824357986450195, loss:4.181478977203369
epoch27: step2000/4680
step 67500: accuracy:0.37599998712539673, confidence:0.8150679469108582, loss:2.39123797416687
epoch27: step2500/4680
step 81000: accuracy:0.2280000001192093, confidence:0.9050329327583313, loss:4.489767551422119
epoch27: step3000/4680
step 94500: accuracy:0.3400000035762787, confidence:0.7921317219734192, loss:2.6593177318573
epoch27: step3500/4680
step 108000: accuracy:0.2669999897480011, confidence:0.8884809613227844, loss:3.4956114292144775
epoch27: step4000/4680
step 121500: accuracy:0.3310000002384186, confidence:0.8410165905952454, loss:3.187237024307251
epoch27: step4500/4680
step 0: accuracy:0.21899999678134918, confidence:0.9136485457420349, loss:4.732195854187012
epoch28: step0/4680
step 14000: accuracy:0.27900001406669617, confidence:0.8751844763755798, loss:3.971963405609131
epoch28: step500/4680
step 28000: accuracy:0.4320000112056732, confidence:0.7678731679916382, loss:2.2814271450042725
epoch28: step1000/4680
step 42000: accuracy:0.4620000123977661, confidence:0.8138094544410706, loss:2.005476713180542
epoch28: step1500/4680
step 56000: accuracy:0.24199999868869781, confidence:0.9040772914886475, loss:4.053818702697754
epoch28: step2000/4680
step 70000: accuracy:0.3700000047683716, confidence:0.7978833913803101, loss:2.5354297161102295
epoch28: step2500/4680
step 84000: accuracy:0.23000000417232513, confidence:0.8542168736457825, loss:3.848834991455078
epoch28: step3000/4680
step 98000: accuracy:0.35100001096725464, confidence:0.7935526967048645, loss:2.941372871398926
epoch28: step3500/4680
step 112000: accuracy:0.23600000143051147, confidence:0.8954911231994629, loss:4.888859748840332
epoch28: step4000/4680
step 126000: accuracy:0.31299999356269836, confidence:0.8241010904312134, loss:3.0679047107696533
epoch28: step4500/4680
step 0: accuracy:0.21199999749660492, confidence:0.8910788893699646, loss:4.546230316162109
epoch29: step0/4680
step 14500: accuracy:0.41100001335144043, confidence:0.7842891216278076, loss:2.0678935050964355
epoch29: step500/4680
step 29000: accuracy:0.49900001287460327, confidence:0.7636854648590088, loss:1.7154808044433594
epoch29: step1000/4680
step 43500: accuracy:0.4399999976158142, confidence:0.8141893744468689, loss:2.2811193466186523
epoch29: step1500/4680
step 58000: accuracy:0.25200000405311584, confidence:0.8991989493370056, loss:4.260427951812744
epoch29: step2000/4680
step 72500: accuracy:0.36500000953674316, confidence:0.8303327560424805, loss:2.5492637157440186
epoch29: step2500/4680
step 87000: accuracy:0.20800000429153442, confidence:0.9094646573066711, loss:4.833690166473389
epoch29: step3000/4680
step 101500: accuracy:0.34700000286102295, confidence:0.8167596459388733, loss:2.9234540462493896
epoch29: step3500/4680
step 116000: accuracy:0.2709999978542328, confidence:0.8652304410934448, loss:3.5640957355499268
epoch29: step4000/4680
step 130500: accuracy:0.3160000145435333, confidence:0.8334836363792419, loss:3.3139514923095703
epoch29: step4500/4680
2018-06-15 19:58:58.444955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:58:58.445214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
2018-06-15 19:59:00.661978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
CHEKPOINT DIR: uniform
model has been loaded from mnist_classifier.pkl
---------
Variables: name (type shape) [size]
---------
Variable:0 (float32_ref 5x5x1x32) [800, bytes: 3200]
Variable_1:0 (float32_ref 32) [32, bytes: 128]
Variable_2:0 (float32_ref 5x5x32x64) [51200, bytes: 204800]
Variable_3:0 (float32_ref 64) [64, bytes: 256]
Variable_4:0 (float32_ref 3136x1024) [3211264, bytes: 12845056]
Variable_5:0 (float32_ref 1024) [1024, bytes: 4096]
Variable_6:0 (float32_ref 1024x10) [10240, bytes: 40960]
Variable_7:0 (float32_ref 10) [10, bytes: 40]
discriminator/d_conv1/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]
discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]
discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]
discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]
generator/g_fc1/Matrix:0 (float32_ref 74x1024) [75776, bytes: 303104]
generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]
generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]
generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]
classifier/c_fc1/Matrix:0 (float32_ref 1024x64) [65536, bytes: 262144]
classifier/c_fc1/bias:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/beta:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/gamma:0 (float32_ref 64) [64, bytes: 256]
classifier/c_fc2/Matrix:0 (float32_ref 64x12) [768, bytes: 3072]
classifier/c_fc2/bias:0 (float32_ref 12) [12, bytes: 48]
Total size of variables: 16552792
Total bytes of variables: 66211168
 [*] Reading checkpoints...
 [*] Success to read MultiModalInfoGAN.model-43721
 [*] Load SUCCESS


SAMPLES SIZE=4680,LABELS=299520


SAVED TRAINING SET generated_training_set_mnist_UniformSample_mu_0_sigma_0.15_rzcc_rzrc_czcc_czrc
 [*] Training finished!
 [*] Testing finished!
2018-06-15 19:59:57.274006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:59:57.274244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
model has been loaded from mnist_classifier.pkl
argmax:[1 1 1 ..., 1 1 1]
step 0: accuracy:0.0, confidence:0.998964250087738, loss:14.234450340270996
Assinging:2
[    0 10000]
argmax:[8 8 8 ..., 8 8 8]
step 0: accuracy:9.999999747378752e-05, confidence:0.9962821006774902, loss:14.078629493713379
Assinging:9
[   0    7    1   20    0    0    1    0 9970    1]
argmax:[5 3 5 ..., 3 3 5]
step 0: accuracy:0.0015999999595806003, confidence:0.9588294625282288, loss:11.314593315124512
Assinging:4
[   0   21    6 5638    0 3895   30    1  393   16]
argmax:[6 6 6 ..., 6 6 6]
step 0: accuracy:0.0, confidence:0.9971836805343628, loss:19.124378204345703
Assinging:7
[   4    4    1    0    0    0 9991]
argmax:[7 7 7 ..., 7 7 7]
step 0: accuracy:9.999999747378752e-05, confidence:0.9975576996803284, loss:11.977566719055176
Assinging:8
[   1    6    0    1    0    0    0 9990    1    1]
argmax:[9 9 0 ..., 9 9 9]
step 0: accuracy:0.4927999973297119, confidence:0.9656084775924683, loss:4.456563472747803
Assinging:5
[  30    2    0    0 5008    0    1   15   16 4928]
argmax:[2 2 2 ..., 2 2 2]
step 0: accuracy:0.0, confidence:0.9954068064689636, loss:19.654895782470703
Assinging:3
[   0   11 9975    0    0    0    0    6    8]
argmax:[9 4 4 ..., 9 9 9]
step 0: accuracy:0.7405999898910522, confidence:0.9674437642097473, loss:2.053767442703247
Assinging:10
[   0   61    0    0 2511    0    0   10   12 7406]
argmax:[0 0 0 ..., 0 0 0]
step 0: accuracy:0.0, confidence:0.9997186660766602, loss:16.891040802001953
Assinging:1
[10000]
argmax:[5 5 3 ..., 3 3 5]
step 0: accuracy:0.0013000000035390258, confidence:0.9315531253814697, loss:8.664572715759277
Assinging:4
[   6    0    4 5294    1 4664    5    2   11   13]
2018-06-15 20:00:26.227060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 20:00:26.227280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
step 0: accuracy:0.10300000011920929, confidence:0.9807925820350647, loss:11.058998107910156
epoch0: step0/4680
step 0: accuracy:0.11800000071525574, confidence:1.0, loss:30.25350570678711
epoch0: step500/4680
step 0: accuracy:0.08900000154972076, confidence:1.0, loss:46.37546920776367
epoch0: step1000/4680
step 0: accuracy:0.09799999743700027, confidence:0.9999681115150452, loss:13.573552131652832
epoch0: step1500/4680
step 0: accuracy:0.09600000083446503, confidence:0.9999927282333374, loss:14.335062026977539
epoch0: step2000/4680
step 0: accuracy:0.09099999815225601, confidence:0.9994105696678162, loss:8.996709823608398
epoch0: step2500/4680
step 0: accuracy:0.07900000363588333, confidence:0.9999855756759644, loss:13.857781410217285
epoch0: step3000/4680
step 0: accuracy:0.1080000028014183, confidence:0.999179482460022, loss:9.129775047302246
epoch0: step3500/4680
step 0: accuracy:0.10199999809265137, confidence:0.9999569058418274, loss:12.576812744140625
epoch0: step4000/4680
step 0: accuracy:0.08500000089406967, confidence:0.9977201223373413, loss:8.142305374145508
epoch0: step4500/4680
step 0: accuracy:0.08299999684095383, confidence:0.9981897473335266, loss:8.427186012268066
epoch1: step0/4680
step 500: accuracy:0.09700000286102295, confidence:0.99869704246521, loss:8.5686616897583
epoch1: step500/4680
step 1000: accuracy:0.09600000083446503, confidence:0.9998121857643127, loss:10.492483139038086
epoch1: step1000/4680
step 1500: accuracy:0.10100000351667404, confidence:0.9999614357948303, loss:15.398726463317871
epoch1: step1500/4680
step 2000: accuracy:0.08699999749660492, confidence:0.997122585773468, loss:8.446090698242188
epoch1: step2000/4680
step 2500: accuracy:0.08299999684095383, confidence:0.999599039554596, loss:9.851359367370605
epoch1: step2500/4680
step 3000: accuracy:0.10400000214576721, confidence:0.9999887943267822, loss:13.46723461151123
epoch1: step3000/4680
step 3500: accuracy:0.10499999672174454, confidence:0.9999877214431763, loss:14.836939811706543
epoch1: step3500/4680
step 4000: accuracy:0.10100000351667404, confidence:0.999973475933075, loss:13.056896209716797
epoch1: step4000/4680
step 4500: accuracy:0.09000000357627869, confidence:0.9995037913322449, loss:9.45052433013916
epoch1: step4500/4680
step 0: accuracy:0.10100000351667404, confidence:0.9995504021644592, loss:9.415407180786133
epoch2: step0/4680
step 1000: accuracy:0.08399999886751175, confidence:0.9993775486946106, loss:9.452058792114258
epoch2: step500/4680
step 2000: accuracy:0.09799999743700027, confidence:0.9999417662620544, loss:10.993352890014648
epoch2: step1000/4680
step 3000: accuracy:0.10000000149011612, confidence:0.9999806880950928, loss:14.678498268127441
epoch2: step1500/4680
step 4000: accuracy:0.10999999940395355, confidence:0.9999423623085022, loss:13.82165813446045
epoch2: step2000/4680
step 5000: accuracy:0.0860000029206276, confidence:0.9999270439147949, loss:12.37549114227295
epoch2: step2500/4680
step 6000: accuracy:0.0949999988079071, confidence:0.9999638199806213, loss:12.152878761291504
epoch2: step3000/4680
step 7000: accuracy:0.09399999678134918, confidence:0.999704897403717, loss:10.944448471069336
epoch2: step3500/4680
step 8000: accuracy:0.08399999886751175, confidence:0.9998536705970764, loss:11.095531463623047
epoch2: step4000/4680
step 9000: accuracy:0.09700000286102295, confidence:0.9999754428863525, loss:12.392127990722656
epoch2: step4500/4680
step 0: accuracy:0.09799999743700027, confidence:0.9999632835388184, loss:12.078178405761719
epoch3: step0/4680
step 1500: accuracy:0.13699999451637268, confidence:0.9991081357002258, loss:8.499969482421875
epoch3: step500/4680
step 3000: accuracy:0.10000000149011612, confidence:0.9999785423278809, loss:11.971323013305664
epoch3: step1000/4680
step 4500: accuracy:0.09799999743700027, confidence:0.9998024702072144, loss:11.709489822387695
epoch3: step1500/4680
step 6000: accuracy:0.08699999749660492, confidence:0.9994533658027649, loss:11.046631813049316
epoch3: step2000/4680
step 7500: accuracy:0.1120000034570694, confidence:0.9995062351226807, loss:10.274553298950195
epoch3: step2500/4680
step 9000: accuracy:0.0989999994635582, confidence:0.9999627470970154, loss:12.006591796875
epoch3: step3000/4680
step 10500: accuracy:0.08699999749660492, confidence:0.9998429417610168, loss:11.540021896362305
epoch3: step3500/4680
step 12000: accuracy:0.09000000357627869, confidence:0.9998010396957397, loss:10.666558265686035
epoch3: step4000/4680
step 13500: accuracy:0.10499999672174454, confidence:0.9993661046028137, loss:9.315850257873535
epoch3: step4500/4680
step 0: accuracy:0.10000000149011612, confidence:0.9993538856506348, loss:9.231182098388672
epoch4: step0/4680
step 2000: accuracy:0.12999999523162842, confidence:0.9929631352424622, loss:6.82835578918457
epoch4: step500/4680
step 4000: accuracy:0.10300000011920929, confidence:0.9998780488967896, loss:10.778545379638672
epoch4: step1000/4680
step 6000: accuracy:0.09000000357627869, confidence:0.9996670484542847, loss:10.450777053833008
epoch4: step1500/4680
step 8000: accuracy:0.0989999994635582, confidence:0.9994138479232788, loss:9.751340866088867
epoch4: step2000/4680
step 10000: accuracy:0.11599999666213989, confidence:0.9998506307601929, loss:11.245623588562012
epoch4: step2500/4680
step 12000: accuracy:0.09600000083446503, confidence:0.9997227191925049, loss:10.297333717346191
epoch4: step3000/4680
step 14000: accuracy:0.08900000154972076, confidence:0.9991388916969299, loss:9.552204132080078
epoch4: step3500/4680
step 16000: accuracy:0.10199999809265137, confidence:0.9995479583740234, loss:9.686492919921875
epoch4: step4000/4680
step 18000: accuracy:0.11100000143051147, confidence:0.9936767220497131, loss:7.279473781585693
epoch4: step4500/4680
step 0: accuracy:0.09600000083446503, confidence:0.9957359433174133, loss:7.766890525817871
epoch5: step0/4680
step 2500: accuracy:0.0949999988079071, confidence:0.935989499092102, loss:5.279424667358398
epoch5: step500/4680
step 5000: accuracy:0.10899999737739563, confidence:0.9995768070220947, loss:9.178628921508789
epoch5: step1000/4680
step 7500: accuracy:0.12700000405311584, confidence:0.9991755485534668, loss:9.540667533874512
epoch5: step1500/4680
step 10000: accuracy:0.09099999815225601, confidence:0.9954435229301453, loss:7.601269721984863
epoch5: step2000/4680
step 12500: accuracy:0.09399999678134918, confidence:0.9996865391731262, loss:10.348238945007324
epoch5: step2500/4680
step 15000: accuracy:0.0860000029206276, confidence:0.9999221563339233, loss:11.060086250305176
epoch5: step3000/4680
step 17500: accuracy:0.11999999731779099, confidence:0.9991739392280579, loss:8.996293067932129
epoch5: step3500/4680
step 20000: accuracy:0.0860000029206276, confidence:0.9994447231292725, loss:9.398865699768066
epoch5: step4000/4680
step 22500: accuracy:0.10199999809265137, confidence:0.9962131977081299, loss:7.8297295570373535
epoch5: step4500/4680
step 0: accuracy:0.10100000351667404, confidence:0.9973956346511841, loss:8.034276962280273
epoch6: step0/4680
step 3000: accuracy:0.10700000077486038, confidence:0.936726450920105, loss:5.975330829620361
epoch6: step500/4680
step 6000: accuracy:0.0860000029206276, confidence:0.9987130165100098, loss:8.72278881072998
epoch6: step1000/4680
step 9000: accuracy:0.10400000214576721, confidence:0.999701976776123, loss:10.63669490814209
epoch6: step1500/4680
step 12000: accuracy:0.09399999678134918, confidence:0.9971093535423279, loss:8.29641342163086
epoch6: step2000/4680
step 15000: accuracy:0.10100000351667404, confidence:0.9996803402900696, loss:10.45087718963623
epoch6: step2500/4680
step 18000: accuracy:0.09399999678134918, confidence:0.9997323155403137, loss:9.885234832763672
epoch6: step3000/4680
step 21000: accuracy:0.09000000357627869, confidence:0.9966120719909668, loss:7.8267717361450195
epoch6: step3500/4680
step 24000: accuracy:0.08900000154972076, confidence:0.999151349067688, loss:8.813394546508789
epoch6: step4000/4680
step 27000: accuracy:0.10999999940395355, confidence:0.9934847950935364, loss:7.183043003082275
epoch6: step4500/4680
step 0: accuracy:0.10700000077486038, confidence:0.9967236518859863, loss:7.701770782470703
epoch7: step0/4680
step 3500: accuracy:0.13099999725818634, confidence:0.8442744612693787, loss:4.545361042022705
epoch7: step500/4680
step 7000: accuracy:0.09799999743700027, confidence:0.9970840811729431, loss:7.993759632110596
epoch7: step1000/4680
step 10500: accuracy:0.10199999809265137, confidence:0.9994298815727234, loss:9.955241203308105
epoch7: step1500/4680
step 14000: accuracy:0.09700000286102295, confidence:0.9884300827980042, loss:7.046869277954102
epoch7: step2000/4680
step 17500: accuracy:0.1080000028014183, confidence:0.9984089136123657, loss:8.607295036315918
epoch7: step2500/4680
step 21000: accuracy:0.10899999737739563, confidence:0.9998241662979126, loss:10.16765308380127
epoch7: step3000/4680
step 24500: accuracy:0.10499999672174454, confidence:0.9928457140922546, loss:7.077078342437744
epoch7: step3500/4680
step 28000: accuracy:0.09099999815225601, confidence:0.9991385340690613, loss:8.811657905578613
epoch7: step4000/4680
step 31500: accuracy:0.10000000149011612, confidence:0.9785976409912109, loss:6.187860488891602
epoch7: step4500/4680
step 0: accuracy:0.09399999678134918, confidence:0.9823340177536011, loss:6.501378059387207
epoch8: step0/4680
step 4000: accuracy:0.12200000137090683, confidence:0.8070778250694275, loss:4.1435041427612305
epoch8: step500/4680
step 8000: accuracy:0.10000000149011612, confidence:0.9993321895599365, loss:9.89706802368164
epoch8: step1000/4680
step 12000: accuracy:0.10700000077486038, confidence:0.9985201954841614, loss:9.111252784729004
epoch8: step1500/4680
step 16000: accuracy:0.1080000028014183, confidence:0.9892334938049316, loss:6.8543596267700195
epoch8: step2000/4680
step 20000: accuracy:0.1120000034570694, confidence:0.9923130869865417, loss:7.2512288093566895
epoch8: step2500/4680
step 24000: accuracy:0.0820000022649765, confidence:0.999679446220398, loss:10.319154739379883
epoch8: step3000/4680
step 28000: accuracy:0.09099999815225601, confidence:0.9826717376708984, loss:6.619203090667725
epoch8: step3500/4680
step 32000: accuracy:0.09600000083446503, confidence:0.9983688592910767, loss:8.173617362976074
epoch8: step4000/4680
step 36000: accuracy:0.10999999940395355, confidence:0.9838459491729736, loss:6.39394998550415
epoch8: step4500/4680
step 0: accuracy:0.09200000017881393, confidence:0.9831501245498657, loss:6.457615375518799
epoch9: step0/4680
step 4500: accuracy:0.14100000262260437, confidence:0.7855902314186096, loss:3.801689863204956
epoch9: step500/4680
step 9000: accuracy:0.09799999743700027, confidence:0.9909917712211609, loss:7.28974723815918
epoch9: step1000/4680
step 13500: accuracy:0.11400000005960464, confidence:0.9930992722511292, loss:7.621954441070557
epoch9: step1500/4680
step 18000: accuracy:0.10999999940395355, confidence:0.9728690981864929, loss:6.150731563568115
epoch9: step2000/4680
step 22500: accuracy:0.09799999743700027, confidence:0.9955255389213562, loss:8.097850799560547
epoch9: step2500/4680
step 27000: accuracy:0.09200000017881393, confidence:0.9974563121795654, loss:8.216018676757812
epoch9: step3000/4680
step 31500: accuracy:0.10599999874830246, confidence:0.9519138336181641, loss:5.7797722816467285
epoch9: step3500/4680
step 36000: accuracy:0.09000000357627869, confidence:0.9926427006721497, loss:7.272434711456299
epoch9: step4000/4680
step 40500: accuracy:0.10199999809265137, confidence:0.962201714515686, loss:5.9674458503723145
epoch9: step4500/4680
step 0: accuracy:0.09200000017881393, confidence:0.9691107273101807, loss:6.141046047210693
epoch10: step0/4680
step 5000: accuracy:0.16899999976158142, confidence:0.7995084524154663, loss:3.7174949645996094
epoch10: step500/4680
step 10000: accuracy:0.10100000351667404, confidence:0.9940797686576843, loss:7.850554943084717
epoch10: step1000/4680
step 15000: accuracy:0.08100000023841858, confidence:0.9850806593894958, loss:7.728359222412109
epoch10: step1500/4680
step 20000: accuracy:0.11400000005960464, confidence:0.9484066367149353, loss:5.621433734893799
epoch10: step2000/4680
step 25000: accuracy:0.13600000739097595, confidence:0.9634818434715271, loss:6.721743583679199
epoch10: step2500/4680
step 30000: accuracy:0.0989999994635582, confidence:0.9974683523178101, loss:7.894949436187744
epoch10: step3000/4680
step 35000: accuracy:0.09300000220537186, confidence:0.918935239315033, loss:5.442551612854004
epoch10: step3500/4680
step 40000: accuracy:0.08299999684095383, confidence:0.9783776998519897, loss:7.173051834106445
epoch10: step4000/4680
step 45000: accuracy:0.13600000739097595, confidence:0.9014551043510437, loss:5.085044860839844
epoch10: step4500/4680
step 0: accuracy:0.1420000046491623, confidence:0.910874605178833, loss:4.9908127784729
epoch11: step0/4680
step 5500: accuracy:0.20200000703334808, confidence:0.81749027967453, loss:3.890105724334717
epoch11: step500/4680
step 11000: accuracy:0.0989999994635582, confidence:0.990673303604126, loss:8.210697174072266
epoch11: step1000/4680
step 16500: accuracy:0.10100000351667404, confidence:0.964269757270813, loss:6.4125566482543945
epoch11: step1500/4680
step 22000: accuracy:0.10100000351667404, confidence:0.9433523416519165, loss:5.74942684173584
epoch11: step2000/4680
step 27500: accuracy:0.1679999977350235, confidence:0.9437782168388367, loss:6.3536529541015625
epoch11: step2500/4680
step 33000: accuracy:0.10400000214576721, confidence:0.9956722259521484, loss:7.900448322296143
epoch11: step3000/4680
step 38500: accuracy:0.11599999666213989, confidence:0.9019007682800293, loss:5.290536880493164
epoch11: step3500/4680
step 44000: accuracy:0.11999999731779099, confidence:0.9491298198699951, loss:6.663898944854736
epoch11: step4000/4680
step 49500: accuracy:0.15299999713897705, confidence:0.9021096229553223, loss:5.331881999969482
epoch11: step4500/4680
step 0: accuracy:0.14499999582767487, confidence:0.9064627885818481, loss:5.2483439445495605
epoch12: step0/4680
step 6000: accuracy:0.20399999618530273, confidence:0.8515080809593201, loss:4.154284477233887
epoch12: step500/4680
step 12000: accuracy:0.08699999749660492, confidence:0.9918409585952759, loss:9.283822059631348
epoch12: step1000/4680
step 18000: accuracy:0.1080000028014183, confidence:0.9759406447410583, loss:7.6416425704956055
epoch12: step1500/4680
step 24000: accuracy:0.11299999803304672, confidence:0.9489216208457947, loss:5.597825527191162
epoch12: step2000/4680
step 30000: accuracy:0.11100000143051147, confidence:0.9820058345794678, loss:8.341333389282227
epoch12: step2500/4680
step 36000: accuracy:0.11299999803304672, confidence:0.9934421181678772, loss:7.480190753936768
epoch12: step3000/4680
step 42000: accuracy:0.11599999666213989, confidence:0.8923054337501526, loss:5.195759296417236
epoch12: step3500/4680
step 48000: accuracy:0.13899999856948853, confidence:0.9100654721260071, loss:5.753920555114746
epoch12: step4000/4680
step 54000: accuracy:0.17499999701976776, confidence:0.8986315727233887, loss:5.081263065338135
epoch12: step4500/4680
step 0: accuracy:0.13699999451637268, confidence:0.8970425128936768, loss:5.348734378814697
epoch13: step0/4680
step 6500: accuracy:0.23499999940395355, confidence:0.8484123945236206, loss:4.170520782470703
epoch13: step500/4680
step 13000: accuracy:0.09700000286102295, confidence:0.9768498539924622, loss:8.522245407104492
epoch13: step1000/4680
step 19500: accuracy:0.10300000011920929, confidence:0.9825430512428284, loss:7.6490254402160645
epoch13: step1500/4680
step 26000: accuracy:0.09700000286102295, confidence:0.9017873406410217, loss:4.8661956787109375
epoch13: step2000/4680
step 32500: accuracy:0.12200000137090683, confidence:0.9721245169639587, loss:8.113597869873047
epoch13: step2500/4680
step 39000: accuracy:0.12600000202655792, confidence:0.9974617958068848, loss:8.706747055053711
epoch13: step3000/4680
step 45500: accuracy:0.13500000536441803, confidence:0.9070668816566467, loss:5.270566463470459
epoch13: step3500/4680
step 52000: accuracy:0.18799999356269836, confidence:0.8836034536361694, loss:5.047744274139404
epoch13: step4000/4680
step 58500: accuracy:0.17800000309944153, confidence:0.9030628800392151, loss:5.314727306365967
epoch13: step4500/4680
step 0: accuracy:0.1770000010728836, confidence:0.8989526629447937, loss:5.143754482269287
epoch14: step0/4680
step 7000: accuracy:0.2240000069141388, confidence:0.8526883125305176, loss:4.504626274108887
epoch14: step500/4680
step 14000: accuracy:0.10999999940395355, confidence:0.9752801060676575, loss:9.01036548614502
epoch14: step1000/4680
step 21000: accuracy:0.10400000214576721, confidence:0.974465012550354, loss:7.501622676849365
epoch14: step1500/4680
step 28000: accuracy:0.09799999743700027, confidence:0.904917299747467, loss:4.997646808624268
epoch14: step2000/4680
step 35000: accuracy:0.1509999930858612, confidence:0.9650729894638062, loss:7.809182643890381
epoch14: step2500/4680
step 42000: accuracy:0.1120000034570694, confidence:0.9943653345108032, loss:8.14925479888916
epoch14: step3000/4680
step 49000: accuracy:0.09600000083446503, confidence:0.9282247424125671, loss:5.790419101715088
epoch14: step3500/4680
step 56000: accuracy:0.14399999380111694, confidence:0.905094563961029, loss:5.536978244781494
epoch14: step4000/4680
step 63000: accuracy:0.1770000010728836, confidence:0.9042014479637146, loss:4.8227410316467285
epoch14: step4500/4680
step 0: accuracy:0.16899999976158142, confidence:0.899788498878479, loss:4.713701248168945
epoch15: step0/4680
step 7500: accuracy:0.20600000023841858, confidence:0.8529371023178101, loss:4.3974480628967285
epoch15: step500/4680
step 15000: accuracy:0.0820000022649765, confidence:0.9836227893829346, loss:9.325146675109863
epoch15: step1000/4680
step 22500: accuracy:0.13099999725818634, confidence:0.9437798857688904, loss:6.600559711456299
epoch15: step1500/4680
step 30000: accuracy:0.15299999713897705, confidence:0.8978268504142761, loss:5.1045660972595215
epoch15: step2000/4680
step 37500: accuracy:0.12099999934434891, confidence:0.993971049785614, loss:9.835365295410156
epoch15: step2500/4680
step 45000: accuracy:0.09000000357627869, confidence:0.9925099015235901, loss:7.934451580047607
epoch15: step3000/4680
step 52500: accuracy:0.13300000131130219, confidence:0.8951600193977356, loss:5.403666019439697
epoch15: step3500/4680
step 60000: accuracy:0.17800000309944153, confidence:0.9000226259231567, loss:5.264133453369141
epoch15: step4000/4680
step 67500: accuracy:0.1679999977350235, confidence:0.8961309790611267, loss:5.212119102478027
epoch15: step4500/4680
step 0: accuracy:0.1720000058412552, confidence:0.8964005708694458, loss:5.034019470214844
epoch16: step0/4680
step 8000: accuracy:0.2409999966621399, confidence:0.8729866743087769, loss:4.618335247039795
epoch16: step500/4680
step 16000: accuracy:0.1120000034570694, confidence:0.9735804796218872, loss:8.69538402557373
epoch16: step1000/4680
step 24000: accuracy:0.1469999998807907, confidence:0.9300543069839478, loss:6.767927169799805
epoch16: step1500/4680
step 32000: accuracy:0.17900000512599945, confidence:0.851627767086029, loss:4.5037336349487305
epoch16: step2000/4680
step 40000: accuracy:0.11599999666213989, confidence:0.9891407489776611, loss:10.028417587280273
epoch16: step2500/4680
step 48000: accuracy:0.1120000034570694, confidence:0.9958866238594055, loss:7.801865100860596
epoch16: step3000/4680
step 56000: accuracy:0.11599999666213989, confidence:0.9192022681236267, loss:5.320803642272949
epoch16: step3500/4680
step 64000: accuracy:0.164000004529953, confidence:0.8889784216880798, loss:5.138981819152832
epoch16: step4000/4680
step 72000: accuracy:0.1599999964237213, confidence:0.882003903388977, loss:5.0179443359375
epoch16: step4500/4680
step 0: accuracy:0.20399999618530273, confidence:0.8834643363952637, loss:4.6856489181518555
epoch17: step0/4680
step 8500: accuracy:0.24199999868869781, confidence:0.8633347749710083, loss:4.535150527954102
epoch17: step500/4680
step 17000: accuracy:0.10100000351667404, confidence:0.9893232583999634, loss:10.224909782409668
epoch17: step1000/4680
step 25500: accuracy:0.15700000524520874, confidence:0.9170560836791992, loss:7.342217922210693
epoch17: step1500/4680
step 34000: accuracy:0.2029999941587448, confidence:0.8131536245346069, loss:4.039502143859863
epoch17: step2000/4680
step 42500: accuracy:0.1860000044107437, confidence:0.9426096081733704, loss:7.440200328826904
epoch17: step2500/4680
step 51000: accuracy:0.09700000286102295, confidence:0.9993288516998291, loss:10.729703903198242
epoch17: step3000/4680
step 59500: accuracy:0.10999999940395355, confidence:0.9202094674110413, loss:5.34304141998291
epoch17: step3500/4680
step 68000: accuracy:0.22300000488758087, confidence:0.8584191799163818, loss:4.420975685119629
epoch17: step4000/4680
step 76500: accuracy:0.17599999904632568, confidence:0.9001679420471191, loss:5.091055393218994
epoch17: step4500/4680
step 0: accuracy:0.1720000058412552, confidence:0.8927916288375854, loss:5.061985015869141
epoch18: step0/4680
step 9000: accuracy:0.23600000143051147, confidence:0.8696609139442444, loss:4.815876483917236
epoch18: step500/4680
step 18000: accuracy:0.11400000005960464, confidence:0.9762647747993469, loss:9.617341041564941
epoch18: step1000/4680
step 27000: accuracy:0.11400000005960464, confidence:0.9480139017105103, loss:8.85240650177002
epoch18: step1500/4680
step 36000: accuracy:0.1720000058412552, confidence:0.8834207057952881, loss:5.21196985244751
epoch18: step2000/4680
step 45000: accuracy:0.10700000077486038, confidence:0.9692570567131042, loss:7.317334175109863
epoch18: step2500/4680
step 54000: accuracy:0.07599999755620956, confidence:0.9928397536277771, loss:8.562560081481934
epoch18: step3000/4680
step 63000: accuracy:0.10400000214576721, confidence:0.9230121970176697, loss:5.745968341827393
epoch18: step3500/4680
step 72000: accuracy:0.24300000071525574, confidence:0.8307594060897827, loss:4.177682399749756
epoch18: step4000/4680
step 81000: accuracy:0.1889999955892563, confidence:0.9008129835128784, loss:5.3391032218933105
epoch18: step4500/4680
step 0: accuracy:0.210999995470047, confidence:0.8907656669616699, loss:4.740772247314453
epoch19: step0/4680
step 9500: accuracy:0.29600000381469727, confidence:0.8443132638931274, loss:4.0016608238220215
epoch19: step500/4680
step 19000: accuracy:0.1469999998807907, confidence:0.9697023034095764, loss:8.331836700439453
epoch19: step1000/4680
step 28500: accuracy:0.15000000596046448, confidence:0.9367541670799255, loss:8.316908836364746
epoch19: step1500/4680
step 38000: accuracy:0.17499999701976776, confidence:0.892134428024292, loss:5.490863800048828
epoch19: step2000/4680
step 47500: accuracy:0.16699999570846558, confidence:0.9615329504013062, loss:7.414290904998779
epoch19: step2500/4680
step 57000: accuracy:0.11800000071525574, confidence:0.9969015121459961, loss:9.01113224029541
epoch19: step3000/4680
step 66500: accuracy:0.13099999725818634, confidence:0.8983194231987, loss:5.188178539276123
epoch19: step3500/4680
step 76000: accuracy:0.2290000021457672, confidence:0.8495124578475952, loss:4.353824615478516
epoch19: step4000/4680
step 85500: accuracy:0.16699999570846558, confidence:0.898991584777832, loss:5.4275312423706055
epoch19: step4500/4680
step 0: accuracy:0.16599999368190765, confidence:0.88861083984375, loss:5.197345733642578
epoch20: step0/4680
step 10000: accuracy:0.26600000262260437, confidence:0.8418850302696228, loss:4.267519950866699
epoch20: step500/4680
step 20000: accuracy:0.11400000005960464, confidence:0.9706411957740784, loss:8.505529403686523
epoch20: step1000/4680
step 30000: accuracy:0.12399999797344208, confidence:0.9561641812324524, loss:8.884876251220703
epoch20: step1500/4680
step 40000: accuracy:0.21899999678134918, confidence:0.8541715145111084, loss:4.535952568054199
epoch20: step2000/4680
step 50000: accuracy:0.1850000023841858, confidence:0.9428561329841614, loss:6.855803489685059
epoch20: step2500/4680
step 60000: accuracy:0.1080000028014183, confidence:0.9995532035827637, loss:12.9713134765625
epoch20: step3000/4680
step 70000: accuracy:0.1080000028014183, confidence:0.9556078910827637, loss:6.198182582855225
epoch20: step3500/4680
step 80000: accuracy:0.3009999990463257, confidence:0.8273423314094543, loss:3.840365171432495
epoch20: step4000/4680
step 90000: accuracy:0.21199999749660492, confidence:0.9005823731422424, loss:4.913671016693115
epoch20: step4500/4680
step 0: accuracy:0.19900000095367432, confidence:0.8805050849914551, loss:4.700045108795166
epoch21: step0/4680
step 10500: accuracy:0.2280000001192093, confidence:0.8743759989738464, loss:4.681258678436279
epoch21: step500/4680
step 21000: accuracy:0.13099999725818634, confidence:0.9743950366973877, loss:9.504612922668457
epoch21: step1000/4680
step 31500: accuracy:0.15399999916553497, confidence:0.9305649995803833, loss:9.072629928588867
epoch21: step1500/4680
step 42000: accuracy:0.2460000067949295, confidence:0.8353434801101685, loss:4.32597541809082
epoch21: step2000/4680
step 52500: accuracy:0.20800000429153442, confidence:0.9137989282608032, loss:5.610320091247559
epoch21: step2500/4680
step 63000: accuracy:0.10899999737739563, confidence:0.9990077018737793, loss:12.099739074707031
epoch21: step3000/4680
step 73500: accuracy:0.12200000137090683, confidence:0.8980339765548706, loss:5.2620673179626465
epoch21: step3500/4680
step 84000: accuracy:0.26600000262260437, confidence:0.8307961821556091, loss:4.261964797973633
epoch21: step4000/4680
step 94500: accuracy:0.17599999904632568, confidence:0.9058046936988831, loss:5.3064751625061035
epoch21: step4500/4680
step 0: accuracy:0.17599999904632568, confidence:0.8913588523864746, loss:4.878430366516113
epoch22: step0/4680
step 11000: accuracy:0.2800000011920929, confidence:0.8534179329872131, loss:4.478557109832764
epoch22: step500/4680
step 22000: accuracy:0.13500000536441803, confidence:0.9745571613311768, loss:7.496225357055664
epoch22: step1000/4680
step 33000: accuracy:0.16699999570846558, confidence:0.9346479773521423, loss:8.495627403259277
epoch22: step1500/4680
step 44000: accuracy:0.22599999606609344, confidence:0.8321157693862915, loss:4.636490345001221
epoch22: step2000/4680
step 55000: accuracy:0.18000000715255737, confidence:0.919409990310669, loss:5.829193592071533
epoch22: step2500/4680
step 66000: accuracy:0.10199999809265137, confidence:0.9970641136169434, loss:10.454119682312012
epoch22: step3000/4680
step 77000: accuracy:0.12399999797344208, confidence:0.9425656199455261, loss:6.070053577423096
epoch22: step3500/4680
step 88000: accuracy:0.32600000500679016, confidence:0.8169064521789551, loss:3.6874818801879883
epoch22: step4000/4680
step 99000: accuracy:0.20000000298023224, confidence:0.8877772688865662, loss:4.773927688598633
epoch22: step4500/4680
step 0: accuracy:0.20600000023841858, confidence:0.8783081769943237, loss:4.707531452178955
epoch23: step0/4680
step 11500: accuracy:0.2770000100135803, confidence:0.8624498248100281, loss:4.943180084228516
epoch23: step500/4680
step 23000: accuracy:0.1589999943971634, confidence:0.9616458415985107, loss:6.959972381591797
epoch23: step1000/4680
step 34500: accuracy:0.17000000178813934, confidence:0.9170615077018738, loss:7.219075679779053
epoch23: step1500/4680
step 46000: accuracy:0.257999986410141, confidence:0.8320740461349487, loss:4.2406768798828125
epoch23: step2000/4680
step 57500: accuracy:0.2150000035762787, confidence:0.8921728134155273, loss:4.802934646606445
epoch23: step2500/4680
step 69000: accuracy:0.10100000351667404, confidence:0.9971899390220642, loss:11.372096061706543
epoch23: step3000/4680
step 80500: accuracy:0.11800000071525574, confidence:0.9164849519729614, loss:5.703357219696045
epoch23: step3500/4680
step 92000: accuracy:0.2759999930858612, confidence:0.8299278616905212, loss:4.086040496826172
epoch23: step4000/4680
step 103500: accuracy:0.20399999618530273, confidence:0.8955557346343994, loss:4.712137222290039
epoch23: step4500/4680
step 0: accuracy:0.2029999941587448, confidence:0.8766679167747498, loss:4.500680923461914
epoch24: step0/4680
step 12000: accuracy:0.31700000166893005, confidence:0.869979202747345, loss:4.727877140045166
epoch24: step500/4680
step 24000: accuracy:0.1459999978542328, confidence:0.9687446355819702, loss:8.268377304077148
epoch24: step1000/4680
step 36000: accuracy:0.14300000667572021, confidence:0.9477710723876953, loss:9.171785354614258
epoch24: step1500/4680
step 48000: accuracy:0.22699999809265137, confidence:0.8712870478630066, loss:4.5642595291137695
epoch24: step2000/4680
step 60000: accuracy:0.23199999332427979, confidence:0.9108702540397644, loss:4.625423908233643
epoch24: step2500/4680
step 72000: accuracy:0.10199999809265137, confidence:0.9879387617111206, loss:8.834548950195312
epoch24: step3000/4680
step 84000: accuracy:0.10100000351667404, confidence:0.9206316471099854, loss:5.909030914306641
epoch24: step3500/4680
step 96000: accuracy:0.3009999990463257, confidence:0.8517939448356628, loss:3.996345281600952
epoch24: step4000/4680
step 108000: accuracy:0.20800000429153442, confidence:0.8706012964248657, loss:4.821160316467285
epoch24: step4500/4680
step 0: accuracy:0.25699999928474426, confidence:0.8625695705413818, loss:4.421020984649658
epoch25: step0/4680
step 12500: accuracy:0.3019999861717224, confidence:0.8701792359352112, loss:4.417725086212158
epoch25: step500/4680
step 25000: accuracy:0.13600000739097595, confidence:0.9669262766838074, loss:7.5709686279296875
epoch25: step1000/4680
step 37500: accuracy:0.16200000047683716, confidence:0.9364806413650513, loss:8.315353393554688
epoch25: step1500/4680
step 50000: accuracy:0.24400000274181366, confidence:0.8301613926887512, loss:4.296325206756592
epoch25: step2000/4680
step 62500: accuracy:0.16099999845027924, confidence:0.9669501781463623, loss:7.938848495483398
epoch25: step2500/4680
step 75000: accuracy:0.09200000017881393, confidence:0.9927489757537842, loss:8.621399879455566
epoch25: step3000/4680
step 87500: accuracy:0.16500000655651093, confidence:0.9009628295898438, loss:5.402939796447754
epoch25: step3500/4680
step 100000: accuracy:0.24400000274181366, confidence:0.8515552282333374, loss:4.261895179748535
epoch25: step4000/4680
step 112500: accuracy:0.25, confidence:0.8446698188781738, loss:4.495449066162109
epoch25: step4500/4680
step 0: accuracy:0.2840000092983246, confidence:0.8611614108085632, loss:4.2512335777282715
epoch26: step0/4680
step 13000: accuracy:0.3160000145435333, confidence:0.8641142845153809, loss:5.020935535430908
epoch26: step500/4680
step 26000: accuracy:0.13300000131130219, confidence:0.9685483574867249, loss:8.610532760620117
epoch26: step1000/4680
step 39000: accuracy:0.16500000655651093, confidence:0.9309176802635193, loss:8.604483604431152
epoch26: step1500/4680
step 52000: accuracy:0.2720000147819519, confidence:0.8189368844032288, loss:4.04594612121582
epoch26: step2000/4680
step 65000: accuracy:0.1899999976158142, confidence:0.9194746017456055, loss:5.913184642791748
epoch26: step2500/4680
step 78000: accuracy:0.09099999815225601, confidence:0.9995063543319702, loss:13.736066818237305
epoch26: step3000/4680
step 91000: accuracy:0.10499999672174454, confidence:0.9395069479942322, loss:6.233476638793945
epoch26: step3500/4680
step 104000: accuracy:0.3149999976158142, confidence:0.8355509638786316, loss:3.7535743713378906
epoch26: step4000/4680
step 117000: accuracy:0.20900000631809235, confidence:0.8663139939308167, loss:4.579514026641846
epoch26: step4500/4680
step 0: accuracy:0.23899999260902405, confidence:0.8608618378639221, loss:4.307843208312988
epoch27: step0/4680
step 13500: accuracy:0.28299999237060547, confidence:0.8737775683403015, loss:4.835596084594727
epoch27: step500/4680
step 27000: accuracy:0.1509999930858612, confidence:0.964769184589386, loss:8.198185920715332
epoch27: step1000/4680
step 40500: accuracy:0.16899999976158142, confidence:0.9311911463737488, loss:7.846940040588379
epoch27: step1500/4680
step 54000: accuracy:0.18799999356269836, confidence:0.8579502105712891, loss:5.068912029266357
epoch27: step2000/4680
step 67500: accuracy:0.210999995470047, confidence:0.9122352004051208, loss:5.752048492431641
epoch27: step2500/4680
step 81000: accuracy:0.09799999743700027, confidence:0.994144082069397, loss:10.037405967712402
epoch27: step3000/4680
step 94500: accuracy:0.11699999868869781, confidence:0.9166070818901062, loss:6.436099529266357
epoch27: step3500/4680
step 108000: accuracy:0.2669999897480011, confidence:0.8397700190544128, loss:4.269308090209961
epoch27: step4000/4680
step 121500: accuracy:0.22499999403953552, confidence:0.8730177283287048, loss:4.391750812530518
epoch27: step4500/4680
step 0: accuracy:0.23600000143051147, confidence:0.8526815176010132, loss:4.195468902587891
epoch28: step0/4680
step 14000: accuracy:0.27399998903274536, confidence:0.8793554306030273, loss:5.473833084106445
epoch28: step500/4680
step 28000: accuracy:0.14900000393390656, confidence:0.9720064401626587, loss:8.228327751159668
epoch28: step1000/4680
step 42000: accuracy:0.15000000596046448, confidence:0.9278913736343384, loss:9.33722972869873
epoch28: step1500/4680
step 56000: accuracy:0.28600001335144043, confidence:0.8342797756195068, loss:4.175926208496094
epoch28: step2000/4680
step 70000: accuracy:0.21299999952316284, confidence:0.8776064515113831, loss:5.109888076782227
epoch28: step2500/4680
step 84000: accuracy:0.08500000089406967, confidence:0.9963894486427307, loss:11.270430564880371
epoch28: step3000/4680
step 98000: accuracy:0.1289999932050705, confidence:0.921630859375, loss:6.3780622482299805
epoch28: step3500/4680
step 112000: accuracy:0.3070000112056732, confidence:0.8494225144386292, loss:4.078092098236084
epoch28: step4000/4680
step 126000: accuracy:0.24400000274181366, confidence:0.8759697079658508, loss:4.666716575622559
epoch28: step4500/4680
step 0: accuracy:0.2680000066757202, confidence:0.862984299659729, loss:4.363322734832764
epoch29: step0/4680
step 14500: accuracy:0.3100000023841858, confidence:0.8693423867225647, loss:4.495667934417725
epoch29: step500/4680
step 29000: accuracy:0.15000000596046448, confidence:0.962053120136261, loss:7.784336090087891
epoch29: step1000/4680
step 43500: accuracy:0.23100000619888306, confidence:0.9049795269966125, loss:7.250669956207275
epoch29: step1500/4680
step 58000: accuracy:0.2460000067949295, confidence:0.837408185005188, loss:4.849737167358398
epoch29: step2000/4680
step 72500: accuracy:0.2029999941587448, confidence:0.9236078858375549, loss:5.957460880279541
epoch29: step2500/4680
step 87000: accuracy:0.08500000089406967, confidence:0.9813377857208252, loss:8.747176170349121
epoch29: step3000/4680
step 101500: accuracy:0.14300000667572021, confidence:0.9086059927940369, loss:6.253155708312988
epoch29: step3500/4680
step 116000: accuracy:0.28700000047683716, confidence:0.8641961812973022, loss:4.52042818069458
epoch29: step4000/4680
step 130500: accuracy:0.23600000143051147, confidence:0.8757144808769226, loss:4.450685501098633
epoch29: step4500/4680
2018-06-15 20:11:02.159518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 20:11:02.159757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
2018-06-15 20:11:04.388037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
CHEKPOINT DIR: uniform
model has been loaded from mnist_classifier.pkl
---------
Variables: name (type shape) [size]
---------
Variable:0 (float32_ref 5x5x1x32) [800, bytes: 3200]
Variable_1:0 (float32_ref 32) [32, bytes: 128]
Variable_2:0 (float32_ref 5x5x32x64) [51200, bytes: 204800]
Variable_3:0 (float32_ref 64) [64, bytes: 256]
Variable_4:0 (float32_ref 3136x1024) [3211264, bytes: 12845056]
Variable_5:0 (float32_ref 1024) [1024, bytes: 4096]
Variable_6:0 (float32_ref 1024x10) [10240, bytes: 40960]
Variable_7:0 (float32_ref 10) [10, bytes: 40]
discriminator/d_conv1/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]
discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]
discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]
discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]
generator/g_fc1/Matrix:0 (float32_ref 74x1024) [75776, bytes: 303104]
generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]
generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]
generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]
classifier/c_fc1/Matrix:0 (float32_ref 1024x64) [65536, bytes: 262144]
classifier/c_fc1/bias:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/beta:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/gamma:0 (float32_ref 64) [64, bytes: 256]
classifier/c_fc2/Matrix:0 (float32_ref 64x12) [768, bytes: 3072]
classifier/c_fc2/bias:0 (float32_ref 12) [12, bytes: 48]
Total size of variables: 16552792
Total bytes of variables: 66211168
 [*] Reading checkpoints...
 [*] Success to read MultiModalInfoGAN.model-43721
 [*] Load SUCCESS


SAMPLES SIZE=4680,LABELS=299520


SAVED TRAINING SET generated_training_set_mnist_UniformSample_mu_0_sigma_0.15_czrc_czcc_rzcc_rzrc
 [*] Training finished!
 [*] Testing finished!
2018-06-15 20:11:54.783858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 20:11:54.784160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
model has been loaded from mnist_classifier.pkl
argmax:[4 4 4 ..., 4 4 4]
step 0: accuracy:0.0, confidence:0.9868118166923523, loss:13.050642013549805
Assinging:5
[   0  117    0    0 9883]
argmax:[5 5 5 ..., 5 5 5]
step 0: accuracy:0.0, confidence:0.979282796382904, loss:10.079038619995117
Assinging:6
[   0  234    0    0    0 9766]
argmax:[8 8 8 ..., 8 8 8]
step 0: accuracy:0.0, confidence:0.9999372959136963, loss:16.00054931640625
Assinging:9
[    0     0     0     0     0     0     0     0 10000]
argmax:[2 2 2 ..., 2 2 2]
step 0: accuracy:0.0, confidence:0.9992236495018005, loss:19.44969367980957
Assinging:3
[    0     0 10000]
argmax:[0 0 0 ..., 0 0 0]
step 0: accuracy:0.0, confidence:0.9998763799667358, loss:17.36652183532715
Assinging:1
[10000]
argmax:[1 1 1 ..., 1 1 1]
step 0: accuracy:0.0, confidence:0.9985288977622986, loss:14.410422325134277
Assinging:2
[    0 10000]
argmax:[9 9 9 ..., 9 9 9]
step 0: accuracy:0.9883000254631042, confidence:0.990810751914978, loss:0.016689734533429146
Assinging:10
[   0    0    0    0    0    0    0    0  117 9883]
argmax:[6 6 6 ..., 6 6 6]
step 0: accuracy:0.0, confidence:0.993722140789032, loss:21.713523864746094
Assinging:7
[   0  117    0    0    0    0 9883]
argmax:[7 7 7 ..., 7 7 7]
step 0: accuracy:0.0, confidence:0.9960007667541504, loss:12.1192626953125
Assinging:8
[    0     0     0     0     0     0     0 10000]
argmax:[3 3 3 ..., 3 3 3]
step 0: accuracy:0.0, confidence:0.9996283054351807, loss:16.491819381713867
Assinging:4
[    0     0     0 10000]
2018-06-15 20:12:17.618617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 20:12:17.618864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
step 0: accuracy:0.1080000028014183, confidence:0.9465721249580383, loss:10.103544235229492
epoch0: step0/4680
step 0: accuracy:0.08799999952316284, confidence:1.0, loss:42.06154251098633
epoch0: step500/4680
step 0: accuracy:0.09099999815225601, confidence:0.9999968409538269, loss:20.869789123535156
epoch0: step1000/4680
step 0: accuracy:0.10199999809265137, confidence:0.9999996423721313, loss:20.374561309814453
epoch0: step1500/4680
step 0: accuracy:0.10000000149011612, confidence:0.9999780654907227, loss:12.960150718688965
epoch0: step2000/4680
step 0: accuracy:0.0729999989271164, confidence:0.9999155402183533, loss:11.349532127380371
epoch0: step2500/4680
step 0: accuracy:0.11599999666213989, confidence:0.9999827742576599, loss:13.324702262878418
epoch0: step3000/4680
step 0: accuracy:0.11299999803304672, confidence:0.9999741911888123, loss:15.364564895629883
epoch0: step3500/4680
step 0: accuracy:0.0949999988079071, confidence:0.999686598777771, loss:10.00251293182373
epoch0: step4000/4680
step 0: accuracy:0.10499999672174454, confidence:0.9993998408317566, loss:9.484841346740723
epoch0: step4500/4680
step 0: accuracy:0.1080000028014183, confidence:0.9997286796569824, loss:10.281842231750488
epoch1: step0/4680
step 500: accuracy:0.10400000214576721, confidence:0.999999463558197, loss:17.51033592224121
epoch1: step500/4680
step 1000: accuracy:0.0820000022649765, confidence:0.9992056488990784, loss:10.147026062011719
epoch1: step1000/4680
step 1500: accuracy:0.10499999672174454, confidence:0.9997315406799316, loss:10.277298927307129
epoch1: step1500/4680
step 2000: accuracy:0.10400000214576721, confidence:0.9999644160270691, loss:13.437326431274414
epoch1: step2000/4680
step 2500: accuracy:0.08500000089406967, confidence:0.9999818205833435, loss:13.151520729064941
epoch1: step2500/4680
step 3000: accuracy:0.11100000143051147, confidence:0.9999997019767761, loss:16.32233238220215
epoch1: step3000/4680
step 3500: accuracy:0.11599999666213989, confidence:0.999926745891571, loss:12.495589256286621
epoch1: step3500/4680
step 4000: accuracy:0.08799999952316284, confidence:0.9998319149017334, loss:10.989574432373047
epoch1: step4000/4680
step 4500: accuracy:0.08699999749660492, confidence:0.9999852776527405, loss:13.898877143859863
epoch1: step4500/4680
step 0: accuracy:0.09600000083446503, confidence:0.9999822974205017, loss:13.363531112670898
epoch2: step0/4680
step 1000: accuracy:0.10499999672174454, confidence:0.9999819993972778, loss:14.230630874633789
epoch2: step500/4680
step 2000: accuracy:0.07599999755620956, confidence:0.9999723434448242, loss:14.131288528442383
epoch2: step1000/4680
step 3000: accuracy:0.09399999678134918, confidence:0.9995446801185608, loss:10.666067123413086
epoch2: step1500/4680
step 4000: accuracy:0.09200000017881393, confidence:0.999971866607666, loss:12.929787635803223
epoch2: step2000/4680
step 5000: accuracy:0.09600000083446503, confidence:0.9999579191207886, loss:12.426539421081543
epoch2: step2500/4680
step 6000: accuracy:0.12800000607967377, confidence:0.9999919533729553, loss:13.321250915527344
epoch2: step3000/4680
step 7000: accuracy:0.09000000357627869, confidence:0.9999170303344727, loss:12.138748168945312
epoch2: step3500/4680
step 8000: accuracy:0.1080000028014183, confidence:0.9998512864112854, loss:11.018232345581055
epoch2: step4000/4680
step 9000: accuracy:0.10599999874830246, confidence:0.999906063079834, loss:11.642271041870117
epoch2: step4500/4680
step 0: accuracy:0.09600000083446503, confidence:0.9999175071716309, loss:11.7951021194458
epoch3: step0/4680
step 1500: accuracy:0.10000000149011612, confidence:0.9999502897262573, loss:12.002613067626953
epoch3: step500/4680
step 3000: accuracy:0.11900000274181366, confidence:0.9998648166656494, loss:11.026041984558105
epoch3: step1000/4680
step 4500: accuracy:0.11699999868869781, confidence:0.999241054058075, loss:9.271864891052246
epoch3: step1500/4680
step 6000: accuracy:0.11100000143051147, confidence:0.9998412132263184, loss:10.441767692565918
epoch3: step2000/4680
step 7500: accuracy:0.07800000160932541, confidence:0.9983007311820984, loss:8.779412269592285
epoch3: step2500/4680
step 9000: accuracy:0.10300000011920929, confidence:0.9999067187309265, loss:11.182893753051758
epoch3: step3000/4680
step 10500: accuracy:0.0989999994635582, confidence:0.9999521970748901, loss:11.982695579528809
epoch3: step3500/4680
step 12000: accuracy:0.10499999672174454, confidence:0.9997844099998474, loss:10.027416229248047
epoch3: step4000/4680
step 13500: accuracy:0.1080000028014183, confidence:0.9998194575309753, loss:10.736031532287598
epoch3: step4500/4680
step 0: accuracy:0.10100000351667404, confidence:0.9998533725738525, loss:10.82575511932373
epoch4: step0/4680
step 2000: accuracy:0.10999999940395355, confidence:0.998628556728363, loss:8.842001914978027
epoch4: step500/4680
step 4000: accuracy:0.0820000022649765, confidence:0.9998689889907837, loss:11.117715835571289
epoch4: step1000/4680
step 6000: accuracy:0.0989999994635582, confidence:0.9992039799690247, loss:9.517688751220703
epoch4: step1500/4680
step 8000: accuracy:0.1080000028014183, confidence:0.9997233152389526, loss:10.192809104919434
epoch4: step2000/4680
step 10000: accuracy:0.09700000286102295, confidence:0.9985916614532471, loss:8.428571701049805
epoch4: step2500/4680
step 12000: accuracy:0.11699999868869781, confidence:0.9999018907546997, loss:10.883735656738281
epoch4: step3000/4680
step 14000: accuracy:0.09799999743700027, confidence:0.999641478061676, loss:10.104326248168945
epoch4: step3500/4680
step 16000: accuracy:0.10599999874830246, confidence:0.999422550201416, loss:9.044136047363281
epoch4: step4000/4680
step 18000: accuracy:0.10599999874830246, confidence:0.9998434782028198, loss:10.930486679077148
epoch4: step4500/4680
step 0: accuracy:0.10499999672174454, confidence:0.999844491481781, loss:10.748234748840332
epoch5: step0/4680
step 2500: accuracy:0.09799999743700027, confidence:0.9980576038360596, loss:8.911571502685547
epoch5: step500/4680
step 5000: accuracy:0.07900000363588333, confidence:0.9992752075195312, loss:9.592496871948242
epoch5: step1000/4680
step 7500: accuracy:0.09300000220537186, confidence:0.9985129237174988, loss:9.288130760192871
epoch5: step1500/4680
step 10000: accuracy:0.1120000034570694, confidence:0.9995419979095459, loss:9.551446914672852
epoch5: step2000/4680
step 12500: accuracy:0.0949999988079071, confidence:0.9995209574699402, loss:9.471503257751465
epoch5: step2500/4680
step 15000: accuracy:0.10199999809265137, confidence:0.9999276995658875, loss:11.949987411499023
epoch5: step3000/4680
step 17500: accuracy:0.10899999737739563, confidence:0.9996481537818909, loss:9.910408020019531
epoch5: step3500/4680
step 20000: accuracy:0.09700000286102295, confidence:0.9993353486061096, loss:9.03323745727539
epoch5: step4000/4680
step 22500: accuracy:0.09399999678134918, confidence:0.9996761679649353, loss:9.870020866394043
epoch5: step4500/4680
step 0: accuracy:0.10599999874830246, confidence:0.999764621257782, loss:9.884997367858887
epoch6: step0/4680
step 3000: accuracy:0.09399999678134918, confidence:0.9932258129119873, loss:7.707688808441162
epoch6: step500/4680
step 6000: accuracy:0.0820000022649765, confidence:0.9999144673347473, loss:11.585583686828613
epoch6: step1000/4680
step 9000: accuracy:0.08299999684095383, confidence:0.9990782737731934, loss:10.216874122619629
epoch6: step1500/4680
step 12000: accuracy:0.10199999809265137, confidence:0.998281717300415, loss:8.320582389831543
epoch6: step2000/4680
step 15000: accuracy:0.10899999737739563, confidence:0.9972327947616577, loss:7.8591628074646
epoch6: step2500/4680
step 18000: accuracy:0.125, confidence:0.9998533725738525, loss:11.016329765319824
epoch6: step3000/4680
step 21000: accuracy:0.10199999809265137, confidence:0.9991644620895386, loss:9.187004089355469
epoch6: step3500/4680
step 24000: accuracy:0.10899999737739563, confidence:0.998400866985321, loss:8.2033109664917
epoch6: step4000/4680
step 27000: accuracy:0.09700000286102295, confidence:0.9994327425956726, loss:9.48965835571289
epoch6: step4500/4680
step 0: accuracy:0.11299999803304672, confidence:0.9995629787445068, loss:9.288046836853027
epoch7: step0/4680
step 3500: accuracy:0.10999999940395355, confidence:0.9905056357383728, loss:7.075866222381592
epoch7: step500/4680
step 7000: accuracy:0.09600000083446503, confidence:0.9986427426338196, loss:9.015405654907227
epoch7: step1000/4680
step 10500: accuracy:0.09799999743700027, confidence:0.9975668787956238, loss:8.930451393127441
epoch7: step1500/4680
step 14000: accuracy:0.125, confidence:0.9981386661529541, loss:8.109930038452148
epoch7: step2000/4680
step 17500: accuracy:0.10599999874830246, confidence:0.9944375157356262, loss:7.517086029052734
epoch7: step2500/4680
step 21000: accuracy:0.09399999678134918, confidence:0.9997313022613525, loss:10.661169052124023
epoch7: step3000/4680
step 24500: accuracy:0.0860000029206276, confidence:0.99896240234375, loss:9.347773551940918
epoch7: step3500/4680
step 28000: accuracy:0.0989999994635582, confidence:0.9970806241035461, loss:7.5815253257751465
epoch7: step4000/4680
step 31500: accuracy:0.10100000351667404, confidence:0.9994058609008789, loss:9.503498077392578
epoch7: step4500/4680
step 0: accuracy:0.09200000017881393, confidence:0.9995126128196716, loss:9.640804290771484
epoch8: step0/4680
step 4000: accuracy:0.09399999678134918, confidence:0.9926553964614868, loss:7.4526472091674805
epoch8: step500/4680
step 8000: accuracy:0.08900000154972076, confidence:0.9973528385162354, loss:8.534124374389648
epoch8: step1000/4680
step 12000: accuracy:0.10300000011920929, confidence:0.9986577033996582, loss:9.025774955749512
epoch8: step1500/4680
step 16000: accuracy:0.09000000357627869, confidence:0.9964189529418945, loss:7.687426567077637
epoch8: step2000/4680
step 20000: accuracy:0.09000000357627869, confidence:0.9961785078048706, loss:7.968420028686523
epoch8: step2500/4680
step 24000: accuracy:0.1080000028014183, confidence:0.9998048543930054, loss:11.036352157592773
epoch8: step3000/4680
step 28000: accuracy:0.09099999815225601, confidence:0.9981908202171326, loss:8.402457237243652
epoch8: step3500/4680
step 32000: accuracy:0.09700000286102295, confidence:0.9959826469421387, loss:7.387872219085693
epoch8: step4000/4680
step 36000: accuracy:0.0949999988079071, confidence:0.9989989399909973, loss:8.770611763000488
epoch8: step4500/4680
step 0: accuracy:0.09799999743700027, confidence:0.9992275238037109, loss:8.900310516357422
epoch9: step0/4680
step 4500: accuracy:0.09000000357627869, confidence:0.9858461618423462, loss:6.635164737701416
epoch9: step500/4680
step 9000: accuracy:0.0860000029206276, confidence:0.9968405961990356, loss:8.467144012451172
epoch9: step1000/4680
step 13500: accuracy:0.10999999940395355, confidence:0.9940551519393921, loss:7.897040843963623
epoch9: step1500/4680
step 18000: accuracy:0.09000000357627869, confidence:0.9950095415115356, loss:7.654045581817627
epoch9: step2000/4680
step 22500: accuracy:0.09300000220537186, confidence:0.9939765334129333, loss:7.393208026885986
epoch9: step2500/4680
step 27000: accuracy:0.09799999743700027, confidence:0.9997298717498779, loss:10.547818183898926
epoch9: step3000/4680
step 31500: accuracy:0.09700000286102295, confidence:0.9986240267753601, loss:8.799396514892578
epoch9: step3500/4680
step 36000: accuracy:0.11100000143051147, confidence:0.9962153434753418, loss:7.364805698394775
epoch9: step4000/4680
step 40500: accuracy:0.11299999803304672, confidence:0.9975371360778809, loss:7.623706817626953
epoch9: step4500/4680
step 0: accuracy:0.10100000351667404, confidence:0.9987390041351318, loss:8.334085464477539
epoch10: step0/4680
step 5000: accuracy:0.07699999958276749, confidence:0.9766477346420288, loss:6.2029805183410645
epoch10: step500/4680
step 10000: accuracy:0.08500000089406967, confidence:0.9972701668739319, loss:8.336442947387695
epoch10: step1000/4680
step 15000: accuracy:0.10400000214576721, confidence:0.9970606565475464, loss:8.193836212158203
epoch10: step1500/4680
step 20000: accuracy:0.09300000220537186, confidence:0.9943119883537292, loss:7.20784330368042
epoch10: step2000/4680
step 25000: accuracy:0.10400000214576721, confidence:0.9672780632972717, loss:5.621504306793213
epoch10: step2500/4680
step 30000: accuracy:0.1120000034570694, confidence:0.9996607899665833, loss:9.978099822998047
epoch10: step3000/4680
step 35000: accuracy:0.10700000077486038, confidence:0.9949043989181519, loss:7.379621982574463
epoch10: step3500/4680
step 40000: accuracy:0.10000000149011612, confidence:0.9938870668411255, loss:6.961541652679443
epoch10: step4000/4680
step 45000: accuracy:0.09200000017881393, confidence:0.9945380091667175, loss:7.1156086921691895
epoch10: step4500/4680
step 0: accuracy:0.09700000286102295, confidence:0.9974260926246643, loss:7.721035957336426
epoch11: step0/4680
step 5500: accuracy:0.0949999988079071, confidence:0.940706729888916, loss:4.933841228485107
epoch11: step500/4680
step 11000: accuracy:0.07599999755620956, confidence:0.9929014444351196, loss:7.589611530303955
epoch11: step1000/4680
step 16500: accuracy:0.10400000214576721, confidence:0.9925484657287598, loss:7.100700855255127
epoch11: step1500/4680
step 22000: accuracy:0.09700000286102295, confidence:0.9875734448432922, loss:6.336393356323242
epoch11: step2000/4680
step 27500: accuracy:0.09399999678134918, confidence:0.9547516703605652, loss:5.415262699127197
epoch11: step2500/4680
step 33000: accuracy:0.12300000339746475, confidence:0.9995125532150269, loss:9.28353214263916
epoch11: step3000/4680
step 38500: accuracy:0.10000000149011612, confidence:0.9834214448928833, loss:6.132347583770752
epoch11: step3500/4680
step 44000: accuracy:0.0949999988079071, confidence:0.9679518938064575, loss:5.6674299240112305
epoch11: step4000/4680
step 49500: accuracy:0.10199999809265137, confidence:0.9813069701194763, loss:6.789823055267334
epoch11: step4500/4680
step 0: accuracy:0.1120000034570694, confidence:0.9907870888710022, loss:7.480068206787109
epoch12: step0/4680
step 6000: accuracy:0.0989999994635582, confidence:0.8375810384750366, loss:3.993957042694092
epoch12: step500/4680
step 12000: accuracy:0.10199999809265137, confidence:0.9430801868438721, loss:5.393354415893555
epoch12: step1000/4680
step 18000: accuracy:0.09300000220537186, confidence:0.9899044036865234, loss:7.210946559906006
epoch12: step1500/4680
step 24000: accuracy:0.08799999952316284, confidence:0.9810067415237427, loss:5.781287670135498
epoch12: step2000/4680
step 30000: accuracy:0.09700000286102295, confidence:0.9675819873809814, loss:5.426611423492432
epoch12: step2500/4680
step 36000: accuracy:0.11100000143051147, confidence:0.9991047382354736, loss:8.5259370803833
epoch12: step3000/4680
step 42000: accuracy:0.0989999994635582, confidence:0.9901148080825806, loss:6.626035690307617
epoch12: step3500/4680
step 48000: accuracy:0.09600000083446503, confidence:0.9411166906356812, loss:4.811306953430176
epoch12: step4000/4680
step 54000: accuracy:0.16300000250339508, confidence:0.9320430159568787, loss:5.516961574554443
epoch12: step4500/4680
step 0: accuracy:0.10899999737739563, confidence:0.9612382054328918, loss:6.687834739685059
epoch13: step0/4680
step 6500: accuracy:0.1379999965429306, confidence:0.752483069896698, loss:3.580909490585327
epoch13: step500/4680
step 13000: accuracy:0.08799999952316284, confidence:0.8902439475059509, loss:4.745086193084717
epoch13: step1000/4680
step 19500: accuracy:0.0989999994635582, confidence:0.9953829050064087, loss:8.0460205078125
epoch13: step1500/4680
step 26000: accuracy:0.10300000011920929, confidence:0.9728911519050598, loss:5.4825520515441895
epoch13: step2000/4680
step 32500: accuracy:0.09799999743700027, confidence:0.9125047922134399, loss:4.773674964904785
epoch13: step2500/4680
step 39000: accuracy:0.1120000034570694, confidence:0.9991244077682495, loss:8.452468872070312
epoch13: step3000/4680
step 45500: accuracy:0.07699999958276749, confidence:0.9712541699409485, loss:5.716330051422119
epoch13: step3500/4680
step 52000: accuracy:0.10700000077486038, confidence:0.8760569095611572, loss:4.123032569885254
epoch13: step4000/4680
step 58500: accuracy:0.17800000309944153, confidence:0.9268635511398315, loss:5.100684642791748
epoch13: step4500/4680
step 0: accuracy:0.1550000011920929, confidence:0.9396933913230896, loss:5.831568717956543
epoch14: step0/4680
step 7000: accuracy:0.19499999284744263, confidence:0.7708673477172852, loss:3.4377148151397705
epoch14: step500/4680
step 14000: accuracy:0.09300000220537186, confidence:0.8875389695167542, loss:4.7901482582092285
epoch14: step1000/4680
step 21000: accuracy:0.08299999684095383, confidence:0.9915916919708252, loss:7.978950023651123
epoch14: step1500/4680
step 28000: accuracy:0.10999999940395355, confidence:0.9776208400726318, loss:5.664638042449951
epoch14: step2000/4680
step 35000: accuracy:0.1080000028014183, confidence:0.8809477090835571, loss:4.28177547454834
epoch14: step2500/4680
step 42000: accuracy:0.11500000208616257, confidence:0.9986814856529236, loss:8.079463005065918
epoch14: step3000/4680
step 49000: accuracy:0.11599999666213989, confidence:0.8891900777816772, loss:4.47977352142334
epoch14: step3500/4680
step 56000: accuracy:0.1850000023841858, confidence:0.8007669448852539, loss:3.4872069358825684
epoch14: step4000/4680
step 63000: accuracy:0.20100000500679016, confidence:0.9297327399253845, loss:5.346982479095459
epoch14: step4500/4680
step 0: accuracy:0.19499999284744263, confidence:0.9333410859107971, loss:5.773463249206543
epoch15: step0/4680
step 7500: accuracy:0.2540000081062317, confidence:0.7708755135536194, loss:3.0296411514282227
epoch15: step500/4680
step 15000: accuracy:0.11699999868869781, confidence:0.8673749566078186, loss:4.83043909072876
epoch15: step1000/4680
step 22500: accuracy:0.10000000149011612, confidence:0.9479790329933167, loss:6.218235015869141
epoch15: step1500/4680
step 30000: accuracy:0.09399999678134918, confidence:0.9507322907447815, loss:4.820821762084961
epoch15: step2000/4680
step 37500: accuracy:0.1080000028014183, confidence:0.9021013975143433, loss:4.579305648803711
epoch15: step2500/4680
step 45000: accuracy:0.10999999940395355, confidence:0.9956787824630737, loss:7.402773380279541
epoch15: step3000/4680
step 52500: accuracy:0.17299999296665192, confidence:0.874247133731842, loss:4.160742282867432
epoch15: step3500/4680
step 60000: accuracy:0.2240000069141388, confidence:0.7756118178367615, loss:3.1351230144500732
epoch15: step4000/4680
step 67500: accuracy:0.2240000069141388, confidence:0.9185912609100342, loss:5.202680587768555
epoch15: step4500/4680
step 0: accuracy:0.1940000057220459, confidence:0.9320838451385498, loss:5.648159980773926
epoch16: step0/4680
step 8000: accuracy:0.21400000154972076, confidence:0.7890552878379822, loss:3.2938754558563232
epoch16: step500/4680
step 16000: accuracy:0.09300000220537186, confidence:0.9480805993080139, loss:7.0289506912231445
epoch16: step1000/4680
step 24000: accuracy:0.11500000208616257, confidence:0.9098681211471558, loss:5.849903106689453
epoch16: step1500/4680
step 32000: accuracy:0.10599999874830246, confidence:0.8428804278373718, loss:3.708069086074829
epoch16: step2000/4680
step 40000: accuracy:0.12399999797344208, confidence:0.854983925819397, loss:4.474746227264404
epoch16: step2500/4680
step 48000: accuracy:0.12399999797344208, confidence:0.9992130398750305, loss:9.044808387756348
epoch16: step3000/4680
step 56000: accuracy:0.18700000643730164, confidence:0.8753940463066101, loss:4.096184253692627
epoch16: step3500/4680
step 64000: accuracy:0.289000004529953, confidence:0.753962516784668, loss:2.8158392906188965
epoch16: step4000/4680
step 72000: accuracy:0.2150000035762787, confidence:0.906808078289032, loss:5.089900493621826
epoch16: step4500/4680
step 0: accuracy:0.18799999356269836, confidence:0.9329697489738464, loss:5.811636924743652
epoch17: step0/4680
step 8500: accuracy:0.26600000262260437, confidence:0.8328340649604797, loss:3.259979486465454
epoch17: step500/4680
step 17000: accuracy:0.10599999874830246, confidence:0.9375852942466736, loss:5.8690619468688965
epoch17: step1000/4680
step 25500: accuracy:0.13300000131130219, confidence:0.8857276439666748, loss:5.619517803192139
epoch17: step1500/4680
step 34000: accuracy:0.10899999737739563, confidence:0.8527461886405945, loss:3.569154739379883
epoch17: step2000/4680
step 42500: accuracy:0.1720000058412552, confidence:0.7743514180183411, loss:3.7499992847442627
epoch17: step2500/4680
step 51000: accuracy:0.10700000077486038, confidence:0.9987506866455078, loss:9.542657852172852
epoch17: step3000/4680
step 59500: accuracy:0.24799999594688416, confidence:0.8399533033370972, loss:3.5436275005340576
epoch17: step3500/4680
step 68000: accuracy:0.34599998593330383, confidence:0.7623244524002075, loss:2.4693548679351807
epoch17: step4000/4680
step 76500: accuracy:0.21199999749660492, confidence:0.8704373836517334, loss:4.349905014038086
epoch17: step4500/4680
step 0: accuracy:0.2029999941587448, confidence:0.9443926811218262, loss:6.889482498168945
epoch18: step0/4680
step 9000: accuracy:0.29899999499320984, confidence:0.7986700534820557, loss:2.5481631755828857
epoch18: step500/4680
step 18000: accuracy:0.19099999964237213, confidence:0.8804012537002563, loss:5.053875923156738
epoch18: step1000/4680
step 27000: accuracy:0.18299999833106995, confidence:0.8680176734924316, loss:5.62350606918335
epoch18: step1500/4680
step 36000: accuracy:0.12399999797344208, confidence:0.817047119140625, loss:3.1808838844299316
epoch18: step2000/4680
step 45000: accuracy:0.20399999618530273, confidence:0.765232264995575, loss:3.4714200496673584
epoch18: step2500/4680
step 54000: accuracy:0.11299999803304672, confidence:0.9849627017974854, loss:7.101528644561768
epoch18: step3000/4680
step 63000: accuracy:0.2669999897480011, confidence:0.8291695713996887, loss:3.175616502761841
epoch18: step3500/4680
step 72000: accuracy:0.30300000309944153, confidence:0.7871031165122986, loss:2.757350444793701
epoch18: step4000/4680
step 81000: accuracy:0.22100000083446503, confidence:0.8634535074234009, loss:4.169524669647217
epoch18: step4500/4680
step 0: accuracy:0.19900000095367432, confidence:0.9027204513549805, loss:4.743673324584961
epoch19: step0/4680
step 9500: accuracy:0.32600000500679016, confidence:0.8462134003639221, loss:3.1488420963287354
epoch19: step500/4680
step 19000: accuracy:0.10400000214576721, confidence:0.9390099048614502, loss:5.953674793243408
epoch19: step1000/4680
step 28500: accuracy:0.2409999966621399, confidence:0.8387494087219238, loss:5.059116840362549
epoch19: step1500/4680
step 38000: accuracy:0.16200000047683716, confidence:0.7356496453285217, loss:2.8279943466186523
epoch19: step2000/4680
step 47500: accuracy:0.23800000548362732, confidence:0.7337210774421692, loss:3.2247347831726074
epoch19: step2500/4680
step 57000: accuracy:0.11500000208616257, confidence:0.9912281632423401, loss:8.017022132873535
epoch19: step3000/4680
step 66500: accuracy:0.27000001072883606, confidence:0.8368043303489685, loss:3.171478748321533
epoch19: step3500/4680
step 76000: accuracy:0.38999998569488525, confidence:0.7920499444007874, loss:2.3516175746917725
epoch19: step4000/4680
step 85500: accuracy:0.2160000056028366, confidence:0.8637767434120178, loss:4.205783843994141
epoch19: step4500/4680
step 0: accuracy:0.1899999976158142, confidence:0.9187561273574829, loss:5.014316082000732
epoch20: step0/4680
step 10000: accuracy:0.2720000147819519, confidence:0.8478269577026367, loss:3.2633168697357178
epoch20: step500/4680
step 20000: accuracy:0.0949999988079071, confidence:0.9745023846626282, loss:6.796392917633057
epoch20: step1000/4680
step 30000: accuracy:0.1850000023841858, confidence:0.8809505701065063, loss:6.034277439117432
epoch20: step1500/4680
step 40000: accuracy:0.20499999821186066, confidence:0.757267951965332, loss:2.7368929386138916
epoch20: step2000/4680
step 50000: accuracy:0.24699999392032623, confidence:0.7476553916931152, loss:3.275613307952881
epoch20: step2500/4680
step 60000: accuracy:0.09399999678134918, confidence:0.9942413568496704, loss:8.789199829101562
epoch20: step3000/4680
step 70000: accuracy:0.3199999928474426, confidence:0.8197906613349915, loss:2.959550380706787
epoch20: step3500/4680
step 80000: accuracy:0.3149999976158142, confidence:0.8181090354919434, loss:2.772981643676758
epoch20: step4000/4680
step 90000: accuracy:0.2409999966621399, confidence:0.853166401386261, loss:4.442038536071777
epoch20: step4500/4680
step 0: accuracy:0.23100000619888306, confidence:0.9039893746376038, loss:4.88725471496582
epoch21: step0/4680
step 10500: accuracy:0.36500000953674316, confidence:0.8166388869285583, loss:2.3046247959136963
epoch21: step500/4680
step 21000: accuracy:0.19699999690055847, confidence:0.8953062891960144, loss:4.946774482727051
epoch21: step1000/4680
step 31500: accuracy:0.20200000703334808, confidence:0.8782432675361633, loss:5.671102046966553
epoch21: step1500/4680
step 42000: accuracy:0.22200000286102295, confidence:0.7669933438301086, loss:2.9167468547821045
epoch21: step2000/4680
step 52500: accuracy:0.33000001311302185, confidence:0.7061580419540405, loss:2.727947235107422
epoch21: step2500/4680
step 63000: accuracy:0.12600000202655792, confidence:0.9971254467964172, loss:9.955375671386719
epoch21: step3000/4680
step 73500: accuracy:0.27799999713897705, confidence:0.7927196025848389, loss:2.9510645866394043
epoch21: step3500/4680
step 84000: accuracy:0.3790000081062317, confidence:0.7815362811088562, loss:2.29331111907959
epoch21: step4000/4680
step 94500: accuracy:0.21899999678134918, confidence:0.8519829511642456, loss:3.8344693183898926
epoch21: step4500/4680
step 0: accuracy:0.22100000083446503, confidence:0.9062644839286804, loss:4.509864807128906
epoch22: step0/4680
step 11000: accuracy:0.30000001192092896, confidence:0.8774118423461914, loss:3.5734219551086426
epoch22: step500/4680
step 22000: accuracy:0.14499999582767487, confidence:0.9261507391929626, loss:5.561311721801758
epoch22: step1000/4680
step 33000: accuracy:0.2240000069141388, confidence:0.8951818943023682, loss:5.73814582824707
epoch22: step1500/4680
step 44000: accuracy:0.25099998712539673, confidence:0.7507047057151794, loss:2.6567184925079346
epoch22: step2000/4680
step 55000: accuracy:0.3700000047683716, confidence:0.6659551858901978, loss:2.5397398471832275
epoch22: step2500/4680
step 66000: accuracy:0.11299999803304672, confidence:0.9962442517280579, loss:11.488724708557129
epoch22: step3000/4680
step 77000: accuracy:0.30300000309944153, confidence:0.8247464895248413, loss:3.0083751678466797
epoch22: step3500/4680
step 88000: accuracy:0.3089999854564667, confidence:0.7859789133071899, loss:2.5836942195892334
epoch22: step4000/4680
step 99000: accuracy:0.23899999260902405, confidence:0.8519697189331055, loss:3.763842821121216
epoch22: step4500/4680
step 0: accuracy:0.20499999821186066, confidence:0.9055539965629578, loss:4.848874568939209
epoch23: step0/4680
step 11500: accuracy:0.2630000114440918, confidence:0.8609794974327087, loss:3.865251064300537
epoch23: step500/4680
step 23000: accuracy:0.1340000033378601, confidence:0.9206779599189758, loss:5.866703033447266
epoch23: step1000/4680
step 34500: accuracy:0.2160000056028366, confidence:0.8886532187461853, loss:5.675155162811279
epoch23: step1500/4680
step 46000: accuracy:0.2980000078678131, confidence:0.7562180757522583, loss:2.6436855792999268
epoch23: step2000/4680
step 57500: accuracy:0.3779999911785126, confidence:0.6736613512039185, loss:2.515368700027466
epoch23: step2500/4680
step 69000: accuracy:0.10999999940395355, confidence:0.9910517930984497, loss:10.480449676513672
epoch23: step3000/4680
step 80500: accuracy:0.31299999356269836, confidence:0.8049672842025757, loss:2.905688524246216
epoch23: step3500/4680
step 92000: accuracy:0.37299999594688416, confidence:0.7860614657402039, loss:2.244621753692627
epoch23: step4000/4680
step 103500: accuracy:0.26899999380111694, confidence:0.8166447877883911, loss:3.192434072494507
epoch23: step4500/4680
step 0: accuracy:0.23899999260902405, confidence:0.8867445588111877, loss:4.058786392211914
epoch24: step0/4680
step 12000: accuracy:0.31200000643730164, confidence:0.8750405311584473, loss:3.21486234664917
epoch24: step500/4680
step 24000: accuracy:0.15000000596046448, confidence:0.9294560551643372, loss:5.7602033615112305
epoch24: step1000/4680
step 36000: accuracy:0.2329999953508377, confidence:0.8844113945960999, loss:5.531556129455566
epoch24: step1500/4680
step 48000: accuracy:0.31299999356269836, confidence:0.7471794486045837, loss:2.471026659011841
epoch24: step2000/4680
step 60000: accuracy:0.40799999237060547, confidence:0.6865139007568359, loss:2.431471347808838
epoch24: step2500/4680
step 72000: accuracy:0.12399999797344208, confidence:0.9610821008682251, loss:8.347834587097168
epoch24: step3000/4680
step 84000: accuracy:0.3160000145435333, confidence:0.8193572759628296, loss:2.8158562183380127
epoch24: step3500/4680
step 96000: accuracy:0.3840000033378601, confidence:0.8084222674369812, loss:2.3179163932800293
epoch24: step4000/4680
step 108000: accuracy:0.27300000190734863, confidence:0.8388711214065552, loss:3.275841474533081
epoch24: step4500/4680
step 0: accuracy:0.22200000286102295, confidence:0.8845109939575195, loss:4.338203430175781
epoch25: step0/4680
step 12500: accuracy:0.2930000126361847, confidence:0.8709493279457092, loss:3.2596752643585205
epoch25: step500/4680
step 25000: accuracy:0.09600000083446503, confidence:0.9455158710479736, loss:6.719167709350586
epoch25: step1000/4680
step 37500: accuracy:0.22100000083446503, confidence:0.8926723599433899, loss:6.067338943481445
epoch25: step1500/4680
step 50000: accuracy:0.24300000071525574, confidence:0.7641247510910034, loss:2.8773982524871826
epoch25: step2000/4680
step 62500: accuracy:0.3630000054836273, confidence:0.6731632947921753, loss:2.5886616706848145
epoch25: step2500/4680
step 75000: accuracy:0.13699999451637268, confidence:0.959617555141449, loss:7.5170207023620605
epoch25: step3000/4680
step 87500: accuracy:0.31200000643730164, confidence:0.8335676193237305, loss:3.0393974781036377
epoch25: step3500/4680
step 100000: accuracy:0.4050000011920929, confidence:0.8172920942306519, loss:2.3800904750823975
epoch25: step4000/4680
step 112500: accuracy:0.2529999911785126, confidence:0.8483561277389526, loss:3.3825035095214844
epoch25: step4500/4680
step 0: accuracy:0.24699999392032623, confidence:0.8874795436859131, loss:4.09641170501709
epoch26: step0/4680
step 13000: accuracy:0.27900001406669617, confidence:0.8768044710159302, loss:3.413707733154297
epoch26: step500/4680
step 26000: accuracy:0.11299999803304672, confidence:0.9503191113471985, loss:6.16053581237793
epoch26: step1000/4680
step 39000: accuracy:0.257999986410141, confidence:0.8775520920753479, loss:5.511740207672119
epoch26: step1500/4680
step 52000: accuracy:0.27399998903274536, confidence:0.7667168974876404, loss:2.8019766807556152
epoch26: step2000/4680
step 65000: accuracy:0.3610000014305115, confidence:0.7147956490516663, loss:2.734846591949463
epoch26: step2500/4680
step 78000: accuracy:0.12700000405311584, confidence:0.9752112030982971, loss:9.38060188293457
epoch26: step3000/4680
step 91000: accuracy:0.29499998688697815, confidence:0.8581259846687317, loss:3.23974609375
epoch26: step3500/4680
step 104000: accuracy:0.3970000147819519, confidence:0.8166728615760803, loss:2.3876936435699463
epoch26: step4000/4680
step 117000: accuracy:0.2770000100135803, confidence:0.791486918926239, loss:2.8041343688964844
epoch26: step4500/4680
step 0: accuracy:0.2529999911785126, confidence:0.8571072816848755, loss:3.6274333000183105
epoch27: step0/4680
step 13500: accuracy:0.2809999883174896, confidence:0.8893003463745117, loss:3.994985818862915
epoch27: step500/4680
step 27000: accuracy:0.1459999978542328, confidence:0.9277364611625671, loss:6.222173690795898
epoch27: step1000/4680
step 40500: accuracy:0.22100000083446503, confidence:0.8898763060569763, loss:6.218230724334717
epoch27: step1500/4680
step 54000: accuracy:0.27000001072883606, confidence:0.7764250040054321, loss:2.861565351486206
epoch27: step2000/4680
step 67500: accuracy:0.39500001072883606, confidence:0.7081088423728943, loss:2.5869345664978027
epoch27: step2500/4680
step 81000: accuracy:0.13899999856948853, confidence:0.9605430364608765, loss:8.12552261352539
epoch27: step3000/4680
step 94500: accuracy:0.29899999499320984, confidence:0.8285308480262756, loss:3.0980136394500732
epoch27: step3500/4680
step 108000: accuracy:0.40400001406669617, confidence:0.8021469712257385, loss:2.2465124130249023
epoch27: step4000/4680
step 121500: accuracy:0.3089999854564667, confidence:0.8116840720176697, loss:2.7815070152282715
epoch27: step4500/4680
step 0: accuracy:0.28200000524520874, confidence:0.8806663155555725, loss:3.7149710655212402
epoch28: step0/4680
step 14000: accuracy:0.27300000190734863, confidence:0.8856115937232971, loss:3.8806869983673096
epoch28: step500/4680
step 28000: accuracy:0.0989999994635582, confidence:0.9547531604766846, loss:6.193912029266357
epoch28: step1000/4680
step 42000: accuracy:0.24199999868869781, confidence:0.8841112852096558, loss:5.451893329620361
epoch28: step1500/4680
step 56000: accuracy:0.32199999690055847, confidence:0.7686573266983032, loss:2.6888341903686523
epoch28: step2000/4680
step 70000: accuracy:0.4269999861717224, confidence:0.6930920481681824, loss:2.444571018218994
epoch28: step2500/4680
step 84000: accuracy:0.1770000010728836, confidence:0.9306061267852783, loss:6.46940279006958
epoch28: step3000/4680
step 98000: accuracy:0.335999995470047, confidence:0.8056957721710205, loss:2.7434866428375244
epoch28: step3500/4680
step 112000: accuracy:0.39500001072883606, confidence:0.8016098737716675, loss:2.3590219020843506
epoch28: step4000/4680
step 126000: accuracy:0.296999990940094, confidence:0.8282158374786377, loss:2.975513458251953
epoch28: step4500/4680
step 0: accuracy:0.2709999978542328, confidence:0.87803053855896, loss:3.849386692047119
epoch29: step0/4680
step 14500: accuracy:0.3109999895095825, confidence:0.8471837043762207, loss:3.0776619911193848
epoch29: step500/4680
step 29000: accuracy:0.12099999934434891, confidence:0.9419541954994202, loss:5.913823127746582
epoch29: step1000/4680
step 43500: accuracy:0.23000000417232513, confidence:0.8820210099220276, loss:5.9159770011901855
epoch29: step1500/4680
step 58000: accuracy:0.2759999930858612, confidence:0.7788511514663696, loss:2.8522391319274902
epoch29: step2000/4680
step 72500: accuracy:0.4180000126361847, confidence:0.6825956702232361, loss:2.5588648319244385
epoch29: step2500/4680
step 87000: accuracy:0.15399999916553497, confidence:0.9655866622924805, loss:8.781172752380371
epoch29: step3000/4680
step 101500: accuracy:0.335999995470047, confidence:0.8353140950202942, loss:2.9044296741485596
epoch29: step3500/4680
step 116000: accuracy:0.35899999737739563, confidence:0.8091781735420227, loss:2.5407564640045166
epoch29: step4000/4680
step 130500: accuracy:0.26899999380111694, confidence:0.8271015882492065, loss:3.1994943618774414
epoch29: step4500/4680
