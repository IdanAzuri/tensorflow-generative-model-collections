2018-06-15 17:39:08.311644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 17:39:08.311868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
2018-06-15 17:39:12.841846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
CHEKPOINT DIR: multi-gaussian/mu_0.1_sigma0.3
model has been loaded from mnist_classifier.pkl
---------
Variables: name (type shape) [size]
---------
Variable:0 (float32_ref 5x5x1x32) [800, bytes: 3200]
Variable_1:0 (float32_ref 32) [32, bytes: 128]
Variable_2:0 (float32_ref 5x5x32x64) [51200, bytes: 204800]
Variable_3:0 (float32_ref 64) [64, bytes: 256]
Variable_4:0 (float32_ref 3136x1024) [3211264, bytes: 12845056]
Variable_5:0 (float32_ref 1024) [1024, bytes: 4096]
Variable_6:0 (float32_ref 1024x10) [10240, bytes: 40960]
Variable_7:0 (float32_ref 10) [10, bytes: 40]
discriminator/d_conv1/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]
discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]
discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]
discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]
generator/g_fc1/Matrix:0 (float32_ref 74x1024) [75776, bytes: 303104]
generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]
generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]
generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]
classifier/c_fc1/Matrix:0 (float32_ref 1024x64) [65536, bytes: 262144]
classifier/c_fc1/bias:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/beta:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/gamma:0 (float32_ref 64) [64, bytes: 256]
classifier/c_fc2/Matrix:0 (float32_ref 64x12) [768, bytes: 3072]
classifier/c_fc2/bias:0 (float32_ref 12) [12, bytes: 48]
Total size of variables: 16552792
Total bytes of variables: 66211168
 [*] Reading checkpoints...
 [*] Success to read MultiModalInfoGAN.model-43721
 [*] Load SUCCESS


SAMPLES SIZE=4680,LABELS=299520


SAVED TRAINING SET generated_training_set_mnist_MultivariateGaussianSampler_mu_0.1_sigma_0.3_czcc_czrc_rzcc_rzrc
 [*] Training finished!
 [*] Testing finished!
2018-06-15 17:40:08.617687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 17:40:08.617899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
2018-06-15 17:40:09.978794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
CHEKPOINT DIR: multi-gaussian/mu_0.1_sigma0.3
model has been loaded from mnist_classifier.pkl
---------
Variables: name (type shape) [size]
---------
Variable:0 (float32_ref 5x5x1x32) [800, bytes: 3200]
Variable_1:0 (float32_ref 32) [32, bytes: 128]
Variable_2:0 (float32_ref 5x5x32x64) [51200, bytes: 204800]
Variable_3:0 (float32_ref 64) [64, bytes: 256]
Variable_4:0 (float32_ref 3136x1024) [3211264, bytes: 12845056]
Variable_5:0 (float32_ref 1024) [1024, bytes: 4096]
Variable_6:0 (float32_ref 1024x10) [10240, bytes: 40960]
Variable_7:0 (float32_ref 10) [10, bytes: 40]
discriminator/d_conv1/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]
discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]
discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]
discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]
generator/g_fc1/Matrix:0 (float32_ref 74x1024) [75776, bytes: 303104]
generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]
generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]
generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]
classifier/c_fc1/Matrix:0 (float32_ref 1024x64) [65536, bytes: 262144]
classifier/c_fc1/bias:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/beta:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/gamma:0 (float32_ref 64) [64, bytes: 256]
classifier/c_fc2/Matrix:0 (float32_ref 64x12) [768, bytes: 3072]
classifier/c_fc2/bias:0 (float32_ref 12) [12, bytes: 48]
Total size of variables: 16552792
Total bytes of variables: 66211168
 [*] Reading checkpoints...
 [*] Success to read MultiModalInfoGAN.model-43721
 [*] Load SUCCESS


SAMPLES SIZE=4680,LABELS=299520


SAVED TRAINING SET generated_training_set_mnist_MultivariateGaussianSampler_mu_0.1_sigma_0.3_czcc_rzcc_czrc_rzrc
 [*] Training finished!
 [*] Testing finished!
2018-06-15 17:41:00.560581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 17:41:00.560792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
2018-06-15 17:41:01.917201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
CHEKPOINT DIR: multi-gaussian/mu_0.1_sigma0.3
model has been loaded from mnist_classifier.pkl
---------
Variables: name (type shape) [size]
---------
Variable:0 (float32_ref 5x5x1x32) [800, bytes: 3200]
Variable_1:0 (float32_ref 32) [32, bytes: 128]
Variable_2:0 (float32_ref 5x5x32x64) [51200, bytes: 204800]
Variable_3:0 (float32_ref 64) [64, bytes: 256]
Variable_4:0 (float32_ref 3136x1024) [3211264, bytes: 12845056]
Variable_5:0 (float32_ref 1024) [1024, bytes: 4096]
Variable_6:0 (float32_ref 1024x10) [10240, bytes: 40960]
Variable_7:0 (float32_ref 10) [10, bytes: 40]
discriminator/d_conv1/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]
discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]
discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]
discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]
generator/g_fc1/Matrix:0 (float32_ref 74x1024) [75776, bytes: 303104]
generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]
generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]
generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]
classifier/c_fc1/Matrix:0 (float32_ref 1024x64) [65536, bytes: 262144]
classifier/c_fc1/bias:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/beta:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/gamma:0 (float32_ref 64) [64, bytes: 256]
classifier/c_fc2/Matrix:0 (float32_ref 64x12) [768, bytes: 3072]
classifier/c_fc2/bias:0 (float32_ref 12) [12, bytes: 48]
Total size of variables: 16552792
Total bytes of variables: 66211168
 [*] Reading checkpoints...
 [*] Success to read MultiModalInfoGAN.model-43721
 [*] Load SUCCESS


SAMPLES SIZE=4680,LABELS=299520


SAVED TRAINING SET generated_training_set_mnist_MultivariateGaussianSampler_mu_0.1_sigma_0.3_rzcc_rzrc_czcc_czrc
 [*] Training finished!
 [*] Testing finished!
2018-06-15 17:41:52.729931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 17:41:52.730155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
2018-06-15 17:41:54.123087: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
CHEKPOINT DIR: multi-gaussian/mu_0.1_sigma0.3
model has been loaded from mnist_classifier.pkl
---------
Variables: name (type shape) [size]
---------
Variable:0 (float32_ref 5x5x1x32) [800, bytes: 3200]
Variable_1:0 (float32_ref 32) [32, bytes: 128]
Variable_2:0 (float32_ref 5x5x32x64) [51200, bytes: 204800]
Variable_3:0 (float32_ref 64) [64, bytes: 256]
Variable_4:0 (float32_ref 3136x1024) [3211264, bytes: 12845056]
Variable_5:0 (float32_ref 1024) [1024, bytes: 4096]
Variable_6:0 (float32_ref 1024x10) [10240, bytes: 40960]
Variable_7:0 (float32_ref 10) [10, bytes: 40]
discriminator/d_conv1/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]
discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]
discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]
discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]
generator/g_fc1/Matrix:0 (float32_ref 74x1024) [75776, bytes: 303104]
generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]
generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]
generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]
classifier/c_fc1/Matrix:0 (float32_ref 1024x64) [65536, bytes: 262144]
classifier/c_fc1/bias:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/beta:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/gamma:0 (float32_ref 64) [64, bytes: 256]
classifier/c_fc2/Matrix:0 (float32_ref 64x12) [768, bytes: 3072]
classifier/c_fc2/bias:0 (float32_ref 12) [12, bytes: 48]
Total size of variables: 16552792
Total bytes of variables: 66211168
 [*] Reading checkpoints...
 [*] Success to read MultiModalInfoGAN.model-43721
 [*] Load SUCCESS


SAMPLES SIZE=4680,LABELS=299520


SAVED TRAINING SET generated_training_set_mnist_MultivariateGaussianSampler_mu_0.1_sigma_0.3_czrc_czcc_rzcc_rzrc
 [*] Training finished!
 [*] Testing finished!
2018-06-15 17:42:42.903464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 17:42:42.903667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
model has been loaded from mnist_classifier.pkl
argmax:[4 4 4 ..., 4 4 4]
step 0: accuracy:0.0, confidence:0.9945226311683655, loss:13.377690315246582
Assinging:5
[   0  314    0    0 9686]
argmax:[8 8 8 ..., 8 8 8]
step 0: accuracy:0.0, confidence:0.9971845746040344, loss:18.636470794677734
Assinging:9
[   0  195    0    0    0    0    0    0 9805]
argmax:[5 5 5 ..., 5 5 5]
step 0: accuracy:0.0, confidence:0.988930881023407, loss:11.860736846923828
Assinging:6
[   0  118    0    0    0 9648    0    0  234]
argmax:[7 7 7 ..., 7 7 7]
step 0: accuracy:0.0, confidence:0.9899298548698425, loss:14.013608932495117
Assinging:8
[   0   39    0    0    0    0    0 9921   40]
argmax:[3 3 3 ..., 3 3 3]
step 0: accuracy:0.0, confidence:0.9999291300773621, loss:14.733097076416016
Assinging:4
[    0     0     0 10000]
argmax:[6 6 6 ..., 6 6 6]
step 0: accuracy:0.0, confidence:0.9926935434341431, loss:22.920068740844727
Assinging:7
[   0   40    0    0    0    0 9960]
argmax:[0 0 0 ..., 0 0 0]
step 0: accuracy:0.0, confidence:0.9997321367263794, loss:17.358617782592773
Assinging:1
[10000]
argmax:[2 2 2 ..., 2 2 2]
step 0: accuracy:0.0, confidence:0.9950249791145325, loss:20.76451873779297
Assinging:3
[   0   78 9922]
argmax:[1 1 1 ..., 1 1 1]
step 0: accuracy:0.0, confidence:0.9991695284843445, loss:14.83737850189209
Assinging:2
[    0 10000]
argmax:[9 9 9 ..., 9 9 9]
step 0: accuracy:1.0, confidence:0.9927464723587036, loss:0.0077295564115047455
Assinging:10
[    0     0     0     0     0     0     0     0     0 10000]
2018-06-15 17:42:59.723654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 17:42:59.723856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
model has been loaded from mnist_classifier.pkl
argmax:[3 3 3 ..., 3 3 3]
step 0: accuracy:0.0, confidence:0.9998593926429749, loss:15.565690994262695
Assinging:4
[    0     0     0 10000]
argmax:[0 0 0 ..., 0 0 0]
step 0: accuracy:0.0, confidence:0.9999388456344604, loss:16.627891540527344
Assinging:1
[10000]
argmax:[8 8 8 ..., 8 8 8]
step 0: accuracy:0.0, confidence:0.9999094009399414, loss:20.065271377563477
Assinging:9
[   0    0    0    1    0    0    0    0 9999]
argmax:[1 1 1 ..., 1 1 1]
step 0: accuracy:0.0, confidence:0.9996061325073242, loss:15.46777629852295
Assinging:2
[    0 10000]
argmax:[5 5 5 ..., 5 5 5]
step 0: accuracy:0.0, confidence:0.9972864985466003, loss:14.466840744018555
Assinging:6
[   0    0    0    3    0 9945   49    0    3]
argmax:[4 4 4 ..., 4 4 4]
step 0: accuracy:0.00019999999494757503, confidence:0.99888014793396, loss:15.013501167297363
Assinging:5
[   0    0    0    0 9998    0    0    0    0    2]
argmax:[6 6 6 ..., 6 6 6]
step 0: accuracy:0.0, confidence:0.9995203018188477, loss:22.2973575592041
Assinging:7
[   2    0    0    0    0    0 9998]
argmax:[9 9 9 ..., 9 9 9]
step 0: accuracy:0.9997000098228455, confidence:0.9986317157745361, loss:0.0019244037102907896
Assinging:10
[   0    0    0    0    0    0    0    1    2 9997]
argmax:[7 7 7 ..., 7 7 7]
step 0: accuracy:0.0, confidence:0.9995036125183105, loss:10.488227844238281
Assinging:8
[    0     0     0     0     0     0     0 10000]
argmax:[2 2 2 ..., 2 2 2]
step 0: accuracy:0.0, confidence:0.9990468621253967, loss:19.2628116607666
Assinging:3
[    0     0 10000]
2018-06-15 17:43:14.846719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 17:43:14.846929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
model has been loaded from mnist_classifier.pkl
argmax:[6 6 6 ..., 6 6 6]
step 0: accuracy:0.0, confidence:0.9932654500007629, loss:18.201793670654297
Assinging:7
[  15   20    8    0    0    0 9955    0    2]
argmax:[1 1 1 ..., 1 1 1]
step 0: accuracy:0.0, confidence:0.9963517785072327, loss:14.343603134155273
Assinging:2
[   0 9997    0    0    0    0    1    0    2]
argmax:[7 7 7 ..., 7 7 7]
step 0: accuracy:0.00019999999494757503, confidence:0.9936644434928894, loss:11.39391040802002
Assinging:8
[   0   27    0    4    1    0    0 9965    1    2]
argmax:[2 2 2 ..., 2 2 2]
step 0: accuracy:0.0, confidence:0.995189368724823, loss:17.569538116455078
Assinging:3
[   0    6 9985    1    0    0    0    1    7]
argmax:[5 5 5 ..., 5 5 5]
step 0: accuracy:0.0003000000142492354, confidence:0.9734734296798706, loss:13.55528736114502
Assinging:6
[   4   34    0   23    0 9516  254    0  166    3]
argmax:[3 3 3 ..., 3 3 3]
step 0: accuracy:0.0, confidence:0.9836820363998413, loss:13.362443923950195
Assinging:4
[   2   41   82 9821    0    4    1    7   42]
argmax:[9 9 9 ..., 9 9 9]
step 0: accuracy:0.9957000017166138, confidence:0.9873422980308533, loss:0.020976461470127106
Assinging:10
[  15    3    1    0    1    0    0   12   11 9957]
argmax:[0 0 0 ..., 0 0 0]
step 0: accuracy:0.0, confidence:0.9970636963844299, loss:15.341217041015625
Assinging:1
[9976    0   10    0    0    0    0    0   14]
argmax:[8 8 8 ..., 8 8 8]
step 0: accuracy:0.0, confidence:0.9972327947616577, loss:16.297216415405273
Assinging:9
[   0   10    7    0    0    2    1    0 9980]
argmax:[4 4 4 ..., 4 4 4]
step 0: accuracy:0.007799999788403511, confidence:0.9824578166007996, loss:10.10866641998291
Assinging:5
[   6   12    0    1 9888    0    1    8    6   78]
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
Traceback (most recent call last):
  File "classifier.py", line 379, in <module>
    main()  # c = CNNClassifier("mnist")  # c.test()
  File "classifier.py", line 371, in main
    preprocess_data(dir, fname, original_dataset_name=original_dataset_name)
  File "classifier.py", line 328, in preprocess_data
    data_X = pickle.load(open(pkl_path, 'rb'))
FileNotFoundError: [Errno 2] No such file or directory: '/cs/labs/daphna/idan.azuri/tensorflow-generative-model-collections/generated_training_set_mnist_MultivariateGaussianSampler_mu_0.1_sigma_0.3_czrc_czcc_rzcc__rzrc.pkl'
2018-06-15 17:43:32.522575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 17:43:32.522783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
step 0: accuracy:0.07400000095367432, confidence:0.634599506855011, loss:5.075428485870361
epoch0: step0/4680
step 0: accuracy:0.08799999952316284, confidence:1.0, loss:56.1959342956543
epoch0: step500/4680
step 0: accuracy:0.08900000154972076, confidence:1.0, loss:28.36995506286621
epoch0: step1000/4680
step 0: accuracy:0.10300000011920929, confidence:0.9855976700782776, loss:6.260082244873047
epoch0: step1500/4680
step 0: accuracy:0.0860000029206276, confidence:0.999316930770874, loss:9.130414009094238
epoch0: step2000/4680
step 0: accuracy:0.07999999821186066, confidence:0.9947187304496765, loss:7.640285491943359
epoch0: step2500/4680
step 0: accuracy:0.11299999803304672, confidence:0.9976841807365417, loss:8.124716758728027
epoch0: step3000/4680
step 0: accuracy:0.0949999988079071, confidence:0.9936491847038269, loss:7.44589376449585
epoch0: step3500/4680
step 0: accuracy:0.11900000274181366, confidence:0.9993751049041748, loss:9.19944953918457
epoch0: step4000/4680
step 0: accuracy:0.11999999731779099, confidence:0.9993704557418823, loss:8.586675643920898
epoch0: step4500/4680
step 0: accuracy:0.11500000208616257, confidence:0.9996544122695923, loss:9.195388793945312
epoch1: step0/4680
step 500: accuracy:0.10400000214576721, confidence:0.9980772733688354, loss:8.039307594299316
epoch1: step500/4680
step 1000: accuracy:0.09600000083446503, confidence:0.999778151512146, loss:11.485881805419922
epoch1: step1000/4680
step 1500: accuracy:0.08299999684095383, confidence:0.9996572732925415, loss:10.90568733215332
epoch1: step1500/4680
step 2000: accuracy:0.11500000208616257, confidence:0.9971729516983032, loss:7.452519416809082
epoch1: step2000/4680
step 2500: accuracy:0.09799999743700027, confidence:0.9975512027740479, loss:7.751394748687744
epoch1: step2500/4680
step 3000: accuracy:0.10300000011920929, confidence:0.9979422092437744, loss:7.596487998962402
epoch1: step3000/4680
step 3500: accuracy:0.07000000029802322, confidence:0.9983099102973938, loss:8.376911163330078
epoch1: step3500/4680
step 4000: accuracy:0.10100000351667404, confidence:0.9982649683952332, loss:8.627029418945312
epoch1: step4000/4680
step 4500: accuracy:0.1289999932050705, confidence:0.9999974966049194, loss:14.002424240112305
epoch1: step4500/4680
step 0: accuracy:0.11500000208616257, confidence:0.9999973177909851, loss:14.02763843536377
epoch2: step0/4680
step 1000: accuracy:0.10499999672174454, confidence:0.9980283379554749, loss:8.005130767822266
epoch2: step500/4680
step 2000: accuracy:0.09799999743700027, confidence:0.9998526573181152, loss:11.473437309265137
epoch2: step1000/4680
step 3000: accuracy:0.09099999815225601, confidence:0.9984465837478638, loss:9.330795288085938
epoch2: step1500/4680
step 4000: accuracy:0.0989999994635582, confidence:0.9960880279541016, loss:7.313778400421143
epoch2: step2000/4680
step 5000: accuracy:0.09799999743700027, confidence:0.9963975548744202, loss:7.371361255645752
epoch2: step2500/4680
step 6000: accuracy:0.08500000089406967, confidence:0.9983915686607361, loss:7.935013294219971
epoch2: step3000/4680
step 7000: accuracy:0.0949999988079071, confidence:0.9995295405387878, loss:9.303828239440918
epoch2: step3500/4680
step 8000: accuracy:0.10199999809265137, confidence:0.998952329158783, loss:8.606657028198242
epoch2: step4000/4680
step 9000: accuracy:0.11400000005960464, confidence:0.9999091625213623, loss:10.953513145446777
epoch2: step4500/4680
step 0: accuracy:0.10599999874830246, confidence:0.999925971031189, loss:11.177350044250488
epoch3: step0/4680
step 1500: accuracy:0.10000000149011612, confidence:0.9998025894165039, loss:10.477877616882324
epoch3: step500/4680
step 3000: accuracy:0.10000000149011612, confidence:0.998079776763916, loss:7.9000654220581055
epoch3: step1000/4680
step 4500: accuracy:0.08900000154972076, confidence:0.9886327385902405, loss:6.987541675567627
epoch3: step1500/4680
step 6000: accuracy:0.10199999809265137, confidence:0.9971672892570496, loss:7.425398826599121
epoch3: step2000/4680
step 7500: accuracy:0.09700000286102295, confidence:0.9957876205444336, loss:7.126216411590576
epoch3: step2500/4680
step 9000: accuracy:0.10499999672174454, confidence:0.999638020992279, loss:9.03098201751709
epoch3: step3000/4680
step 10500: accuracy:0.09399999678134918, confidence:0.9982857704162598, loss:8.194808006286621
epoch3: step3500/4680
step 12000: accuracy:0.11299999803304672, confidence:0.9984682202339172, loss:7.94293737411499
epoch3: step4000/4680
step 13500: accuracy:0.10300000011920929, confidence:0.998871386051178, loss:8.590072631835938
epoch3: step4500/4680
step 0: accuracy:0.11400000005960464, confidence:0.9996733665466309, loss:9.465413093566895
epoch4: step0/4680
step 2000: accuracy:0.10999999940395355, confidence:0.9915033578872681, loss:6.969470977783203
epoch4: step500/4680
step 4000: accuracy:0.10300000011920929, confidence:0.994667112827301, loss:6.8365559577941895
epoch4: step1000/4680
step 6000: accuracy:0.10499999672174454, confidence:0.9381622672080994, loss:5.086226463317871
epoch4: step1500/4680
step 8000: accuracy:0.10199999809265137, confidence:0.9623873829841614, loss:5.181787014007568
epoch4: step2000/4680
step 10000: accuracy:0.08699999749660492, confidence:0.9872398972511292, loss:5.990417003631592
epoch4: step2500/4680
step 12000: accuracy:0.0860000029206276, confidence:0.9995699524879456, loss:9.025094985961914
epoch4: step3000/4680
step 14000: accuracy:0.10999999940395355, confidence:0.9978834390640259, loss:7.70961332321167
epoch4: step3500/4680
step 16000: accuracy:0.0989999994635582, confidence:0.9981188774108887, loss:7.654202461242676
epoch4: step4000/4680
step 18000: accuracy:0.12099999934434891, confidence:0.9987015128135681, loss:8.506516456604004
epoch4: step4500/4680
step 0: accuracy:0.1289999932050705, confidence:0.999290406703949, loss:8.799712181091309
epoch5: step0/4680
step 2500: accuracy:0.09799999743700027, confidence:0.9697879552841187, loss:5.68519926071167
epoch5: step500/4680
step 5000: accuracy:0.10899999737739563, confidence:0.9848458170890808, loss:5.749980926513672
epoch5: step1000/4680
step 7500: accuracy:0.09200000017881393, confidence:0.8398361802101135, loss:4.045279502868652
epoch5: step1500/4680
step 10000: accuracy:0.1340000033378601, confidence:0.8453673720359802, loss:3.987759828567505
epoch5: step2000/4680
step 12500: accuracy:0.09200000017881393, confidence:0.9519661068916321, loss:5.080374240875244
epoch5: step2500/4680
step 15000: accuracy:0.10100000351667404, confidence:0.9908176064491272, loss:6.196616172790527
epoch5: step3000/4680
step 17500: accuracy:0.09600000083446503, confidence:0.9558787941932678, loss:5.284700393676758
epoch5: step3500/4680
step 20000: accuracy:0.10599999874830246, confidence:0.971953809261322, loss:5.659850597381592
epoch5: step4000/4680
step 22500: accuracy:0.1550000011920929, confidence:0.8847414255142212, loss:4.558642864227295
epoch5: step4500/4680
step 0: accuracy:0.12399999797344208, confidence:0.9355737566947937, loss:5.657520771026611
epoch6: step0/4680
step 3000: accuracy:0.17299999296665192, confidence:0.7756093144416809, loss:2.993995189666748
epoch6: step500/4680
step 6000: accuracy:0.0860000029206276, confidence:0.9595316052436829, loss:4.8053975105285645
epoch6: step1000/4680
step 9000: accuracy:0.14499999582767487, confidence:0.7641839385032654, loss:3.193599224090576
epoch6: step1500/4680
step 12000: accuracy:0.16899999976158142, confidence:0.8257558345794678, loss:3.4087841510772705
epoch6: step2000/4680
step 15000: accuracy:0.19699999690055847, confidence:0.8519154191017151, loss:3.674468994140625
epoch6: step2500/4680
step 18000: accuracy:0.11999999731779099, confidence:0.940963864326477, loss:5.202327251434326
epoch6: step3000/4680
step 21000: accuracy:0.1120000034570694, confidence:0.9005792737007141, loss:4.815228462219238
epoch6: step3500/4680
step 24000: accuracy:0.10599999874830246, confidence:0.9355025887489319, loss:5.464480876922607
epoch6: step4000/4680
step 27000: accuracy:0.19699999690055847, confidence:0.8816239833831787, loss:4.532978534698486
epoch6: step4500/4680
step 0: accuracy:0.15600000321865082, confidence:0.9319842457771301, loss:5.549536228179932
epoch7: step0/4680
step 3500: accuracy:0.27900001406669617, confidence:0.6670883893966675, loss:2.0977373123168945
epoch7: step500/4680
step 7000: accuracy:0.1120000034570694, confidence:0.8874515295028687, loss:3.754533052444458
epoch7: step1000/4680
step 10500: accuracy:0.17499999701976776, confidence:0.7254037261009216, loss:2.827204704284668
epoch7: step1500/4680
step 14000: accuracy:0.19499999284744263, confidence:0.8192840218544006, loss:3.295185089111328
epoch7: step2000/4680
step 17500: accuracy:0.20999999344348907, confidence:0.8327303528785706, loss:3.6590864658355713
epoch7: step2500/4680
step 21000: accuracy:0.1589999943971634, confidence:0.9396199584007263, loss:4.785417079925537
epoch7: step3000/4680
step 24500: accuracy:0.1899999976158142, confidence:0.8388526439666748, loss:3.725955009460449
epoch7: step3500/4680
step 28000: accuracy:0.16300000250339508, confidence:0.8958538770675659, loss:4.798635482788086
epoch7: step4000/4680
step 31500: accuracy:0.2409999966621399, confidence:0.8497946858406067, loss:4.0622453689575195
epoch7: step4500/4680
step 0: accuracy:0.23199999332427979, confidence:0.8875977993011475, loss:4.568786144256592
epoch8: step0/4680
step 4000: accuracy:0.3720000088214874, confidence:0.6494141221046448, loss:1.824241280555725
epoch8: step500/4680
step 8000: accuracy:0.11500000208616257, confidence:0.9254536628723145, loss:4.417207717895508
epoch8: step1000/4680
step 12000: accuracy:0.22499999403953552, confidence:0.7461545467376709, loss:2.7505688667297363
epoch8: step1500/4680
step 16000: accuracy:0.22699999809265137, confidence:0.8185287117958069, loss:3.1233415603637695
epoch8: step2000/4680
step 20000: accuracy:0.25200000405311584, confidence:0.8075507879257202, loss:3.425057888031006
epoch8: step2500/4680
step 24000: accuracy:0.1899999976158142, confidence:0.906699001789093, loss:4.901361465454102
epoch8: step3000/4680
step 28000: accuracy:0.2759999930858612, confidence:0.8067895174026489, loss:3.406456470489502
epoch8: step3500/4680
step 32000: accuracy:0.19300000369548798, confidence:0.8772075772285461, loss:4.299164295196533
epoch8: step4000/4680
step 36000: accuracy:0.23999999463558197, confidence:0.823982298374176, loss:3.726135015487671
epoch8: step4500/4680
step 0: accuracy:0.210999995470047, confidence:0.8980139493942261, loss:4.981136798858643
epoch9: step0/4680
step 4500: accuracy:0.39500001072883606, confidence:0.6209269165992737, loss:1.6959012746810913
epoch9: step500/4680
step 9000: accuracy:0.19300000369548798, confidence:0.8314751982688904, loss:3.196305751800537
epoch9: step1000/4680
step 13500: accuracy:0.25099998712539673, confidence:0.7596437335014343, loss:2.3189873695373535
epoch9: step1500/4680
step 18000: accuracy:0.26100000739097595, confidence:0.8310001492500305, loss:3.1970882415771484
epoch9: step2000/4680
step 22500: accuracy:0.27799999713897705, confidence:0.8212392926216125, loss:3.3126611709594727
epoch9: step2500/4680
step 27000: accuracy:0.2290000021457672, confidence:0.8989606499671936, loss:4.385146617889404
epoch9: step3000/4680
step 31500: accuracy:0.32499998807907104, confidence:0.7968164682388306, loss:3.1169469356536865
epoch9: step3500/4680
step 36000: accuracy:0.17499999701976776, confidence:0.8846973776817322, loss:4.6077094078063965
epoch9: step4000/4680
step 40500: accuracy:0.27900001406669617, confidence:0.8314060568809509, loss:3.3746273517608643
epoch9: step4500/4680
step 0: accuracy:0.20999999344348907, confidence:0.8892613649368286, loss:4.670961380004883
epoch10: step0/4680
step 5000: accuracy:0.3630000054836273, confidence:0.6935371160507202, loss:1.9737393856048584
epoch10: step500/4680
step 10000: accuracy:0.15299999713897705, confidence:0.8988117575645447, loss:3.948835849761963
epoch10: step1000/4680
step 15000: accuracy:0.28700000047683716, confidence:0.7629984021186829, loss:2.5011932849884033
epoch10: step1500/4680
step 20000: accuracy:0.2639999985694885, confidence:0.8564085364341736, loss:3.6534154415130615
epoch10: step2000/4680
step 25000: accuracy:0.3100000023841858, confidence:0.8117536902427673, loss:3.397656202316284
epoch10: step2500/4680
step 30000: accuracy:0.25999999046325684, confidence:0.8837791681289673, loss:4.557531356811523
epoch10: step3000/4680
step 35000: accuracy:0.3440000116825104, confidence:0.7951608896255493, loss:3.1082265377044678
epoch10: step3500/4680
step 40000: accuracy:0.22699999809265137, confidence:0.858794093132019, loss:3.8462941646575928
epoch10: step4000/4680
step 45000: accuracy:0.29100000858306885, confidence:0.8366371393203735, loss:3.5114481449127197
epoch10: step4500/4680
step 0: accuracy:0.2240000069141388, confidence:0.9172163009643555, loss:5.177602767944336
epoch11: step0/4680
step 5500: accuracy:0.382999986410141, confidence:0.69852215051651, loss:1.9062904119491577
epoch11: step500/4680
step 11000: accuracy:0.1809999942779541, confidence:0.8506127595901489, loss:3.15010404586792
epoch11: step1000/4680
step 16500: accuracy:0.2630000114440918, confidence:0.7614401578903198, loss:2.3072643280029297
epoch11: step1500/4680
step 22000: accuracy:0.2930000126361847, confidence:0.7952935695648193, loss:2.5290451049804688
epoch11: step2000/4680
step 27500: accuracy:0.28200000524520874, confidence:0.7855459451675415, loss:2.876624584197998
epoch11: step2500/4680
step 33000: accuracy:0.2680000066757202, confidence:0.8841222524642944, loss:4.389152526855469
epoch11: step3000/4680
step 38500: accuracy:0.34599998593330383, confidence:0.8046144843101501, loss:3.1302976608276367
epoch11: step3500/4680
step 44000: accuracy:0.19900000095367432, confidence:0.8493989109992981, loss:4.289495944976807
epoch11: step4000/4680
step 49500: accuracy:0.3499999940395355, confidence:0.8286408185958862, loss:3.049452066421509
epoch11: step4500/4680
step 0: accuracy:0.3160000145435333, confidence:0.8518183827400208, loss:3.5765585899353027
epoch12: step0/4680
step 6000: accuracy:0.4350000023841858, confidence:0.720498263835907, loss:1.7635400295257568
epoch12: step500/4680
step 12000: accuracy:0.2720000147819519, confidence:0.8131609559059143, loss:2.8496005535125732
epoch12: step1000/4680
step 18000: accuracy:0.2460000067949295, confidence:0.8003153204917908, loss:2.729121685028076
epoch12: step1500/4680
step 24000: accuracy:0.23499999940395355, confidence:0.8721013069152832, loss:4.199371814727783
epoch12: step2000/4680
step 30000: accuracy:0.27300000190734863, confidence:0.8607277870178223, loss:4.556214332580566
epoch12: step2500/4680
step 36000: accuracy:0.2529999911785126, confidence:0.8917214870452881, loss:3.906830310821533
epoch12: step3000/4680
step 42000: accuracy:0.32899999618530273, confidence:0.8120569586753845, loss:3.379248857498169
epoch12: step3500/4680
step 48000: accuracy:0.25699999928474426, confidence:0.8536417484283447, loss:3.6772806644439697
epoch12: step4000/4680
step 54000: accuracy:0.2980000078678131, confidence:0.835969865322113, loss:3.3110153675079346
epoch12: step4500/4680
step 0: accuracy:0.24899999797344208, confidence:0.8826252818107605, loss:4.3985700607299805
epoch13: step0/4680
step 6500: accuracy:0.3799999952316284, confidence:0.7469800710678101, loss:2.1812996864318848
epoch13: step500/4680
step 13000: accuracy:0.2029999941587448, confidence:0.8487182259559631, loss:2.98349666595459
epoch13: step1000/4680
step 19500: accuracy:0.25, confidence:0.7709800601005554, loss:2.3156580924987793
epoch13: step1500/4680
step 26000: accuracy:0.335999995470047, confidence:0.7745150327682495, loss:2.4208879470825195
epoch13: step2000/4680
step 32500: accuracy:0.28700000047683716, confidence:0.8296492099761963, loss:3.247969150543213
epoch13: step2500/4680
step 39000: accuracy:0.24699999392032623, confidence:0.8700056672096252, loss:5.611791610717773
epoch13: step3000/4680
step 45500: accuracy:0.35499998927116394, confidence:0.8083875179290771, loss:3.1786410808563232
epoch13: step3500/4680
step 52000: accuracy:0.27000001072883606, confidence:0.8473595380783081, loss:3.6854629516601562
epoch13: step4000/4680
step 58500: accuracy:0.3569999933242798, confidence:0.8356239199638367, loss:2.9606127738952637
epoch13: step4500/4680
step 0: accuracy:0.27300000190734863, confidence:0.8776976466178894, loss:3.9270219802856445
epoch14: step0/4680
step 7000: accuracy:0.39800000190734863, confidence:0.789457380771637, loss:2.176860809326172
epoch14: step500/4680
step 14000: accuracy:0.30799999833106995, confidence:0.8019810914993286, loss:2.819011688232422
epoch14: step1000/4680
step 21000: accuracy:0.2639999985694885, confidence:0.8264551758766174, loss:2.8770172595977783
epoch14: step1500/4680
step 28000: accuracy:0.27300000190734863, confidence:0.88047856092453, loss:3.670473098754883
epoch14: step2000/4680
step 35000: accuracy:0.31299999356269836, confidence:0.8486329913139343, loss:3.5855953693389893
epoch14: step2500/4680
step 42000: accuracy:0.3179999887943268, confidence:0.8925471305847168, loss:3.510822296142578
epoch14: step3000/4680
step 49000: accuracy:0.28299999237060547, confidence:0.845622181892395, loss:3.7954788208007812
epoch14: step3500/4680
step 56000: accuracy:0.25099998712539673, confidence:0.8576968908309937, loss:3.9773142337799072
epoch14: step4000/4680
step 63000: accuracy:0.28999999165534973, confidence:0.8541141152381897, loss:3.589412212371826
epoch14: step4500/4680
step 0: accuracy:0.24199999868869781, confidence:0.8940141797065735, loss:4.620216369628906
epoch15: step0/4680
step 7500: accuracy:0.4970000088214874, confidence:0.690062940120697, loss:1.5218597650527954
epoch15: step500/4680
step 15000: accuracy:0.23100000619888306, confidence:0.7967903017997742, loss:2.5574252605438232
epoch15: step1000/4680
step 22500: accuracy:0.2409999966621399, confidence:0.7621537446975708, loss:2.2550394535064697
epoch15: step1500/4680
step 30000: accuracy:0.289000004529953, confidence:0.8106638193130493, loss:2.764662981033325
epoch15: step2000/4680
step 37500: accuracy:0.27799999713897705, confidence:0.8278533816337585, loss:3.4064040184020996
epoch15: step2500/4680
step 45000: accuracy:0.25, confidence:0.8854650855064392, loss:4.63710880279541
epoch15: step3000/4680
step 52500: accuracy:0.34200000762939453, confidence:0.775145947933197, loss:2.7662973403930664
epoch15: step3500/4680
step 60000: accuracy:0.25600001215934753, confidence:0.8330655694007874, loss:3.2506916522979736
epoch15: step4000/4680
step 67500: accuracy:0.36500000953674316, confidence:0.7986511588096619, loss:2.6477742195129395
epoch15: step4500/4680
step 0: accuracy:0.31700000166893005, confidence:0.8298342227935791, loss:3.2589051723480225
epoch16: step0/4680
step 8000: accuracy:0.45100000500679016, confidence:0.7653768062591553, loss:1.9456974267959595
epoch16: step500/4680
step 16000: accuracy:0.39899998903274536, confidence:0.8095738291740417, loss:2.439453125
epoch16: step1000/4680
step 24000: accuracy:0.2619999945163727, confidence:0.8816659450531006, loss:3.611428737640381
epoch16: step1500/4680
step 32000: accuracy:0.2619999945163727, confidence:0.8801054954528809, loss:3.8133888244628906
epoch16: step2000/4680
step 40000: accuracy:0.2750000059604645, confidence:0.8739433884620667, loss:5.1876349449157715
epoch16: step2500/4680
step 48000: accuracy:0.3179999887943268, confidence:0.905086874961853, loss:3.7464258670806885
epoch16: step3000/4680
step 56000: accuracy:0.4230000078678131, confidence:0.81374591588974, loss:2.9992198944091797
epoch16: step3500/4680
step 64000: accuracy:0.10999999940395355, confidence:0.9655537605285645, loss:6.881165027618408
epoch16: step4000/4680
step 72000: accuracy:0.2619999945163727, confidence:0.8886173963546753, loss:3.9385640621185303
epoch16: step4500/4680
step 0: accuracy:0.26100000739097595, confidence:0.9074021577835083, loss:4.476816177368164
epoch17: step0/4680
step 8500: accuracy:0.5239999890327454, confidence:0.7172341346740723, loss:1.4470857381820679
epoch17: step500/4680
step 17000: accuracy:0.22599999606609344, confidence:0.8440841436386108, loss:2.809777021408081
epoch17: step1000/4680
step 25500: accuracy:0.25099998712539673, confidence:0.7683374881744385, loss:2.2598164081573486
epoch17: step1500/4680
step 34000: accuracy:0.2720000147819519, confidence:0.8580958247184753, loss:3.898801326751709
epoch17: step2000/4680
step 42500: accuracy:0.3050000071525574, confidence:0.8132360577583313, loss:2.8814210891723633
epoch17: step2500/4680
step 51000: accuracy:0.2619999945163727, confidence:0.8849081993103027, loss:5.518772602081299
epoch17: step3000/4680
step 59500: accuracy:0.39100000262260437, confidence:0.7998224496841431, loss:2.7484724521636963
epoch17: step3500/4680
step 68000: accuracy:0.2770000100135803, confidence:0.8248518705368042, loss:3.1365103721618652
epoch17: step4000/4680
step 76500: accuracy:0.4059999883174896, confidence:0.805562436580658, loss:2.452660083770752
epoch17: step4500/4680
step 0: accuracy:0.35600000619888306, confidence:0.8219239711761475, loss:2.925422430038452
epoch18: step0/4680
step 9000: accuracy:0.4059999883174896, confidence:0.7884386777877808, loss:2.0727920532226562
epoch18: step500/4680
step 18000: accuracy:0.42100000381469727, confidence:0.7867631316184998, loss:2.0592124462127686
epoch18: step1000/4680
step 27000: accuracy:0.27900001406669617, confidence:0.8717981576919556, loss:3.3123703002929688
epoch18: step1500/4680
step 36000: accuracy:0.2879999876022339, confidence:0.8835041522979736, loss:4.084756851196289
epoch18: step2000/4680
step 45000: accuracy:0.3479999899864197, confidence:0.8081012964248657, loss:2.96632981300354
epoch18: step2500/4680
step 54000: accuracy:0.24699999392032623, confidence:0.9131865501403809, loss:5.329094409942627
epoch18: step3000/4680
step 63000: accuracy:0.3919999897480011, confidence:0.8201377391815186, loss:3.432509183883667
epoch18: step3500/4680
step 72000: accuracy:0.2529999911785126, confidence:0.8863382339477539, loss:4.1422438621521
epoch18: step4000/4680
step 81000: accuracy:0.2849999964237213, confidence:0.8715829849243164, loss:3.616363048553467
epoch18: step4500/4680
step 0: accuracy:0.25999999046325684, confidence:0.9150809645652771, loss:4.4920125007629395
epoch19: step0/4680
step 9500: accuracy:0.48399999737739563, confidence:0.7203929424285889, loss:1.629607081413269
epoch19: step500/4680
step 19000: accuracy:0.2639999985694885, confidence:0.8201687335968018, loss:2.611104965209961
epoch19: step1000/4680
step 28500: accuracy:0.2540000081062317, confidence:0.8007404804229736, loss:2.44219970703125
epoch19: step1500/4680
step 38000: accuracy:0.2800000011920929, confidence:0.8106011748313904, loss:2.898729085922241
epoch19: step2000/4680
step 47500: accuracy:0.35100001096725464, confidence:0.7799751162528992, loss:2.4994771480560303
epoch19: step2500/4680
step 57000: accuracy:0.3160000145435333, confidence:0.8520973920822144, loss:3.4996371269226074
epoch19: step3000/4680
step 66500: accuracy:0.3709999918937683, confidence:0.764717161655426, loss:2.5108253955841064
epoch19: step3500/4680
step 76000: accuracy:0.3199999928474426, confidence:0.7972273230552673, loss:2.786299228668213
epoch19: step4000/4680
step 85500: accuracy:0.42100000381469727, confidence:0.7877705097198486, loss:2.2372233867645264
epoch19: step4500/4680
step 0: accuracy:0.36899998784065247, confidence:0.8246845006942749, loss:2.8043766021728516
epoch20: step0/4680
step 10000: accuracy:0.4449999928474426, confidence:0.7790309190750122, loss:1.870197057723999
epoch20: step500/4680
step 20000: accuracy:0.3799999952316284, confidence:0.7930740714073181, loss:2.215158224105835
epoch20: step1000/4680
step 30000: accuracy:0.2720000147819519, confidence:0.8643035888671875, loss:3.243406295776367
epoch20: step1500/4680
step 40000: accuracy:0.28200000524520874, confidence:0.8478339910507202, loss:3.4225780963897705
epoch20: step2000/4680
step 50000: accuracy:0.3409999907016754, confidence:0.8452165126800537, loss:3.4820399284362793
epoch20: step2500/4680
step 60000: accuracy:0.24899999797344208, confidence:0.9258232712745667, loss:4.807103157043457
epoch20: step3000/4680
step 70000: accuracy:0.3630000054836273, confidence:0.8352804183959961, loss:3.0733306407928467
epoch20: step3500/4680
step 80000: accuracy:0.23499999940395355, confidence:0.8594782948493958, loss:3.8895926475524902
epoch20: step4000/4680
step 90000: accuracy:0.29100000858306885, confidence:0.8640045523643494, loss:3.3687500953674316
epoch20: step4500/4680
step 0: accuracy:0.24899999797344208, confidence:0.9030526280403137, loss:4.441032409667969
epoch21: step0/4680
step 10500: accuracy:0.5389999747276306, confidence:0.7441352009773254, loss:1.4167636632919312
epoch21: step500/4680
step 21000: accuracy:0.26499998569488525, confidence:0.8162891268730164, loss:2.6526029109954834
epoch21: step1000/4680
step 31500: accuracy:0.23399999737739563, confidence:0.7774423360824585, loss:2.269690990447998
epoch21: step1500/4680
step 42000: accuracy:0.3230000138282776, confidence:0.8243646025657654, loss:2.776132345199585
epoch21: step2000/4680
step 52500: accuracy:0.3100000023841858, confidence:0.8190332651138306, loss:3.0504651069641113
epoch21: step2500/4680
step 63000: accuracy:0.3009999990463257, confidence:0.8816738128662109, loss:3.6698129177093506
epoch21: step3000/4680
step 73500: accuracy:0.4050000011920929, confidence:0.7890492677688599, loss:2.6134352684020996
epoch21: step3500/4680
step 84000: accuracy:0.2939999997615814, confidence:0.8221360445022583, loss:3.0577478408813477
epoch21: step4000/4680
step 94500: accuracy:0.4350000023841858, confidence:0.8092973828315735, loss:2.3103740215301514
epoch21: step4500/4680
step 0: accuracy:0.3529999852180481, confidence:0.8407613039016724, loss:2.9833195209503174
epoch22: step0/4680
step 11000: accuracy:0.41200000047683716, confidence:0.7728671431541443, loss:2.0438389778137207
epoch22: step500/4680
step 22000: accuracy:0.4169999957084656, confidence:0.805439293384552, loss:1.9119935035705566
epoch22: step1000/4680
step 33000: accuracy:0.3009999990463257, confidence:0.8543604016304016, loss:2.9116945266723633
epoch22: step1500/4680
step 44000: accuracy:0.3190000057220459, confidence:0.8880730271339417, loss:4.08443546295166
epoch22: step2000/4680
step 55000: accuracy:0.35100001096725464, confidence:0.8293688893318176, loss:3.4551382064819336
epoch22: step2500/4680
step 66000: accuracy:0.20499999821186066, confidence:0.9096257090568542, loss:6.246843338012695
epoch22: step3000/4680
step 77000: accuracy:0.33500000834465027, confidence:0.863836407661438, loss:3.5945966243743896
epoch22: step3500/4680
step 88000: accuracy:0.23800000548362732, confidence:0.8810574412345886, loss:4.1674909591674805
epoch22: step4000/4680
step 99000: accuracy:0.3089999854564667, confidence:0.8530467748641968, loss:3.4255330562591553
epoch22: step4500/4680
step 0: accuracy:0.2540000081062317, confidence:0.8979553580284119, loss:4.491366863250732
epoch23: step0/4680
step 11500: accuracy:0.4819999933242798, confidence:0.7650116682052612, loss:1.828060507774353
epoch23: step500/4680
step 23000: accuracy:0.3070000112056732, confidence:0.8150761723518372, loss:2.581699848175049
epoch23: step1000/4680
step 34500: accuracy:0.2549999952316284, confidence:0.806454598903656, loss:2.450446128845215
epoch23: step1500/4680
step 46000: accuracy:0.3149999976158142, confidence:0.8420786261558533, loss:3.087106227874756
epoch23: step2000/4680
step 57500: accuracy:0.36899998784065247, confidence:0.7928749918937683, loss:2.3340768814086914
epoch23: step2500/4680
step 69000: accuracy:0.36500000953674316, confidence:0.8753747344017029, loss:3.4339492321014404
epoch23: step3000/4680
step 80500: accuracy:0.34200000762939453, confidence:0.8373050689697266, loss:3.448944330215454
epoch23: step3500/4680
step 92000: accuracy:0.28299999237060547, confidence:0.8442740440368652, loss:3.852198839187622
epoch23: step4000/4680
step 103500: accuracy:0.4449999928474426, confidence:0.8107200264930725, loss:2.407876968383789
epoch23: step4500/4680
step 0: accuracy:0.382999986410141, confidence:0.8376948237419128, loss:2.804887056350708
epoch24: step0/4680
step 12000: accuracy:0.5619999766349792, confidence:0.7599444389343262, loss:1.3920127153396606
epoch24: step500/4680
step 24000: accuracy:0.3700000047683716, confidence:0.7956114411354065, loss:2.327043294906616
epoch24: step1000/4680
step 36000: accuracy:0.2619999945163727, confidence:0.8708316683769226, loss:3.207767963409424
epoch24: step1500/4680
step 48000: accuracy:0.2669999897480011, confidence:0.8974599242210388, loss:4.681814670562744
epoch24: step2000/4680
step 60000: accuracy:0.3409999907016754, confidence:0.8589687347412109, loss:4.053461074829102
epoch24: step2500/4680
step 72000: accuracy:0.2370000034570694, confidence:0.9203665256500244, loss:4.759849548339844
epoch24: step3000/4680
step 84000: accuracy:0.35199999809265137, confidence:0.8155414462089539, loss:3.0871732234954834
epoch24: step3500/4680
step 96000: accuracy:0.2849999964237213, confidence:0.827181875705719, loss:3.1962192058563232
epoch24: step4000/4680
step 108000: accuracy:0.35100001096725464, confidence:0.8287919163703918, loss:2.9265806674957275
epoch24: step4500/4680
step 0: accuracy:0.3479999899864197, confidence:0.8404107689857483, loss:3.224830150604248
epoch25: step0/4680
step 12500: accuracy:0.4959999918937683, confidence:0.7817078828811646, loss:1.876423716545105
epoch25: step500/4680
step 25000: accuracy:0.2549999952316284, confidence:0.852491557598114, loss:3.658419370651245
epoch25: step1000/4680
step 37500: accuracy:0.30300000309944153, confidence:0.7817635536193848, loss:2.151813268661499
epoch25: step1500/4680
step 50000: accuracy:0.3370000123977661, confidence:0.863210141658783, loss:3.5911386013031006
epoch25: step2000/4680
step 62500: accuracy:0.3790000081062317, confidence:0.7832648158073425, loss:2.3791050910949707
epoch25: step2500/4680
step 75000: accuracy:0.2770000100135803, confidence:0.8811293244361877, loss:5.048547267913818
epoch25: step3000/4680
step 87500: accuracy:0.41100001335144043, confidence:0.7938762307167053, loss:3.036968946456909
epoch25: step3500/4680
step 100000: accuracy:0.16599999368190765, confidence:0.9195824265480042, loss:5.617032051086426
epoch25: step4000/4680
step 112500: accuracy:0.41600000858306885, confidence:0.8207332491874695, loss:2.462862968444824
epoch25: step4500/4680
step 0: accuracy:0.3370000123977661, confidence:0.8361608982086182, loss:2.9929916858673096
epoch26: step0/4680
step 13000: accuracy:0.621999979019165, confidence:0.7467410564422607, loss:1.1575980186462402
epoch26: step500/4680
step 26000: accuracy:0.3490000069141388, confidence:0.7976553440093994, loss:2.3957016468048096
epoch26: step1000/4680
step 39000: accuracy:0.2750000059604645, confidence:0.819795548915863, loss:2.5934760570526123
epoch26: step1500/4680
step 52000: accuracy:0.26600000262260437, confidence:0.879621684551239, loss:4.606428146362305
epoch26: step2000/4680
step 65000: accuracy:0.3959999978542328, confidence:0.813157856464386, loss:2.5661966800689697
epoch26: step2500/4680
step 78000: accuracy:0.34299999475479126, confidence:0.877818763256073, loss:3.9225916862487793
epoch26: step3000/4680
step 91000: accuracy:0.37299999594688416, confidence:0.7977024912834167, loss:2.8617899417877197
epoch26: step3500/4680
step 104000: accuracy:0.33899998664855957, confidence:0.8189031481742859, loss:2.9157049655914307
epoch26: step4000/4680
step 117000: accuracy:0.40299999713897705, confidence:0.8212325572967529, loss:2.50730562210083
epoch26: step4500/4680
step 0: accuracy:0.3109999895095825, confidence:0.8523765802383423, loss:3.4284510612487793
epoch27: step0/4680
step 13500: accuracy:0.4659999907016754, confidence:0.8259161114692688, loss:2.188988447189331
epoch27: step500/4680
step 27000: accuracy:0.22100000083446503, confidence:0.856982946395874, loss:3.879455327987671
epoch27: step1000/4680
step 40500: accuracy:0.3269999921321869, confidence:0.8067216873168945, loss:2.2664711475372314
epoch27: step1500/4680
step 54000: accuracy:0.41100001335144043, confidence:0.8455928564071655, loss:2.7071001529693604
epoch27: step2000/4680
step 67500: accuracy:0.39500001072883606, confidence:0.8284485340118408, loss:2.6848719120025635
epoch27: step2500/4680
step 81000: accuracy:0.2809999883174896, confidence:0.8903530836105347, loss:4.485245227813721
epoch27: step3000/4680
step 94500: accuracy:0.45399999618530273, confidence:0.8257113695144653, loss:2.7149736881256104
epoch27: step3500/4680
step 108000: accuracy:0.19699999690055847, confidence:0.8835869431495667, loss:4.530179500579834
epoch27: step4000/4680
step 121500: accuracy:0.414000004529953, confidence:0.8278307914733887, loss:2.494021415710449
epoch27: step4500/4680
step 0: accuracy:0.3440000116825104, confidence:0.8505903482437134, loss:3.203357219696045
epoch28: step0/4680
step 14000: accuracy:0.5680000185966492, confidence:0.7097853422164917, loss:1.2524017095565796
epoch28: step500/4680
step 28000: accuracy:0.2930000126361847, confidence:0.8065240383148193, loss:2.5617120265960693
epoch28: step1000/4680
step 42000: accuracy:0.2460000067949295, confidence:0.8045646548271179, loss:2.466050624847412
epoch28: step1500/4680
step 56000: accuracy:0.2770000100135803, confidence:0.8747992515563965, loss:4.139804363250732
epoch28: step2000/4680
step 70000: accuracy:0.32100000977516174, confidence:0.8033298254013062, loss:2.696356773376465
epoch28: step2500/4680
step 84000: accuracy:0.32199999690055847, confidence:0.8772437572479248, loss:3.757469654083252
epoch28: step3000/4680
step 98000: accuracy:0.3799999952316284, confidence:0.7962043285369873, loss:2.7994768619537354
epoch28: step3500/4680
step 112000: accuracy:0.36500000953674316, confidence:0.7984693646430969, loss:2.664449453353882
epoch28: step4000/4680
step 126000: accuracy:0.43799999356269836, confidence:0.7948315143585205, loss:2.2190561294555664
epoch28: step4500/4680
step 0: accuracy:0.32499998807907104, confidence:0.8372613191604614, loss:3.1636853218078613
epoch29: step0/4680
step 14500: accuracy:0.5389999747276306, confidence:0.790894627571106, loss:1.5872570276260376
epoch29: step500/4680
step 29000: accuracy:0.3089999854564667, confidence:0.8227006793022156, loss:2.8299179077148438
epoch29: step1000/4680
step 43500: accuracy:0.24899999797344208, confidence:0.8935834765434265, loss:3.6872236728668213
epoch29: step1500/4680
step 58000: accuracy:0.4009999930858612, confidence:0.8695178031921387, loss:3.1056013107299805
epoch29: step2000/4680
step 72500: accuracy:0.3970000147819519, confidence:0.7986912727355957, loss:2.5746729373931885
epoch29: step2500/4680
step 87000: accuracy:0.25600001215934753, confidence:0.8813906908035278, loss:5.274523735046387
epoch29: step3000/4680
step 101500: accuracy:0.42500001192092896, confidence:0.8386594653129578, loss:2.7154886722564697
epoch29: step3500/4680
step 116000: accuracy:0.20100000500679016, confidence:0.8927333950996399, loss:4.549709320068359
epoch29: step4000/4680
step 130500: accuracy:0.39399999380111694, confidence:0.8238127827644348, loss:2.557711362838745
epoch29: step4500/4680
2018-06-15 17:52:29.530579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 17:52:29.530790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
step 0: accuracy:0.11599999666213989, confidence:0.7966257929801941, loss:6.78522253036499
epoch0: step0/4680
step 0: accuracy:0.11500000208616257, confidence:1.0, loss:43.38222885131836
epoch0: step500/4680
step 0: accuracy:0.10700000077486038, confidence:1.0, loss:55.459388732910156
epoch0: step1000/4680
step 0: accuracy:0.10199999809265137, confidence:1.0, loss:20.572839736938477
epoch0: step1500/4680
step 0: accuracy:0.1340000033378601, confidence:0.9998039603233337, loss:9.852588653564453
epoch0: step2000/4680
step 0: accuracy:0.09300000220537186, confidence:0.9961115121841431, loss:7.852293491363525
epoch0: step2500/4680
step 0: accuracy:0.07900000363588333, confidence:0.9979545474052429, loss:8.15129566192627
epoch0: step3000/4680
step 0: accuracy:0.07900000363588333, confidence:0.9970357418060303, loss:7.817784786224365
epoch0: step3500/4680
step 0: accuracy:0.10199999809265137, confidence:0.9965488910675049, loss:8.04481029510498
epoch0: step4000/4680
step 0: accuracy:0.10499999672174454, confidence:0.998917281627655, loss:8.650121688842773
epoch0: step4500/4680
step 0: accuracy:0.1080000028014183, confidence:0.999449610710144, loss:9.172477722167969
epoch1: step0/4680
step 500: accuracy:0.09000000357627869, confidence:0.98948734998703, loss:6.381647109985352
epoch1: step500/4680
step 1000: accuracy:0.10700000077486038, confidence:0.9929709434509277, loss:7.659969806671143
epoch1: step1000/4680
step 1500: accuracy:0.10499999672174454, confidence:0.9990075826644897, loss:9.299222946166992
epoch1: step1500/4680
step 2000: accuracy:0.14000000059604645, confidence:0.9976400136947632, loss:7.839263439178467
epoch1: step2000/4680
step 2500: accuracy:0.09200000017881393, confidence:0.9901876449584961, loss:6.549920082092285
epoch1: step2500/4680
step 3000: accuracy:0.10400000214576721, confidence:0.9953776597976685, loss:7.308465957641602
epoch1: step3000/4680
step 3500: accuracy:0.11400000005960464, confidence:0.996543824672699, loss:7.068727016448975
epoch1: step3500/4680
step 4000: accuracy:0.10100000351667404, confidence:0.9971579313278198, loss:7.716960430145264
epoch1: step4000/4680
step 4500: accuracy:0.08699999749660492, confidence:0.9988577365875244, loss:8.253175735473633
epoch1: step4500/4680
step 0: accuracy:0.09600000083446503, confidence:0.999527096748352, loss:8.954606056213379
epoch2: step0/4680
step 1000: accuracy:0.10700000077486038, confidence:0.9960944652557373, loss:6.788763523101807
epoch2: step500/4680
step 2000: accuracy:0.09700000286102295, confidence:0.9906525611877441, loss:7.6637492179870605
epoch2: step1000/4680
step 3000: accuracy:0.09399999678134918, confidence:0.9936891794204712, loss:7.51186466217041
epoch2: step1500/4680
step 4000: accuracy:0.11699999868869781, confidence:0.9893484115600586, loss:6.354429244995117
epoch2: step2000/4680
step 5000: accuracy:0.07800000160932541, confidence:0.9928661584854126, loss:6.866248607635498
epoch2: step2500/4680
step 6000: accuracy:0.0949999988079071, confidence:0.9754512906074524, loss:5.835118770599365
epoch2: step3000/4680
step 7000: accuracy:0.1080000028014183, confidence:0.9861197471618652, loss:5.93731689453125
epoch2: step3500/4680
step 8000: accuracy:0.08399999886751175, confidence:0.9975961446762085, loss:7.752026557922363
epoch2: step4000/4680
step 9000: accuracy:0.10599999874830246, confidence:0.9979046583175659, loss:7.437378883361816
epoch2: step4500/4680
step 0: accuracy:0.09600000083446503, confidence:0.9992466568946838, loss:8.444840431213379
epoch3: step0/4680
step 1500: accuracy:0.08699999749660492, confidence:0.9558711647987366, loss:4.564667701721191
epoch3: step500/4680
step 3000: accuracy:0.10300000011920929, confidence:0.8563564419746399, loss:4.072657585144043
epoch3: step1000/4680
step 4500: accuracy:0.11699999868869781, confidence:0.9656859040260315, loss:5.4918413162231445
epoch3: step1500/4680
step 6000: accuracy:0.1289999932050705, confidence:0.9962083697319031, loss:7.0655107498168945
epoch3: step2000/4680
step 7500: accuracy:0.08900000154972076, confidence:0.9877991676330566, loss:6.613826751708984
epoch3: step2500/4680
step 9000: accuracy:0.0989999994635582, confidence:0.9883787035942078, loss:6.807244777679443
epoch3: step3000/4680
step 10500: accuracy:0.13699999451637268, confidence:0.923478901386261, loss:4.709251880645752
epoch3: step3500/4680
step 12000: accuracy:0.09000000357627869, confidence:0.9979073405265808, loss:8.005837440490723
epoch3: step4000/4680
step 13500: accuracy:0.1080000028014183, confidence:0.9986869096755981, loss:7.839346885681152
epoch3: step4500/4680
step 0: accuracy:0.10100000351667404, confidence:0.9995037913322449, loss:8.78125286102295
epoch4: step0/4680
step 2000: accuracy:0.10400000214576721, confidence:0.9659326076507568, loss:5.0307159423828125
epoch4: step500/4680
step 4000: accuracy:0.10199999809265137, confidence:0.9024818539619446, loss:4.787690162658691
epoch4: step1000/4680
step 6000: accuracy:0.10599999874830246, confidence:0.911087155342102, loss:4.386014938354492
epoch4: step1500/4680
step 8000: accuracy:0.1080000028014183, confidence:0.9755825400352478, loss:6.085209369659424
epoch4: step2000/4680
step 10000: accuracy:0.07400000095367432, confidence:0.979813814163208, loss:6.858516693115234
epoch4: step2500/4680
step 12000: accuracy:0.09600000083446503, confidence:0.9911121726036072, loss:7.481688499450684
epoch4: step3000/4680
step 14000: accuracy:0.12999999523162842, confidence:0.9161437749862671, loss:4.966044902801514
epoch4: step3500/4680
step 16000: accuracy:0.10199999809265137, confidence:0.986077070236206, loss:6.783357620239258
epoch4: step4000/4680
step 18000: accuracy:0.10599999874830246, confidence:0.9986819624900818, loss:7.9051594734191895
epoch4: step4500/4680
step 0: accuracy:0.10499999672174454, confidence:0.999556303024292, loss:8.894514083862305
epoch5: step0/4680
step 2500: accuracy:0.11500000208616257, confidence:0.9351792931556702, loss:4.2909345626831055
epoch5: step500/4680
step 5000: accuracy:0.14399999380111694, confidence:0.7485557794570923, loss:3.2338945865631104
epoch5: step1000/4680
step 7500: accuracy:0.17599999904632568, confidence:0.8471826314926147, loss:3.771381139755249
epoch5: step1500/4680
step 10000: accuracy:0.2370000034570694, confidence:0.7715131640434265, loss:2.9573354721069336
epoch5: step2000/4680
step 12500: accuracy:0.17100000381469727, confidence:0.9344092607498169, loss:7.34735631942749
epoch5: step2500/4680
step 15000: accuracy:0.0860000029206276, confidence:0.9912409782409668, loss:7.753129959106445
epoch5: step3000/4680
step 17500: accuracy:0.17000000178813934, confidence:0.8667567372322083, loss:3.976963758468628
epoch5: step3500/4680
step 20000: accuracy:0.10599999874830246, confidence:0.9698386192321777, loss:7.165196418762207
epoch5: step4000/4680
step 22500: accuracy:0.09399999678134918, confidence:0.9920529723167419, loss:6.816832065582275
epoch5: step4500/4680
step 0: accuracy:0.10599999874830246, confidence:0.9979953765869141, loss:7.968628883361816
epoch6: step0/4680
step 3000: accuracy:0.32499998807907104, confidence:0.6900098323822021, loss:2.2441935539245605
epoch6: step500/4680
step 6000: accuracy:0.32199999690055847, confidence:0.6301313638687134, loss:2.1636786460876465
epoch6: step1000/4680
step 9000: accuracy:0.24300000071525574, confidence:0.7299704551696777, loss:2.980076551437378
epoch6: step1500/4680
step 12000: accuracy:0.2709999978542328, confidence:0.7190877795219421, loss:2.4112982749938965
epoch6: step2000/4680
step 15000: accuracy:0.164000004529953, confidence:0.8883858919143677, loss:5.437735080718994
epoch6: step2500/4680
step 18000: accuracy:0.17499999701976776, confidence:0.8977332711219788, loss:4.443994522094727
epoch6: step3000/4680
step 21000: accuracy:0.26100000739097595, confidence:0.8295484185218811, loss:3.1713919639587402
epoch6: step3500/4680
step 24000: accuracy:0.10400000214576721, confidence:0.9716334939002991, loss:7.754551887512207
epoch6: step4000/4680
step 27000: accuracy:0.10100000351667404, confidence:0.9722519516944885, loss:6.13851261138916
epoch6: step4500/4680
step 0: accuracy:0.11299999803304672, confidence:0.9931225180625916, loss:7.171367168426514
epoch7: step0/4680
step 3500: accuracy:0.34299999475479126, confidence:0.7343776822090149, loss:2.6373608112335205
epoch7: step500/4680
step 7000: accuracy:0.3160000145435333, confidence:0.6862725615501404, loss:2.3281500339508057
epoch7: step1000/4680
step 10500: accuracy:0.3269999921321869, confidence:0.7348827123641968, loss:2.5941600799560547
epoch7: step1500/4680
step 14000: accuracy:0.26499998569488525, confidence:0.7463513016700745, loss:2.554368734359741
epoch7: step2000/4680
step 17500: accuracy:0.20499999821186066, confidence:0.8759525418281555, loss:4.411970138549805
epoch7: step2500/4680
step 21000: accuracy:0.16899999976158142, confidence:0.8743026852607727, loss:4.344100475311279
epoch7: step3000/4680
step 24500: accuracy:0.24400000274181366, confidence:0.8380728363990784, loss:3.063778877258301
epoch7: step3500/4680
step 28000: accuracy:0.1120000034570694, confidence:0.9706073999404907, loss:7.516371726989746
epoch7: step4000/4680
step 31500: accuracy:0.11999999731779099, confidence:0.9616388082504272, loss:5.963802337646484
epoch7: step4500/4680
step 0: accuracy:0.0949999988079071, confidence:0.9889118671417236, loss:7.305402755737305
epoch8: step0/4680
step 4000: accuracy:0.3970000147819519, confidence:0.7111310362815857, loss:2.082176923751831
epoch8: step500/4680
step 8000: accuracy:0.18700000643730164, confidence:0.7999467253684998, loss:3.6631293296813965
epoch8: step1000/4680
step 12000: accuracy:0.1899999976158142, confidence:0.8794528841972351, loss:4.025050163269043
epoch8: step1500/4680
step 16000: accuracy:0.2879999876022339, confidence:0.7860437035560608, loss:2.767393112182617
epoch8: step2000/4680
step 20000: accuracy:0.20999999344348907, confidence:0.8808802962303162, loss:4.89866828918457
epoch8: step2500/4680
step 24000: accuracy:0.19900000095367432, confidence:0.8725237250328064, loss:4.301762104034424
epoch8: step3000/4680
step 28000: accuracy:0.23899999260902405, confidence:0.8482903242111206, loss:3.7371699810028076
epoch8: step3500/4680
step 32000: accuracy:0.09600000083446503, confidence:0.9903797507286072, loss:8.526337623596191
epoch8: step4000/4680
step 36000: accuracy:0.12099999934434891, confidence:0.9450454115867615, loss:5.78196382522583
epoch8: step4500/4680
step 0: accuracy:0.10100000351667404, confidence:0.9838019013404846, loss:7.316952228546143
epoch9: step0/4680
step 4500: accuracy:0.3409999907016754, confidence:0.7818115949630737, loss:2.801534652709961
epoch9: step500/4680
step 9000: accuracy:0.328000009059906, confidence:0.7584396004676819, loss:2.5639078617095947
epoch9: step1000/4680
step 13500: accuracy:0.335999995470047, confidence:0.7902540564537048, loss:3.2628560066223145
epoch9: step1500/4680
step 18000: accuracy:0.3619999885559082, confidence:0.7693867087364197, loss:2.4063148498535156
epoch9: step2000/4680
step 22500: accuracy:0.22100000083446503, confidence:0.8418421745300293, loss:4.167878150939941
epoch9: step2500/4680
step 27000: accuracy:0.19599999487400055, confidence:0.8386742472648621, loss:4.461853504180908
epoch9: step3000/4680
step 31500: accuracy:0.1979999989271164, confidence:0.8313753604888916, loss:3.3321619033813477
epoch9: step3500/4680
step 36000: accuracy:0.14300000667572021, confidence:0.9534515142440796, loss:6.953750133514404
epoch9: step4000/4680
step 40500: accuracy:0.18400000035762787, confidence:0.9354320168495178, loss:5.226461410522461
epoch9: step4500/4680
step 0: accuracy:0.13099999725818634, confidence:0.9699511528015137, loss:6.695524215698242
epoch10: step0/4680
step 5000: accuracy:0.3790000081062317, confidence:0.7366610169410706, loss:2.2478559017181396
epoch10: step500/4680
step 10000: accuracy:0.40400001406669617, confidence:0.6683408617973328, loss:1.974455714225769
epoch10: step1000/4680
step 15000: accuracy:0.328000009059906, confidence:0.7191146016120911, loss:2.519104242324829
epoch10: step1500/4680
step 20000: accuracy:0.3499999940395355, confidence:0.7351946830749512, loss:2.264274835586548
epoch10: step2000/4680
step 25000: accuracy:0.23199999332427979, confidence:0.8880318403244019, loss:4.533469200134277
epoch10: step2500/4680
step 30000: accuracy:0.23899999260902405, confidence:0.8592772483825684, loss:3.6445438861846924
epoch10: step3000/4680
step 35000: accuracy:0.2460000067949295, confidence:0.8473034501075745, loss:3.531172037124634
epoch10: step3500/4680
step 40000: accuracy:0.08399999886751175, confidence:0.9837353229522705, loss:8.536376953125
epoch10: step4000/4680
step 45000: accuracy:0.20200000703334808, confidence:0.9082344770431519, loss:5.153109073638916
epoch10: step4500/4680
step 0: accuracy:0.14499999582767487, confidence:0.9501623511314392, loss:6.2474517822265625
epoch11: step0/4680
step 5500: accuracy:0.40799999237060547, confidence:0.7701729536056519, loss:2.6005799770355225
epoch11: step500/4680
step 11000: accuracy:0.33399999141693115, confidence:0.8384147882461548, loss:3.2900824546813965
epoch11: step1000/4680
step 16500: accuracy:0.367000013589859, confidence:0.790544331073761, loss:2.8292620182037354
epoch11: step1500/4680
step 22000: accuracy:0.34200000762939453, confidence:0.7735334634780884, loss:2.5349204540252686
epoch11: step2000/4680
step 27500: accuracy:0.24300000071525574, confidence:0.8664789199829102, loss:4.203072547912598
epoch11: step2500/4680
step 33000: accuracy:0.20100000500679016, confidence:0.8752687573432922, loss:5.4176926612854
epoch11: step3000/4680
step 38500: accuracy:0.15600000321865082, confidence:0.8918822407722473, loss:4.387466907501221
epoch11: step3500/4680
step 44000: accuracy:0.10000000149011612, confidence:0.9856612086296082, loss:8.29371166229248
epoch11: step4000/4680
step 49500: accuracy:0.1509999930858612, confidence:0.9466232061386108, loss:5.513039588928223
epoch11: step4500/4680
step 0: accuracy:0.14000000059604645, confidence:0.971638560295105, loss:6.689640045166016
epoch12: step0/4680
step 6000: accuracy:0.36899998784065247, confidence:0.6943255662918091, loss:2.1596145629882812
epoch12: step500/4680
step 12000: accuracy:0.37299999594688416, confidence:0.6509719491004944, loss:2.087739944458008
epoch12: step1000/4680
step 18000: accuracy:0.30300000309944153, confidence:0.7652840614318848, loss:2.8013081550598145
epoch12: step1500/4680
step 24000: accuracy:0.33500000834465027, confidence:0.7675779461860657, loss:2.472480058670044
epoch12: step2000/4680
step 30000: accuracy:0.18400000035762787, confidence:0.8971642851829529, loss:5.057960033416748
epoch12: step2500/4680
step 36000: accuracy:0.2840000092983246, confidence:0.8723863363265991, loss:3.748361825942993
epoch12: step3000/4680
step 42000: accuracy:0.2240000069141388, confidence:0.8601147532463074, loss:4.189763069152832
epoch12: step3500/4680
step 48000: accuracy:0.11400000005960464, confidence:0.9572799801826477, loss:7.665680885314941
epoch12: step4000/4680
step 54000: accuracy:0.2460000067949295, confidence:0.882382333278656, loss:4.561773777008057
epoch12: step4500/4680
step 0: accuracy:0.1459999978542328, confidence:0.9384639263153076, loss:6.111344337463379
epoch13: step0/4680
step 6500: accuracy:0.328000009059906, confidence:0.8118996620178223, loss:3.2541186809539795
epoch13: step500/4680
step 13000: accuracy:0.31200000643730164, confidence:0.8706920146942139, loss:3.641918897628784
epoch13: step1000/4680
step 19500: accuracy:0.28700000047683716, confidence:0.8424015045166016, loss:3.5843465328216553
epoch13: step1500/4680
step 26000: accuracy:0.34200000762939453, confidence:0.7656140327453613, loss:2.5400454998016357
epoch13: step2000/4680
step 32500: accuracy:0.2280000001192093, confidence:0.8763442039489746, loss:4.387058258056641
epoch13: step2500/4680
step 39000: accuracy:0.2540000081062317, confidence:0.8597980737686157, loss:4.995420455932617
epoch13: step3000/4680
step 45500: accuracy:0.20999999344348907, confidence:0.9006707668304443, loss:4.105913162231445
epoch13: step3500/4680
step 52000: accuracy:0.11999999731779099, confidence:0.972993016242981, loss:7.154996395111084
epoch13: step4000/4680
step 58500: accuracy:0.1589999943971634, confidence:0.9519124627113342, loss:5.473587989807129
epoch13: step4500/4680
step 0: accuracy:0.13199999928474426, confidence:0.9720345735549927, loss:6.676523208618164
epoch14: step0/4680
step 7000: accuracy:0.3709999918937683, confidence:0.6962642073631287, loss:2.1665310859680176
epoch14: step500/4680
step 14000: accuracy:0.3709999918937683, confidence:0.6469692587852478, loss:2.0258684158325195
epoch14: step1000/4680
step 21000: accuracy:0.3569999933242798, confidence:0.7464768886566162, loss:2.7185964584350586
epoch14: step1500/4680
step 28000: accuracy:0.3700000047683716, confidence:0.7543950080871582, loss:2.3438475131988525
epoch14: step2000/4680
step 35000: accuracy:0.20000000298023224, confidence:0.8777326941490173, loss:4.708103656768799
epoch14: step2500/4680
step 42000: accuracy:0.36399999260902405, confidence:0.787808358669281, loss:2.7376632690429688
epoch14: step3000/4680
step 49000: accuracy:0.20999999344348907, confidence:0.8712878227233887, loss:4.053853988647461
epoch14: step3500/4680
step 56000: accuracy:0.09799999743700027, confidence:0.9671869277954102, loss:8.310882568359375
epoch14: step4000/4680
step 63000: accuracy:0.2590000033378601, confidence:0.8820689916610718, loss:4.512804985046387
epoch14: step4500/4680
step 0: accuracy:0.20100000500679016, confidence:0.9185788631439209, loss:5.661404132843018
epoch15: step0/4680
step 7500: accuracy:0.2800000011920929, confidence:0.864667534828186, loss:4.073178768157959
epoch15: step500/4680
step 15000: accuracy:0.33000001311302185, confidence:0.8789824843406677, loss:3.6844232082366943
epoch15: step1000/4680
step 22500: accuracy:0.2290000021457672, confidence:0.8962535858154297, loss:4.297990322113037
epoch15: step1500/4680
step 30000: accuracy:0.30300000309944153, confidence:0.7761974930763245, loss:2.709902763366699
epoch15: step2000/4680
step 37500: accuracy:0.23399999737739563, confidence:0.8762568235397339, loss:4.398207187652588
epoch15: step2500/4680
step 45000: accuracy:0.20000000298023224, confidence:0.8430617451667786, loss:5.292418956756592
epoch15: step3000/4680
step 52500: accuracy:0.19699999690055847, confidence:0.8694632053375244, loss:3.8370397090911865
epoch15: step3500/4680
step 60000: accuracy:0.13600000739097595, confidence:0.9545835852622986, loss:7.3119378089904785
epoch15: step4000/4680
step 67500: accuracy:0.20200000703334808, confidence:0.9332028031349182, loss:4.726244926452637
epoch15: step4500/4680
step 0: accuracy:0.15600000321865082, confidence:0.965550422668457, loss:6.269520282745361
epoch16: step0/4680
step 8000: accuracy:0.35100001096725464, confidence:0.7053408622741699, loss:2.14262056350708
epoch16: step500/4680
step 16000: accuracy:0.3930000066757202, confidence:0.675331711769104, loss:2.0691773891448975
epoch16: step1000/4680
step 24000: accuracy:0.3440000116825104, confidence:0.7635932564735413, loss:2.7888216972351074
epoch16: step1500/4680
step 32000: accuracy:0.3930000066757202, confidence:0.7612928748130798, loss:2.3033647537231445
epoch16: step2000/4680
step 40000: accuracy:0.17399999499320984, confidence:0.8739986419677734, loss:4.704753875732422
epoch16: step2500/4680
step 48000: accuracy:0.3529999852180481, confidence:0.8191384077072144, loss:2.8797779083251953
epoch16: step3000/4680
step 56000: accuracy:0.23399999737739563, confidence:0.8745747208595276, loss:4.012575149536133
epoch16: step3500/4680
step 64000: accuracy:0.09700000286102295, confidence:0.9622087478637695, loss:8.039589881896973
epoch16: step4000/4680
step 72000: accuracy:0.27399998903274536, confidence:0.8603183627128601, loss:4.3502678871154785
epoch16: step4500/4680
step 0: accuracy:0.1889999955892563, confidence:0.9219691753387451, loss:5.7233781814575195
epoch17: step0/4680
step 8500: accuracy:0.28600001335144043, confidence:0.8548965454101562, loss:3.957718849182129
epoch17: step500/4680
step 17000: accuracy:0.2590000033378601, confidence:0.8884717226028442, loss:4.2267303466796875
epoch17: step1000/4680
step 25500: accuracy:0.27000001072883606, confidence:0.9009433388710022, loss:4.178827285766602
epoch17: step1500/4680
step 34000: accuracy:0.3109999895095825, confidence:0.7802350521087646, loss:2.6504406929016113
epoch17: step2000/4680
step 42500: accuracy:0.23399999737739563, confidence:0.8821375966072083, loss:4.7668776512146
epoch17: step2500/4680
step 51000: accuracy:0.2290000021457672, confidence:0.8333731293678284, loss:4.762994289398193
epoch17: step3000/4680
step 59500: accuracy:0.210999995470047, confidence:0.8735555410385132, loss:3.889273166656494
epoch17: step3500/4680
step 68000: accuracy:0.1379999965429306, confidence:0.943372368812561, loss:6.415783405303955
epoch17: step4000/4680
step 76500: accuracy:0.1770000010728836, confidence:0.9401976466178894, loss:5.303714752197266
epoch17: step4500/4680
step 0: accuracy:0.17000000178813934, confidence:0.9594427943229675, loss:6.100266933441162
epoch18: step0/4680
step 9000: accuracy:0.33799999952316284, confidence:0.711186945438385, loss:2.3230767250061035
epoch18: step500/4680
step 18000: accuracy:0.38600000739097595, confidence:0.6585771441459656, loss:1.9925413131713867
epoch18: step1000/4680
step 27000: accuracy:0.3330000042915344, confidence:0.7704834938049316, loss:2.791175127029419
epoch18: step1500/4680
step 36000: accuracy:0.4000000059604645, confidence:0.727253258228302, loss:2.1695656776428223
epoch18: step2000/4680
step 45000: accuracy:0.19599999487400055, confidence:0.8891685009002686, loss:5.154317378997803
epoch18: step2500/4680
step 54000: accuracy:0.27799999713897705, confidence:0.8383229374885559, loss:3.4401862621307373
epoch18: step3000/4680
step 63000: accuracy:0.21699999272823334, confidence:0.8483633995056152, loss:3.7889280319213867
epoch18: step3500/4680
step 72000: accuracy:0.08100000023841858, confidence:0.9769741296768188, loss:8.143424987792969
epoch18: step4000/4680
step 81000: accuracy:0.27000001072883606, confidence:0.8745861053466797, loss:4.461134910583496
epoch18: step4500/4680
step 0: accuracy:0.18199999630451202, confidence:0.9261986613273621, loss:5.666366100311279
epoch19: step0/4680
step 9500: accuracy:0.3019999861717224, confidence:0.8429000377655029, loss:3.8239660263061523
epoch19: step500/4680
step 19000: accuracy:0.2849999964237213, confidence:0.879726767539978, loss:4.106575012207031
epoch19: step1000/4680
step 28500: accuracy:0.289000004529953, confidence:0.8813356161117554, loss:3.803246259689331
epoch19: step1500/4680
step 38000: accuracy:0.36899998784065247, confidence:0.790083110332489, loss:2.4865760803222656
epoch19: step2000/4680
step 47500: accuracy:0.2370000034570694, confidence:0.8726320862770081, loss:4.4041428565979
epoch19: step2500/4680
step 57000: accuracy:0.16200000047683716, confidence:0.9633636474609375, loss:6.650084018707275
epoch19: step3000/4680
step 66500: accuracy:0.27399998903274536, confidence:0.8573490381240845, loss:3.5723307132720947
epoch19: step3500/4680
step 76000: accuracy:0.17000000178813934, confidence:0.9318238496780396, loss:6.459575653076172
epoch19: step4000/4680
step 85500: accuracy:0.17399999499320984, confidence:0.9168475270271301, loss:4.78220796585083
epoch19: step4500/4680
step 0: accuracy:0.15299999713897705, confidence:0.9550784826278687, loss:6.0546979904174805
epoch20: step0/4680
step 10000: accuracy:0.35100001096725464, confidence:0.7165010571479797, loss:2.262158155441284
epoch20: step500/4680
step 20000: accuracy:0.4050000011920929, confidence:0.6927124857902527, loss:2.094068765640259
epoch20: step1000/4680
step 30000: accuracy:0.38600000739097595, confidence:0.7702688574790955, loss:2.5830938816070557
epoch20: step1500/4680
step 40000: accuracy:0.3709999918937683, confidence:0.7752995491027832, loss:2.494032382965088
epoch20: step2000/4680
step 50000: accuracy:0.2280000001192093, confidence:0.8675602674484253, loss:4.632451057434082
epoch20: step2500/4680
step 60000: accuracy:0.33500000834465027, confidence:0.7991539239883423, loss:3.054305076599121
epoch20: step3000/4680
step 70000: accuracy:0.2460000067949295, confidence:0.8842184543609619, loss:4.206234455108643
epoch20: step3500/4680
step 80000: accuracy:0.13099999725818634, confidence:0.9805728793144226, loss:8.382285118103027
epoch20: step4000/4680
step 90000: accuracy:0.26100000739097595, confidence:0.8794146776199341, loss:4.434870719909668
epoch20: step4500/4680
step 0: accuracy:0.1889999955892563, confidence:0.9333201050758362, loss:5.9681243896484375
epoch21: step0/4680
step 10500: accuracy:0.2680000066757202, confidence:0.863893985748291, loss:4.076736927032471
epoch21: step500/4680
step 21000: accuracy:0.28700000047683716, confidence:0.8943786025047302, loss:4.309391498565674
epoch21: step1000/4680
step 31500: accuracy:0.3580000102519989, confidence:0.8479250073432922, loss:3.1562576293945312
epoch21: step1500/4680
step 42000: accuracy:0.38600000739097595, confidence:0.7620192170143127, loss:2.358412265777588
epoch21: step2000/4680
step 52500: accuracy:0.24799999594688416, confidence:0.865917980670929, loss:4.095013618469238
epoch21: step2500/4680
step 63000: accuracy:0.1599999964237213, confidence:0.9265071749687195, loss:7.289584159851074
epoch21: step3000/4680
step 73500: accuracy:0.26100000739097595, confidence:0.871965229511261, loss:3.931671619415283
epoch21: step3500/4680
step 84000: accuracy:0.10100000351667404, confidence:0.9686528444290161, loss:6.956222057342529
epoch21: step4000/4680
step 94500: accuracy:0.16599999368190765, confidence:0.9171713590621948, loss:5.015270233154297
epoch21: step4500/4680
step 0: accuracy:0.1809999942779541, confidence:0.9501593112945557, loss:5.873784065246582
epoch22: step0/4680
step 11000: accuracy:0.367000013589859, confidence:0.7381317019462585, loss:2.3509974479675293
epoch22: step500/4680
step 22000: accuracy:0.4000000059604645, confidence:0.6812688112258911, loss:1.9686150550842285
epoch22: step1000/4680
step 33000: accuracy:0.35899999737739563, confidence:0.7794973850250244, loss:2.7165369987487793
epoch22: step1500/4680
step 44000: accuracy:0.3919999897480011, confidence:0.7458110451698303, loss:2.2163095474243164
epoch22: step2000/4680
step 55000: accuracy:0.2370000034570694, confidence:0.885413408279419, loss:4.651378154754639
epoch22: step2500/4680
step 66000: accuracy:0.3310000002384186, confidence:0.8124542832374573, loss:3.179924249649048
epoch22: step3000/4680
step 77000: accuracy:0.2150000035762787, confidence:0.8998128771781921, loss:4.681222915649414
epoch22: step3500/4680
step 88000: accuracy:0.10999999940395355, confidence:0.9775813221931458, loss:8.715139389038086
epoch22: step4000/4680
step 99000: accuracy:0.25, confidence:0.8877527117729187, loss:4.64265775680542
epoch22: step4500/4680
step 0: accuracy:0.1860000044107437, confidence:0.926728367805481, loss:5.608029842376709
epoch23: step0/4680
step 11500: accuracy:0.3009999990463257, confidence:0.854965090751648, loss:3.740739107131958
epoch23: step500/4680
step 23000: accuracy:0.29899999499320984, confidence:0.8810760974884033, loss:3.7119569778442383
epoch23: step1000/4680
step 34500: accuracy:0.3070000112056732, confidence:0.873090922832489, loss:3.6940436363220215
epoch23: step1500/4680
step 46000: accuracy:0.3700000047683716, confidence:0.7726672887802124, loss:2.4105114936828613
epoch23: step2000/4680
step 57500: accuracy:0.24899999797344208, confidence:0.8694000244140625, loss:4.3274102210998535
epoch23: step2500/4680
step 69000: accuracy:0.20800000429153442, confidence:0.8636760711669922, loss:5.792697906494141
epoch23: step3000/4680
step 80500: accuracy:0.210999995470047, confidence:0.8956769704818726, loss:4.546646595001221
epoch23: step3500/4680
step 92000: accuracy:0.10899999737739563, confidence:0.9557792544364929, loss:6.740830421447754
epoch23: step4000/4680
step 103500: accuracy:0.15600000321865082, confidence:0.9521907567977905, loss:5.896655082702637
epoch23: step4500/4680
step 0: accuracy:0.17000000178813934, confidence:0.9665787220001221, loss:6.4838175773620605
epoch24: step0/4680
step 12000: accuracy:0.32100000977516174, confidence:0.7444987297058105, loss:2.516850471496582
epoch24: step500/4680
step 24000: accuracy:0.4440000057220459, confidence:0.6824349164962769, loss:1.9172132015228271
epoch24: step1000/4680
step 36000: accuracy:0.37700000405311584, confidence:0.7739141583442688, loss:2.7021138668060303
epoch24: step1500/4680
step 48000: accuracy:0.37700000405311584, confidence:0.7486288547515869, loss:2.250610113143921
epoch24: step2000/4680
step 60000: accuracy:0.21899999678134918, confidence:0.8748770952224731, loss:4.551731109619141
epoch24: step2500/4680
step 72000: accuracy:0.33500000834465027, confidence:0.8295638561248779, loss:3.158047676086426
epoch24: step3000/4680
step 84000: accuracy:0.20800000429153442, confidence:0.89463871717453, loss:4.392538070678711
epoch24: step3500/4680
step 96000: accuracy:0.13300000131130219, confidence:0.9516299962997437, loss:7.430657386779785
epoch24: step4000/4680
step 108000: accuracy:0.23100000619888306, confidence:0.8785110712051392, loss:4.625566005706787
epoch24: step4500/4680
step 0: accuracy:0.18000000715255737, confidence:0.9226465225219727, loss:5.732648849487305
epoch25: step0/4680
step 12500: accuracy:0.2639999985694885, confidence:0.8481397032737732, loss:3.750302314758301
epoch25: step500/4680
step 25000: accuracy:0.31200000643730164, confidence:0.8780154585838318, loss:3.8304057121276855
epoch25: step1000/4680
step 37500: accuracy:0.32100000977516174, confidence:0.8560754060745239, loss:3.387486219406128
epoch25: step1500/4680
step 50000: accuracy:0.37599998712539673, confidence:0.7883530855178833, loss:2.566833972930908
epoch25: step2000/4680
step 62500: accuracy:0.2549999952316284, confidence:0.871515691280365, loss:4.303125381469727
epoch25: step2500/4680
step 75000: accuracy:0.1469999998807907, confidence:0.96675044298172, loss:7.5844597816467285
epoch25: step3000/4680
step 87500: accuracy:0.23999999463558197, confidence:0.8716974258422852, loss:3.8502414226531982
epoch25: step3500/4680
step 100000: accuracy:0.13899999856948853, confidence:0.9331741333007812, loss:6.524210453033447
epoch25: step4000/4680
step 112500: accuracy:0.17000000178813934, confidence:0.9370205402374268, loss:5.107819557189941
epoch25: step4500/4680
step 0: accuracy:0.1679999977350235, confidence:0.9551925659179688, loss:6.098822116851807
epoch26: step0/4680
step 13000: accuracy:0.36000001430511475, confidence:0.7316696643829346, loss:2.3678836822509766
epoch26: step500/4680
step 26000: accuracy:0.42500001192092896, confidence:0.6891342401504517, loss:2.010385751724243
epoch26: step1000/4680
step 39000: accuracy:0.3720000088214874, confidence:0.7684258818626404, loss:2.5501561164855957
epoch26: step1500/4680
step 52000: accuracy:0.3790000081062317, confidence:0.7585082054138184, loss:2.434765577316284
epoch26: step2000/4680
step 65000: accuracy:0.2150000035762787, confidence:0.8638425469398499, loss:4.712283611297607
epoch26: step2500/4680
step 78000: accuracy:0.32499998807907104, confidence:0.8276324272155762, loss:3.381166458129883
epoch26: step3000/4680
step 91000: accuracy:0.20999999344348907, confidence:0.8903959393501282, loss:4.874960422515869
epoch26: step3500/4680
step 104000: accuracy:0.10300000011920929, confidence:0.9679433107376099, loss:8.952232360839844
epoch26: step4000/4680
step 117000: accuracy:0.25699999928474426, confidence:0.8960643410682678, loss:5.095125675201416
epoch26: step4500/4680
step 0: accuracy:0.17599999904632568, confidence:0.9388777017593384, loss:6.238938331604004
epoch27: step0/4680
step 13500: accuracy:0.27399998903274536, confidence:0.8617928624153137, loss:4.316333770751953
epoch27: step500/4680
step 27000: accuracy:0.3019999861717224, confidence:0.8855751752853394, loss:4.215688705444336
epoch27: step1000/4680
step 40500: accuracy:0.34200000762939453, confidence:0.8561791777610779, loss:3.299938440322876
epoch27: step1500/4680
step 54000: accuracy:0.38100001215934753, confidence:0.7722564935684204, loss:2.4243080615997314
epoch27: step2000/4680
step 67500: accuracy:0.23000000417232513, confidence:0.8835869431495667, loss:4.935229301452637
epoch27: step2500/4680
step 81000: accuracy:0.210999995470047, confidence:0.8841018080711365, loss:5.033741474151611
epoch27: step3000/4680
step 94500: accuracy:0.2590000033378601, confidence:0.8766362071037292, loss:3.5903806686401367
epoch27: step3500/4680
step 108000: accuracy:0.1379999965429306, confidence:0.9291557669639587, loss:6.759151458740234
epoch27: step4000/4680
step 121500: accuracy:0.18799999356269836, confidence:0.9312706589698792, loss:5.131916046142578
epoch27: step4500/4680
step 0: accuracy:0.18400000035762787, confidence:0.9527350664138794, loss:5.834901809692383
epoch28: step0/4680
step 14000: accuracy:0.4129999876022339, confidence:0.7456458806991577, loss:2.1987032890319824
epoch28: step500/4680
step 28000: accuracy:0.41499999165534973, confidence:0.7009558081626892, loss:2.132547616958618
epoch28: step1000/4680
step 42000: accuracy:0.38999998569488525, confidence:0.7926000952720642, loss:2.805232048034668
epoch28: step1500/4680
step 56000: accuracy:0.3919999897480011, confidence:0.7553325295448303, loss:2.3740346431732178
epoch28: step2000/4680
step 70000: accuracy:0.20600000023841858, confidence:0.8780337572097778, loss:4.7722954750061035
epoch28: step2500/4680
step 84000: accuracy:0.32100000977516174, confidence:0.8372352123260498, loss:3.2750115394592285
epoch28: step3000/4680
step 98000: accuracy:0.23399999737739563, confidence:0.8758836388587952, loss:3.981015682220459
epoch28: step3500/4680
step 112000: accuracy:0.13099999725818634, confidence:0.9518991708755493, loss:8.026398658752441
epoch28: step4000/4680
step 126000: accuracy:0.2529999911785126, confidence:0.8647108674049377, loss:4.554852485656738
epoch28: step4500/4680
step 0: accuracy:0.20200000703334808, confidence:0.9164984226226807, loss:5.544834136962891
epoch29: step0/4680
step 14500: accuracy:0.25, confidence:0.8837025165557861, loss:4.669853210449219
epoch29: step500/4680
step 29000: accuracy:0.2750000059604645, confidence:0.8961789608001709, loss:4.3586554527282715
epoch29: step1000/4680
step 43500: accuracy:0.33000001311302185, confidence:0.8578911423683167, loss:3.168531894683838
epoch29: step1500/4680
step 58000: accuracy:0.41600000858306885, confidence:0.7512933611869812, loss:2.090975522994995
epoch29: step2000/4680
step 72500: accuracy:0.23800000548362732, confidence:0.8753989934921265, loss:4.4771857261657715
epoch29: step2500/4680
step 87000: accuracy:0.14300000667572021, confidence:0.960873007774353, loss:6.710084915161133
epoch29: step3000/4680
step 101500: accuracy:0.23600000143051147, confidence:0.8625235557556152, loss:3.710280656814575
epoch29: step3500/4680
step 116000: accuracy:0.125, confidence:0.9507169127464294, loss:6.844444274902344
epoch29: step4000/4680
step 130500: accuracy:0.15299999713897705, confidence:0.9323334693908691, loss:5.310318470001221
epoch29: step4500/4680
2018-06-15 18:01:20.235239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 18:01:20.235440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
step 0: accuracy:0.09399999678134918, confidence:0.7578234076499939, loss:7.556766033172607
epoch0: step0/4680
step 0: accuracy:0.09000000357627869, confidence:1.0, loss:76.59684753417969
epoch0: step500/4680
step 0: accuracy:0.08799999952316284, confidence:1.0, loss:27.795673370361328
epoch0: step1000/4680
step 0: accuracy:0.10400000214576721, confidence:0.9997643828392029, loss:10.599282264709473
epoch0: step1500/4680
step 0: accuracy:0.10000000149011612, confidence:0.9999419450759888, loss:11.906137466430664
epoch0: step2000/4680
step 0: accuracy:0.09300000220537186, confidence:0.9990254044532776, loss:8.737295150756836
epoch0: step2500/4680
step 0: accuracy:0.10700000077486038, confidence:0.9999855756759644, loss:12.617340087890625
epoch0: step3000/4680
step 0: accuracy:0.11299999803304672, confidence:0.9999616146087646, loss:12.203583717346191
epoch0: step3500/4680
step 0: accuracy:0.10100000351667404, confidence:0.9997924566268921, loss:10.754076957702637
epoch0: step4000/4680
step 0: accuracy:0.08900000154972076, confidence:0.9997097849845886, loss:10.821704864501953
epoch0: step4500/4680
step 0: accuracy:0.1120000034570694, confidence:0.9998241066932678, loss:10.838695526123047
epoch1: step0/4680
step 500: accuracy:0.09600000083446503, confidence:0.9962553381919861, loss:7.555020809173584
epoch1: step500/4680
step 1000: accuracy:0.1120000034570694, confidence:0.9999700784683228, loss:12.745430946350098
epoch1: step1000/4680
step 1500: accuracy:0.0860000029206276, confidence:0.9997761845588684, loss:10.472414016723633
epoch1: step1500/4680
step 2000: accuracy:0.10400000214576721, confidence:0.999740481376648, loss:9.8716402053833
epoch1: step2000/4680
step 2500: accuracy:0.09200000017881393, confidence:0.9999068379402161, loss:12.06031608581543
epoch1: step2500/4680
step 3000: accuracy:0.08100000023841858, confidence:0.999701976776123, loss:10.052277565002441
epoch1: step3000/4680
step 3500: accuracy:0.11599999666213989, confidence:0.9999846816062927, loss:12.35217571258545
epoch1: step3500/4680
step 4000: accuracy:0.10400000214576721, confidence:0.9999514222145081, loss:11.53898811340332
epoch1: step4000/4680
step 4500: accuracy:0.08900000154972076, confidence:0.9999380111694336, loss:10.963892936706543
epoch1: step4500/4680
step 0: accuracy:0.09000000357627869, confidence:0.9998276829719543, loss:9.986359596252441
epoch2: step0/4680
step 1000: accuracy:0.08900000154972076, confidence:0.999962329864502, loss:12.126115798950195
epoch2: step500/4680
step 2000: accuracy:0.11500000208616257, confidence:0.999947726726532, loss:11.476605415344238
epoch2: step1000/4680
step 3000: accuracy:0.09799999743700027, confidence:0.9998660683631897, loss:10.795578002929688
epoch2: step1500/4680
step 4000: accuracy:0.09200000017881393, confidence:0.9998971223831177, loss:11.935626983642578
epoch2: step2000/4680
step 5000: accuracy:0.07800000160932541, confidence:0.9998178482055664, loss:11.048588752746582
epoch2: step2500/4680
step 6000: accuracy:0.08500000089406967, confidence:0.9994381070137024, loss:9.85204029083252
epoch2: step3000/4680
step 7000: accuracy:0.09000000357627869, confidence:0.999856173992157, loss:10.463505744934082
epoch2: step3500/4680
step 8000: accuracy:0.10199999809265137, confidence:0.9997998476028442, loss:10.392570495605469
epoch2: step4000/4680
step 9000: accuracy:0.09799999743700027, confidence:0.9997488856315613, loss:9.514044761657715
epoch2: step4500/4680
step 0: accuracy:0.10700000077486038, confidence:0.9997009038925171, loss:9.258689880371094
epoch3: step0/4680
step 1500: accuracy:0.09799999743700027, confidence:0.9995965957641602, loss:9.625288963317871
epoch3: step500/4680
step 3000: accuracy:0.09200000017881393, confidence:0.9998055696487427, loss:10.150864601135254
epoch3: step1000/4680
step 4500: accuracy:0.11500000208616257, confidence:0.999829888343811, loss:11.117213249206543
epoch3: step1500/4680
step 6000: accuracy:0.11100000143051147, confidence:0.9992842078208923, loss:9.533329010009766
epoch3: step2000/4680
step 7500: accuracy:0.08900000154972076, confidence:0.9997501373291016, loss:10.676767349243164
epoch3: step2500/4680
step 9000: accuracy:0.11800000071525574, confidence:0.9993171095848083, loss:9.313210487365723
epoch3: step3000/4680
step 10500: accuracy:0.0989999994635582, confidence:0.9996699690818787, loss:9.730648040771484
epoch3: step3500/4680
step 12000: accuracy:0.11299999803304672, confidence:0.9995642304420471, loss:9.734477996826172
epoch3: step4000/4680
step 13500: accuracy:0.08799999952316284, confidence:0.9994053244590759, loss:8.899214744567871
epoch3: step4500/4680
step 0: accuracy:0.09799999743700027, confidence:0.9994113445281982, loss:8.765249252319336
epoch4: step0/4680
step 2000: accuracy:0.09300000220537186, confidence:0.9900825023651123, loss:7.009460926055908
epoch4: step500/4680
step 4000: accuracy:0.12099999934434891, confidence:0.9994044899940491, loss:8.944280624389648
epoch4: step1000/4680
step 6000: accuracy:0.10100000351667404, confidence:0.9993935823440552, loss:9.41860580444336
epoch4: step1500/4680
step 8000: accuracy:0.1080000028014183, confidence:0.9992275834083557, loss:9.250398635864258
epoch4: step2000/4680
step 10000: accuracy:0.07400000095367432, confidence:0.9993608593940735, loss:9.439356803894043
epoch4: step2500/4680
step 12000: accuracy:0.09799999743700027, confidence:0.9991809725761414, loss:9.430166244506836
epoch4: step3000/4680
step 14000: accuracy:0.09799999743700027, confidence:0.9992173910140991, loss:9.274726867675781
epoch4: step3500/4680
step 16000: accuracy:0.09399999678134918, confidence:0.9985055923461914, loss:8.656329154968262
epoch4: step4000/4680
step 18000: accuracy:0.10000000149011612, confidence:0.9990065693855286, loss:8.400498390197754
epoch4: step4500/4680
step 0: accuracy:0.09700000286102295, confidence:0.999086856842041, loss:8.413446426391602
epoch5: step0/4680
step 2500: accuracy:0.0989999994635582, confidence:0.9922332167625427, loss:7.1381964683532715
epoch5: step500/4680
step 5000: accuracy:0.12600000202655792, confidence:0.9990808367729187, loss:8.249687194824219
epoch5: step1000/4680
step 7500: accuracy:0.0949999988079071, confidence:0.9981702566146851, loss:8.215715408325195
epoch5: step1500/4680
step 10000: accuracy:0.1120000034570694, confidence:0.9986616373062134, loss:8.611488342285156
epoch5: step2000/4680
step 12500: accuracy:0.07800000160932541, confidence:0.999496579170227, loss:9.93720531463623
epoch5: step2500/4680
step 15000: accuracy:0.09600000083446503, confidence:0.999252438545227, loss:9.310046195983887
epoch5: step3000/4680
step 17500: accuracy:0.10899999737739563, confidence:0.9995940327644348, loss:9.732420921325684
epoch5: step3500/4680
step 20000: accuracy:0.09200000017881393, confidence:0.9982506036758423, loss:8.49892807006836
epoch5: step4000/4680
step 22500: accuracy:0.0860000029206276, confidence:0.9984776973724365, loss:8.28345012664795
epoch5: step4500/4680
step 0: accuracy:0.08500000089406967, confidence:0.9988105893135071, loss:8.43042278289795
epoch6: step0/4680
step 3000: accuracy:0.09799999743700027, confidence:0.9690743088722229, loss:6.024797439575195
epoch6: step500/4680
step 6000: accuracy:0.11500000208616257, confidence:0.9975016117095947, loss:7.499244689941406
epoch6: step1000/4680
step 9000: accuracy:0.09300000220537186, confidence:0.9959049224853516, loss:7.514854431152344
epoch6: step1500/4680
step 12000: accuracy:0.10199999809265137, confidence:0.9984431862831116, loss:8.644572257995605
epoch6: step2000/4680
step 15000: accuracy:0.07999999821186066, confidence:0.99879390001297, loss:9.190166473388672
epoch6: step2500/4680
step 18000: accuracy:0.09600000083446503, confidence:0.9991291165351868, loss:9.33460521697998
epoch6: step3000/4680
step 21000: accuracy:0.10199999809265137, confidence:0.9993557333946228, loss:9.48756217956543
epoch6: step3500/4680
step 24000: accuracy:0.10199999809265137, confidence:0.9981909394264221, loss:8.421723365783691
epoch6: step4000/4680
step 27000: accuracy:0.0860000029206276, confidence:0.9986400008201599, loss:8.372559547424316
epoch6: step4500/4680
step 0: accuracy:0.09200000017881393, confidence:0.9988713264465332, loss:8.450855255126953
epoch7: step0/4680
step 3500: accuracy:0.10000000149011612, confidence:0.9805803894996643, loss:6.297598361968994
epoch7: step500/4680
step 7000: accuracy:0.1080000028014183, confidence:0.9862082600593567, loss:6.1709370613098145
epoch7: step1000/4680
step 10500: accuracy:0.10000000149011612, confidence:0.9897018074989319, loss:6.37339448928833
epoch7: step1500/4680
step 14000: accuracy:0.125, confidence:0.9984695315361023, loss:8.297197341918945
epoch7: step2000/4680
step 17500: accuracy:0.07900000363588333, confidence:0.9963495135307312, loss:8.134538650512695
epoch7: step2500/4680
step 21000: accuracy:0.10300000011920929, confidence:0.9988537430763245, loss:9.154324531555176
epoch7: step3000/4680
step 24500: accuracy:0.0860000029206276, confidence:0.9985541105270386, loss:8.69905948638916
epoch7: step3500/4680
step 28000: accuracy:0.10300000011920929, confidence:0.9980714917182922, loss:8.168234825134277
epoch7: step4000/4680
step 31500: accuracy:0.11599999666213989, confidence:0.9988583326339722, loss:8.243953704833984
epoch7: step4500/4680
step 0: accuracy:0.10700000077486038, confidence:0.9990220069885254, loss:8.376935958862305
epoch8: step0/4680
step 4000: accuracy:0.09799999743700027, confidence:0.985471248626709, loss:6.474227428436279
epoch8: step500/4680
step 8000: accuracy:0.09799999743700027, confidence:0.952117919921875, loss:4.870182037353516
epoch8: step1000/4680
step 12000: accuracy:0.11299999803304672, confidence:0.9587014317512512, loss:4.972057342529297
epoch8: step1500/4680
step 16000: accuracy:0.09000000357627869, confidence:0.9984850883483887, loss:8.683384895324707
epoch8: step2000/4680
step 20000: accuracy:0.09200000017881393, confidence:0.9976238012313843, loss:8.425114631652832
epoch8: step2500/4680
step 24000: accuracy:0.10499999672174454, confidence:0.9978631734848022, loss:8.486831665039062
epoch8: step3000/4680
step 28000: accuracy:0.09099999815225601, confidence:0.9983912706375122, loss:8.43232536315918
epoch8: step3500/4680
step 32000: accuracy:0.09200000017881393, confidence:0.995796263217926, loss:7.464046955108643
epoch8: step4000/4680
step 36000: accuracy:0.09099999815225601, confidence:0.9987909197807312, loss:8.599234580993652
epoch8: step4500/4680
step 0: accuracy:0.10499999672174454, confidence:0.9989749193191528, loss:8.450565338134766
epoch9: step0/4680
step 4500: accuracy:0.09399999678134918, confidence:0.939356803894043, loss:5.142960071563721
epoch9: step500/4680
step 9000: accuracy:0.11900000274181366, confidence:0.8971949219703674, loss:4.302074432373047
epoch9: step1000/4680
step 13500: accuracy:0.13899999856948853, confidence:0.7617404460906982, loss:3.4017281532287598
epoch9: step1500/4680
step 18000: accuracy:0.09000000357627869, confidence:0.993859052658081, loss:7.450540542602539
epoch9: step2000/4680
step 22500: accuracy:0.0949999988079071, confidence:0.9983851313591003, loss:8.59189510345459
epoch9: step2500/4680
step 27000: accuracy:0.09200000017881393, confidence:0.9981144666671753, loss:8.260249137878418
epoch9: step3000/4680
step 31500: accuracy:0.09700000286102295, confidence:0.9990494251251221, loss:8.663994789123535
epoch9: step3500/4680
step 36000: accuracy:0.11900000274181366, confidence:0.9944194555282593, loss:6.937645435333252
epoch9: step4000/4680
step 40500: accuracy:0.09799999743700027, confidence:0.9969211220741272, loss:7.738153457641602
epoch9: step4500/4680
step 0: accuracy:0.11400000005960464, confidence:0.9976105690002441, loss:7.768535137176514
epoch10: step0/4680
step 5000: accuracy:0.12300000339746475, confidence:0.8812587261199951, loss:4.175360679626465
epoch10: step500/4680
step 10000: accuracy:0.1679999977350235, confidence:0.7825245261192322, loss:3.2161777019500732
epoch10: step1000/4680
step 15000: accuracy:0.20600000023841858, confidence:0.7144170999526978, loss:2.8929946422576904
epoch10: step1500/4680
step 20000: accuracy:0.09300000220537186, confidence:0.9800461530685425, loss:7.812763690948486
epoch10: step2000/4680
step 25000: accuracy:0.10000000149011612, confidence:0.9928648471832275, loss:8.38033676147461
epoch10: step2500/4680
step 30000: accuracy:0.09600000083446503, confidence:0.9978653192520142, loss:8.385281562805176
epoch10: step3000/4680
step 35000: accuracy:0.10700000077486038, confidence:0.9984387159347534, loss:8.094463348388672
epoch10: step3500/4680
step 40000: accuracy:0.0989999994635582, confidence:0.9916995763778687, loss:6.747476100921631
epoch10: step4000/4680
step 45000: accuracy:0.10100000351667404, confidence:0.997121274471283, loss:7.687730312347412
epoch10: step4500/4680
step 0: accuracy:0.10599999874830246, confidence:0.9973064064979553, loss:7.708877086639404
epoch11: step0/4680
step 5500: accuracy:0.1509999930858612, confidence:0.8729262948036194, loss:4.130892276763916
epoch11: step500/4680
step 11000: accuracy:0.22499999403953552, confidence:0.7571465373039246, loss:2.934988498687744
epoch11: step1000/4680
step 16500: accuracy:0.23600000143051147, confidence:0.7118945121765137, loss:2.7988221645355225
epoch11: step1500/4680
step 22000: accuracy:0.09700000286102295, confidence:0.9790993928909302, loss:7.0427751541137695
epoch11: step2000/4680
step 27500: accuracy:0.07500000298023224, confidence:0.9852463006973267, loss:8.38306713104248
epoch11: step2500/4680
step 33000: accuracy:0.09700000286102295, confidence:0.9944167733192444, loss:8.557441711425781
epoch11: step3000/4680
step 38500: accuracy:0.10000000149011612, confidence:0.9944275617599487, loss:7.255068778991699
epoch11: step3500/4680
step 44000: accuracy:0.08699999749660492, confidence:0.974678099155426, loss:6.136270999908447
epoch11: step4000/4680
step 49500: accuracy:0.10400000214576721, confidence:0.9901289343833923, loss:6.893180847167969
epoch11: step4500/4680
step 0: accuracy:0.10499999672174454, confidence:0.9920381903648376, loss:6.9377760887146
epoch12: step0/4680
step 6000: accuracy:0.22200000286102295, confidence:0.7736750245094299, loss:2.8848941326141357
epoch12: step500/4680
step 12000: accuracy:0.2590000033378601, confidence:0.7372108101844788, loss:2.5067992210388184
epoch12: step1000/4680
step 18000: accuracy:0.2709999978542328, confidence:0.7408921718597412, loss:2.706780433654785
epoch12: step1500/4680
step 24000: accuracy:0.11999999731779099, confidence:0.9397447109222412, loss:6.119383811950684
epoch12: step2000/4680
step 30000: accuracy:0.10000000149011612, confidence:0.9721842408180237, loss:7.7147626876831055
epoch12: step2500/4680
step 36000: accuracy:0.11400000005960464, confidence:0.9943979382514954, loss:8.469625473022461
epoch12: step3000/4680
step 42000: accuracy:0.0989999994635582, confidence:0.995661199092865, loss:7.596810817718506
epoch12: step3500/4680
step 48000: accuracy:0.1120000034570694, confidence:0.9468706250190735, loss:5.280737400054932
epoch12: step4000/4680
step 54000: accuracy:0.09000000357627869, confidence:0.9773970246315002, loss:6.871951103210449
epoch12: step4500/4680
step 0: accuracy:0.12200000137090683, confidence:0.9798915982246399, loss:6.650894641876221
epoch13: step0/4680
step 6500: accuracy:0.25999999046325684, confidence:0.7341785430908203, loss:2.46917462348938
epoch13: step500/4680
step 13000: accuracy:0.31200000643730164, confidence:0.7347860336303711, loss:2.201131820678711
epoch13: step1000/4680
step 19500: accuracy:0.28600001335144043, confidence:0.7602040767669678, loss:2.7267863750457764
epoch13: step1500/4680
step 26000: accuracy:0.14499999582767487, confidence:0.9155293703079224, loss:5.218997955322266
epoch13: step2000/4680
step 32500: accuracy:0.1080000028014183, confidence:0.9777541160583496, loss:8.24619197845459
epoch13: step2500/4680
step 39000: accuracy:0.09399999678134918, confidence:0.9911882877349854, loss:8.647371292114258
epoch13: step3000/4680
step 45500: accuracy:0.07699999958276749, confidence:0.9913959503173828, loss:7.25350284576416
epoch13: step3500/4680
step 52000: accuracy:0.11699999868869781, confidence:0.9341644644737244, loss:5.436054706573486
epoch13: step4000/4680
step 58500: accuracy:0.11900000274181366, confidence:0.9643762111663818, loss:6.210806846618652
epoch13: step4500/4680
step 0: accuracy:0.10300000011920929, confidence:0.9653414487838745, loss:6.5051093101501465
epoch14: step0/4680
step 7000: accuracy:0.2770000100135803, confidence:0.7509664297103882, loss:2.469942331314087
epoch14: step500/4680
step 14000: accuracy:0.35199999809265137, confidence:0.7357131242752075, loss:2.1506781578063965
epoch14: step1000/4680
step 21000: accuracy:0.33000001311302185, confidence:0.7641053199768066, loss:2.6219255924224854
epoch14: step1500/4680
step 28000: accuracy:0.14800000190734863, confidence:0.9149022698402405, loss:5.089326858520508
epoch14: step2000/4680
step 35000: accuracy:0.11800000071525574, confidence:0.9653810262680054, loss:7.438377380371094
epoch14: step2500/4680
step 42000: accuracy:0.08299999684095383, confidence:0.9908440113067627, loss:9.030139923095703
epoch14: step3000/4680
step 49000: accuracy:0.08799999952316284, confidence:0.9895258545875549, loss:6.818034648895264
epoch14: step3500/4680
step 56000: accuracy:0.10300000011920929, confidence:0.9247506260871887, loss:5.292603969573975
epoch14: step4000/4680
step 63000: accuracy:0.12200000137090683, confidence:0.9755417704582214, loss:6.544203281402588
epoch14: step4500/4680
step 0: accuracy:0.10899999737739563, confidence:0.9676864147186279, loss:6.627660274505615
epoch15: step0/4680
step 7500: accuracy:0.31700000166893005, confidence:0.7369071245193481, loss:2.2930662631988525
epoch15: step500/4680
step 15000: accuracy:0.3019999861717224, confidence:0.7327259182929993, loss:2.1960668563842773
epoch15: step1000/4680
step 22500: accuracy:0.30399999022483826, confidence:0.7632213234901428, loss:2.754408359527588
epoch15: step1500/4680
step 30000: accuracy:0.14100000262260437, confidence:0.9166780114173889, loss:5.0705156326293945
epoch15: step2000/4680
step 37500: accuracy:0.11400000005960464, confidence:0.9741680026054382, loss:8.09334659576416
epoch15: step2500/4680
step 45000: accuracy:0.10100000351667404, confidence:0.980671226978302, loss:8.66362476348877
epoch15: step3000/4680
step 52500: accuracy:0.10700000077486038, confidence:0.9815572500228882, loss:6.761371612548828
epoch15: step3500/4680
step 60000: accuracy:0.10100000351667404, confidence:0.9344034790992737, loss:5.407589912414551
epoch15: step4000/4680
step 67500: accuracy:0.14100000262260437, confidence:0.9534820318222046, loss:6.067873477935791
epoch15: step4500/4680
step 0: accuracy:0.125, confidence:0.9527093768119812, loss:5.959460735321045
epoch16: step0/4680
step 8000: accuracy:0.26600000262260437, confidence:0.7908579111099243, loss:2.8391201496124268
epoch16: step500/4680
step 16000: accuracy:0.35100001096725464, confidence:0.7519629597663879, loss:2.221921443939209
epoch16: step1000/4680
step 24000: accuracy:0.3089999854564667, confidence:0.788844645023346, loss:2.9065234661102295
epoch16: step1500/4680
step 32000: accuracy:0.12399999797344208, confidence:0.9303311705589294, loss:5.55109977722168
epoch16: step2000/4680
step 40000: accuracy:0.10499999672174454, confidence:0.9729814529418945, loss:7.871774673461914
epoch16: step2500/4680
step 48000: accuracy:0.10100000351667404, confidence:0.9635187983512878, loss:7.783361434936523
epoch16: step3000/4680
step 56000: accuracy:0.09799999743700027, confidence:0.9797229766845703, loss:6.931912422180176
epoch16: step3500/4680
step 64000: accuracy:0.12800000607967377, confidence:0.9217640161514282, loss:5.098094940185547
epoch16: step4000/4680
step 72000: accuracy:0.12099999934434891, confidence:0.9652678370475769, loss:6.2932515144348145
epoch16: step4500/4680
step 0: accuracy:0.10999999940395355, confidence:0.9505531787872314, loss:6.204347133636475
epoch17: step0/4680
step 8500: accuracy:0.3499999940395355, confidence:0.7494154572486877, loss:2.290144681930542
epoch17: step500/4680
step 17000: accuracy:0.4129999876022339, confidence:0.7203741073608398, loss:1.838724136352539
epoch17: step1000/4680
step 25500: accuracy:0.3720000088214874, confidence:0.7704480886459351, loss:2.4581704139709473
epoch17: step1500/4680
step 34000: accuracy:0.17000000178813934, confidence:0.9034205675125122, loss:5.008260726928711
epoch17: step2000/4680
step 42500: accuracy:0.12300000339746475, confidence:0.9771621823310852, loss:8.438224792480469
epoch17: step2500/4680
step 51000: accuracy:0.11900000274181366, confidence:0.9666136503219604, loss:7.7907586097717285
epoch17: step3000/4680
step 59500: accuracy:0.09399999678134918, confidence:0.9840051531791687, loss:7.2306132316589355
epoch17: step3500/4680
step 68000: accuracy:0.14300000667572021, confidence:0.9190059900283813, loss:4.900189399719238
epoch17: step4000/4680
step 76500: accuracy:0.1379999965429306, confidence:0.9485024213790894, loss:6.144223690032959
epoch17: step4500/4680
step 0: accuracy:0.13500000536441803, confidence:0.9465665221214294, loss:5.891491413116455
epoch18: step0/4680
step 9000: accuracy:0.34299999475479126, confidence:0.7998479604721069, loss:2.6746513843536377
epoch18: step500/4680
step 18000: accuracy:0.4009999930858612, confidence:0.7598922252655029, loss:2.122842788696289
epoch18: step1000/4680
step 27000: accuracy:0.35100001096725464, confidence:0.7872133851051331, loss:2.5892388820648193
epoch18: step1500/4680
step 36000: accuracy:0.1679999977350235, confidence:0.897617757320404, loss:5.079488277435303
epoch18: step2000/4680
step 45000: accuracy:0.12399999797344208, confidence:0.9705986380577087, loss:8.031496047973633
epoch18: step2500/4680
step 54000: accuracy:0.1289999932050705, confidence:0.966300368309021, loss:7.812967777252197
epoch18: step3000/4680
step 63000: accuracy:0.09700000286102295, confidence:0.9633759260177612, loss:6.598696708679199
epoch18: step3500/4680
step 72000: accuracy:0.14100000262260437, confidence:0.9291271567344666, loss:5.171102523803711
epoch18: step4000/4680
step 81000: accuracy:0.14000000059604645, confidence:0.9568645358085632, loss:5.993354797363281
epoch18: step4500/4680
step 0: accuracy:0.1550000011920929, confidence:0.9417270421981812, loss:5.782601833343506
epoch19: step0/4680
step 9500: accuracy:0.3869999945163727, confidence:0.7586304545402527, loss:2.2961344718933105
epoch19: step500/4680
step 19000: accuracy:0.44699999690055847, confidence:0.75422602891922, loss:1.80644953250885
epoch19: step1000/4680
step 28500: accuracy:0.33799999952316284, confidence:0.7771303653717041, loss:2.588588237762451
epoch19: step1500/4680
step 38000: accuracy:0.14499999582767487, confidence:0.9040729403495789, loss:4.989900588989258
epoch19: step2000/4680
step 47500: accuracy:0.12800000607967377, confidence:0.9598100781440735, loss:7.133431434631348
epoch19: step2500/4680
step 57000: accuracy:0.12300000339746475, confidence:0.9571709036827087, loss:7.8209919929504395
epoch19: step3000/4680
step 66500: accuracy:0.09799999743700027, confidence:0.9586496353149414, loss:6.8328118324279785
epoch19: step3500/4680
step 76000: accuracy:0.14800000190734863, confidence:0.9026529788970947, loss:4.522125720977783
epoch19: step4000/4680
step 85500: accuracy:0.13699999451637268, confidence:0.9542145729064941, loss:6.06511926651001
epoch19: step4500/4680
step 0: accuracy:0.14000000059604645, confidence:0.945830225944519, loss:5.6928019523620605
epoch20: step0/4680
step 10000: accuracy:0.3630000054836273, confidence:0.7778235673904419, loss:2.3825504779815674
epoch20: step500/4680
step 20000: accuracy:0.41600000858306885, confidence:0.7483000159263611, loss:1.903275728225708
epoch20: step1000/4680
step 30000: accuracy:0.4169999957084656, confidence:0.7976875901222229, loss:2.352571725845337
epoch20: step1500/4680
step 40000: accuracy:0.1420000046491623, confidence:0.9410327076911926, loss:5.822535991668701
epoch20: step2000/4680
step 50000: accuracy:0.12700000405311584, confidence:0.9576379656791687, loss:7.16267728805542
epoch20: step2500/4680
step 60000: accuracy:0.14499999582767487, confidence:0.9579482674598694, loss:7.477161884307861
epoch20: step3000/4680
step 70000: accuracy:0.10300000011920929, confidence:0.9634129405021667, loss:6.8289690017700195
epoch20: step3500/4680
step 80000: accuracy:0.15299999713897705, confidence:0.89431232213974, loss:4.405961036682129
epoch20: step4000/4680
step 90000: accuracy:0.1550000011920929, confidence:0.9388799667358398, loss:5.799225807189941
epoch20: step4500/4680
step 0: accuracy:0.17399999499320984, confidence:0.933408260345459, loss:5.361527919769287
epoch21: step0/4680
step 10500: accuracy:0.33399999141693115, confidence:0.8279755711555481, loss:2.94840931892395
epoch21: step500/4680
step 21000: accuracy:0.4230000078678131, confidence:0.7754185795783997, loss:2.105531930923462
epoch21: step1000/4680
step 31500: accuracy:0.42800000309944153, confidence:0.8110555410385132, loss:2.4679524898529053
epoch21: step1500/4680
step 42000: accuracy:0.14100000262260437, confidence:0.932536244392395, loss:5.921594619750977
epoch21: step2000/4680
step 52500: accuracy:0.125, confidence:0.9606997966766357, loss:7.331700801849365
epoch21: step2500/4680
step 63000: accuracy:0.14900000393390656, confidence:0.9511662125587463, loss:7.287532806396484
epoch21: step3000/4680
step 73500: accuracy:0.08799999952316284, confidence:0.9626665115356445, loss:6.870599746704102
epoch21: step3500/4680
step 84000: accuracy:0.11599999666213989, confidence:0.8993575572967529, loss:4.52720832824707
epoch21: step4000/4680
step 94500: accuracy:0.1770000010728836, confidence:0.9396556615829468, loss:5.425055503845215
epoch21: step4500/4680
step 0: accuracy:0.14800000190734863, confidence:0.9329636693000793, loss:5.640763759613037
epoch22: step0/4680
step 11000: accuracy:0.3440000116825104, confidence:0.8243409991264343, loss:2.866821765899658
epoch22: step500/4680
step 22000: accuracy:0.46700000762939453, confidence:0.777736485004425, loss:1.8975131511688232
epoch22: step1000/4680
step 33000: accuracy:0.40299999713897705, confidence:0.7774327397346497, loss:2.4645047187805176
epoch22: step1500/4680
step 44000: accuracy:0.15399999916553497, confidence:0.9182542562484741, loss:5.620755672454834
epoch22: step2000/4680
step 55000: accuracy:0.12800000607967377, confidence:0.9585422277450562, loss:7.477156162261963
epoch22: step2500/4680
step 66000: accuracy:0.12800000607967377, confidence:0.9472644329071045, loss:7.566603660583496
epoch22: step3000/4680
step 77000: accuracy:0.11299999803304672, confidence:0.9548353552818298, loss:6.7335076332092285
epoch22: step3500/4680
step 88000: accuracy:0.13300000131130219, confidence:0.9089942574501038, loss:4.604193687438965
epoch22: step4000/4680
step 99000: accuracy:0.1509999930858612, confidence:0.9420241713523865, loss:5.83536958694458
epoch22: step4500/4680
step 0: accuracy:0.16099999845027924, confidence:0.9332099556922913, loss:5.599671840667725
epoch23: step0/4680
step 11500: accuracy:0.31700000166893005, confidence:0.8102333545684814, loss:2.8201096057891846
epoch23: step500/4680
step 23000: accuracy:0.4620000123977661, confidence:0.7649844884872437, loss:1.7811493873596191
epoch23: step1000/4680
step 34500: accuracy:0.4350000023841858, confidence:0.7854613661766052, loss:2.255080461502075
epoch23: step1500/4680
step 46000: accuracy:0.15299999713897705, confidence:0.9308305382728577, loss:5.757678985595703
epoch23: step2000/4680
step 57500: accuracy:0.13500000536441803, confidence:0.9587386250495911, loss:7.259982109069824
epoch23: step2500/4680
step 69000: accuracy:0.164000004529953, confidence:0.9511541128158569, loss:7.246130466461182
epoch23: step3000/4680
step 80500: accuracy:0.12300000339746475, confidence:0.949309766292572, loss:6.58698844909668
epoch23: step3500/4680
step 92000: accuracy:0.1720000058412552, confidence:0.8953877091407776, loss:4.423030376434326
epoch23: step4000/4680
step 103500: accuracy:0.14499999582767487, confidence:0.9480926990509033, loss:5.836696147918701
epoch23: step4500/4680
step 0: accuracy:0.1720000058412552, confidence:0.9338423013687134, loss:5.449332237243652
epoch24: step0/4680
step 12000: accuracy:0.328000009059906, confidence:0.8155437111854553, loss:2.9117953777313232
epoch24: step500/4680
step 24000: accuracy:0.4740000069141388, confidence:0.7512380480766296, loss:1.7559319734573364
epoch24: step1000/4680
step 36000: accuracy:0.4320000112056732, confidence:0.7855455279350281, loss:2.33622407913208
epoch24: step1500/4680
step 48000: accuracy:0.1589999943971634, confidence:0.9208957552909851, loss:5.134079933166504
epoch24: step2000/4680
step 60000: accuracy:0.13099999725818634, confidence:0.9548718929290771, loss:6.905394554138184
epoch24: step2500/4680
step 72000: accuracy:0.15399999916553497, confidence:0.9443687796592712, loss:7.572290420532227
epoch24: step3000/4680
step 84000: accuracy:0.1420000046491623, confidence:0.9362074136734009, loss:6.367931842803955
epoch24: step3500/4680
step 96000: accuracy:0.17000000178813934, confidence:0.8955543041229248, loss:4.438623905181885
epoch24: step4000/4680
step 108000: accuracy:0.16099999845027924, confidence:0.9416956305503845, loss:5.767543792724609
epoch24: step4500/4680
step 0: accuracy:0.164000004529953, confidence:0.927736759185791, loss:5.166945457458496
epoch25: step0/4680
step 12500: accuracy:0.33399999141693115, confidence:0.8659250736236572, loss:3.4529080390930176
epoch25: step500/4680
step 25000: accuracy:0.45899999141693115, confidence:0.7897710800170898, loss:2.0526161193847656
epoch25: step1000/4680
step 37500: accuracy:0.4339999854564667, confidence:0.7722386717796326, loss:2.2290544509887695
epoch25: step1500/4680
step 50000: accuracy:0.14000000059604645, confidence:0.9307395219802856, loss:5.87878942489624
epoch25: step2000/4680
step 62500: accuracy:0.12399999797344208, confidence:0.9531776309013367, loss:6.877739429473877
epoch25: step2500/4680
step 75000: accuracy:0.11999999731779099, confidence:0.96885085105896, loss:8.280220031738281
epoch25: step3000/4680
step 87500: accuracy:0.12099999934434891, confidence:0.9534819722175598, loss:6.704921245574951
epoch25: step3500/4680
step 100000: accuracy:0.14399999380111694, confidence:0.9141610264778137, loss:4.55460262298584
epoch25: step4000/4680
step 112500: accuracy:0.15600000321865082, confidence:0.9460198879241943, loss:5.647464752197266
epoch25: step4500/4680
step 0: accuracy:0.14100000262260437, confidence:0.9223045110702515, loss:5.399740695953369
epoch26: step0/4680
step 13000: accuracy:0.3160000145435333, confidence:0.8191559314727783, loss:2.9553613662719727
epoch26: step500/4680
step 26000: accuracy:0.4480000138282776, confidence:0.7744218111038208, loss:1.8880687952041626
epoch26: step1000/4680
step 39000: accuracy:0.42399999499320984, confidence:0.7761322855949402, loss:2.267883062362671
epoch26: step1500/4680
step 52000: accuracy:0.18700000643730164, confidence:0.9120675325393677, loss:4.862083435058594
epoch26: step2000/4680
step 65000: accuracy:0.12800000607967377, confidence:0.9547522068023682, loss:7.022622585296631
epoch26: step2500/4680
step 78000: accuracy:0.16599999368190765, confidence:0.9433047771453857, loss:7.131898880004883
epoch26: step3000/4680
step 91000: accuracy:0.12999999523162842, confidence:0.9455990791320801, loss:6.728843688964844
epoch26: step3500/4680
step 104000: accuracy:0.1459999978542328, confidence:0.9091241955757141, loss:4.54041862487793
epoch26: step4000/4680
step 117000: accuracy:0.16699999570846558, confidence:0.941900372505188, loss:5.618808746337891
epoch26: step4500/4680
step 0: accuracy:0.15800000727176666, confidence:0.9358181953430176, loss:5.458924770355225
epoch27: step0/4680
step 13500: accuracy:0.31700000166893005, confidence:0.8349407911300659, loss:3.1068153381347656
epoch27: step500/4680
step 27000: accuracy:0.46700000762939453, confidence:0.762519121170044, loss:1.7901759147644043
epoch27: step1000/4680
step 40500: accuracy:0.4359999895095825, confidence:0.7763604521751404, loss:2.060809850692749
epoch27: step1500/4680
step 54000: accuracy:0.14900000393390656, confidence:0.9189265966415405, loss:5.499156951904297
epoch27: step2000/4680
step 67500: accuracy:0.16200000047683716, confidence:0.9498607516288757, loss:6.708564758300781
epoch27: step2500/4680
step 81000: accuracy:0.13300000131130219, confidence:0.9551783204078674, loss:7.551034450531006
epoch27: step3000/4680
step 94500: accuracy:0.10400000214576721, confidence:0.9529165029525757, loss:7.0141987800598145
epoch27: step3500/4680
step 108000: accuracy:0.164000004529953, confidence:0.9113170504570007, loss:4.2266316413879395
epoch27: step4000/4680
step 121500: accuracy:0.14100000262260437, confidence:0.9381663203239441, loss:5.568259239196777
epoch27: step4500/4680
step 0: accuracy:0.164000004529953, confidence:0.9275279641151428, loss:5.284450054168701
epoch28: step0/4680
step 14000: accuracy:0.30399999022483826, confidence:0.8259410262107849, loss:2.9949147701263428
epoch28: step500/4680
step 28000: accuracy:0.4729999899864197, confidence:0.7454657554626465, loss:1.6804776191711426
epoch28: step1000/4680
step 42000: accuracy:0.460999995470047, confidence:0.7962345480918884, loss:2.1311347484588623
epoch28: step1500/4680
step 56000: accuracy:0.14100000262260437, confidence:0.9365255832672119, loss:5.751822471618652
epoch28: step2000/4680
step 70000: accuracy:0.15399999916553497, confidence:0.949661135673523, loss:6.423818588256836
epoch28: step2500/4680
step 84000: accuracy:0.1770000010728836, confidence:0.9331856369972229, loss:6.593513011932373
epoch28: step3000/4680
step 98000: accuracy:0.13099999725818634, confidence:0.9408820867538452, loss:6.944400310516357
epoch28: step3500/4680
step 112000: accuracy:0.17499999701976776, confidence:0.9025737643241882, loss:4.451316833496094
epoch28: step4000/4680
step 126000: accuracy:0.15000000596046448, confidence:0.9299008250236511, loss:5.711747646331787
epoch28: step4500/4680
step 0: accuracy:0.16500000655651093, confidence:0.913131058216095, loss:5.223554611206055
epoch29: step0/4680
step 14500: accuracy:0.34299999475479126, confidence:0.8326449990272522, loss:3.024197816848755
epoch29: step500/4680
step 29000: accuracy:0.4699999988079071, confidence:0.7624331712722778, loss:1.7784106731414795
epoch29: step1000/4680
step 43500: accuracy:0.46399998664855957, confidence:0.7836290597915649, loss:2.0713353157043457
epoch29: step1500/4680
step 58000: accuracy:0.12300000339746475, confidence:0.9450719356536865, loss:6.306939125061035
epoch29: step2000/4680
step 72500: accuracy:0.14499999582767487, confidence:0.9475648403167725, loss:6.635432720184326
epoch29: step2500/4680
step 87000: accuracy:0.15399999916553497, confidence:0.9404407739639282, loss:6.758474826812744
epoch29: step3000/4680
step 101500: accuracy:0.11699999868869781, confidence:0.9556954503059387, loss:7.155747890472412
epoch29: step3500/4680
step 116000: accuracy:0.12999999523162842, confidence:0.9091062545776367, loss:4.480940818786621
epoch29: step4000/4680
step 130500: accuracy:0.13699999451637268, confidence:0.9360042810440063, loss:5.674310684204102
epoch29: step4500/4680
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
usage: classifier.py [-h] [--dir_name DIR_NAME] [--preprocess PREPROCESS]
                     [--fname FNAME] [--original ORIGINAL]
classifier.py: error: unrecognized arguments: python3 classifier.py -- fname
