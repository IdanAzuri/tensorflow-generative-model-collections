2018-06-15 19:36:06.636323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:36:06.636545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
2018-06-15 19:36:11.082889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
CHEKPOINT DIR: multi-gaussian/mu_0.1_sigma0.3
model has been loaded from mnist_classifier.pkl
---------
Variables: name (type shape) [size]
---------
Variable:0 (float32_ref 5x5x1x32) [800, bytes: 3200]
Variable_1:0 (float32_ref 32) [32, bytes: 128]
Variable_2:0 (float32_ref 5x5x32x64) [51200, bytes: 204800]
Variable_3:0 (float32_ref 64) [64, bytes: 256]
Variable_4:0 (float32_ref 3136x1024) [3211264, bytes: 12845056]
Variable_5:0 (float32_ref 1024) [1024, bytes: 4096]
Variable_6:0 (float32_ref 1024x10) [10240, bytes: 40960]
Variable_7:0 (float32_ref 10) [10, bytes: 40]
discriminator/d_conv1/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]
discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]
discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]
discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]
generator/g_fc1/Matrix:0 (float32_ref 74x1024) [75776, bytes: 303104]
generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]
generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]
generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]
classifier/c_fc1/Matrix:0 (float32_ref 1024x64) [65536, bytes: 262144]
classifier/c_fc1/bias:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/beta:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/gamma:0 (float32_ref 64) [64, bytes: 256]
classifier/c_fc2/Matrix:0 (float32_ref 64x12) [768, bytes: 3072]
classifier/c_fc2/bias:0 (float32_ref 12) [12, bytes: 48]
Total size of variables: 16552792
Total bytes of variables: 66211168
 [*] Reading checkpoints...
 [*] Success to read MultiModalInfoGAN.model-43721
 [*] Load SUCCESS


SAMPLES SIZE=4680,LABELS=299520


SAVED TRAINING SET generated_training_set_mnist_MultivariateGaussianSampler_mu_0.1_sigma_0.3_czcc_czrc_rzcc_rzrc
 [*] Training finished!
 [*] Testing finished!
2018-06-15 19:36:59.134627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:36:59.134778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
2018-06-15 19:37:00.477308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
CHEKPOINT DIR: multi-gaussian/mu_0.1_sigma0.3
model has been loaded from mnist_classifier.pkl
---------
Variables: name (type shape) [size]
---------
Variable:0 (float32_ref 5x5x1x32) [800, bytes: 3200]
Variable_1:0 (float32_ref 32) [32, bytes: 128]
Variable_2:0 (float32_ref 5x5x32x64) [51200, bytes: 204800]
Variable_3:0 (float32_ref 64) [64, bytes: 256]
Variable_4:0 (float32_ref 3136x1024) [3211264, bytes: 12845056]
Variable_5:0 (float32_ref 1024) [1024, bytes: 4096]
Variable_6:0 (float32_ref 1024x10) [10240, bytes: 40960]
Variable_7:0 (float32_ref 10) [10, bytes: 40]
discriminator/d_conv1/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]
discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]
discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]
discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]
generator/g_fc1/Matrix:0 (float32_ref 74x1024) [75776, bytes: 303104]
generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]
generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]
generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]
classifier/c_fc1/Matrix:0 (float32_ref 1024x64) [65536, bytes: 262144]
classifier/c_fc1/bias:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/beta:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/gamma:0 (float32_ref 64) [64, bytes: 256]
classifier/c_fc2/Matrix:0 (float32_ref 64x12) [768, bytes: 3072]
classifier/c_fc2/bias:0 (float32_ref 12) [12, bytes: 48]
Total size of variables: 16552792
Total bytes of variables: 66211168
 [*] Reading checkpoints...
 [*] Success to read MultiModalInfoGAN.model-43721
 [*] Load SUCCESS


SAMPLES SIZE=4680,LABELS=299520


SAVED TRAINING SET generated_training_set_mnist_MultivariateGaussianSampler_mu_0.1_sigma_0.3_czcc_rzcc_czrc_rzrc
 [*] Training finished!
 [*] Testing finished!
2018-06-15 19:37:45.505743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:37:45.505918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
2018-06-15 19:37:46.848041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
CHEKPOINT DIR: multi-gaussian/mu_0.1_sigma0.3
model has been loaded from mnist_classifier.pkl
---------
Variables: name (type shape) [size]
---------
Variable:0 (float32_ref 5x5x1x32) [800, bytes: 3200]
Variable_1:0 (float32_ref 32) [32, bytes: 128]
Variable_2:0 (float32_ref 5x5x32x64) [51200, bytes: 204800]
Variable_3:0 (float32_ref 64) [64, bytes: 256]
Variable_4:0 (float32_ref 3136x1024) [3211264, bytes: 12845056]
Variable_5:0 (float32_ref 1024) [1024, bytes: 4096]
Variable_6:0 (float32_ref 1024x10) [10240, bytes: 40960]
Variable_7:0 (float32_ref 10) [10, bytes: 40]
discriminator/d_conv1/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]
discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]
discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]
discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]
generator/g_fc1/Matrix:0 (float32_ref 74x1024) [75776, bytes: 303104]
generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]
generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]
generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]
classifier/c_fc1/Matrix:0 (float32_ref 1024x64) [65536, bytes: 262144]
classifier/c_fc1/bias:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/beta:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/gamma:0 (float32_ref 64) [64, bytes: 256]
classifier/c_fc2/Matrix:0 (float32_ref 64x12) [768, bytes: 3072]
classifier/c_fc2/bias:0 (float32_ref 12) [12, bytes: 48]
Total size of variables: 16552792
Total bytes of variables: 66211168
 [*] Reading checkpoints...
 [*] Success to read MultiModalInfoGAN.model-43721
 [*] Load SUCCESS


SAMPLES SIZE=4680,LABELS=299520


SAVED TRAINING SET generated_training_set_mnist_MultivariateGaussianSampler_mu_0.1_sigma_0.3_rzcc_rzrc_czcc_czrc
 [*] Training finished!
 [*] Testing finished!
2018-06-15 19:38:34.489984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:38:34.490183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
2018-06-15 19:38:35.830381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
CHEKPOINT DIR: multi-gaussian/mu_0.1_sigma0.3
model has been loaded from mnist_classifier.pkl
---------
Variables: name (type shape) [size]
---------
Variable:0 (float32_ref 5x5x1x32) [800, bytes: 3200]
Variable_1:0 (float32_ref 32) [32, bytes: 128]
Variable_2:0 (float32_ref 5x5x32x64) [51200, bytes: 204800]
Variable_3:0 (float32_ref 64) [64, bytes: 256]
Variable_4:0 (float32_ref 3136x1024) [3211264, bytes: 12845056]
Variable_5:0 (float32_ref 1024) [1024, bytes: 4096]
Variable_6:0 (float32_ref 1024x10) [10240, bytes: 40960]
Variable_7:0 (float32_ref 10) [10, bytes: 40]
discriminator/d_conv1/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]
discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]
discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]
discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]
generator/g_fc1/Matrix:0 (float32_ref 74x1024) [75776, bytes: 303104]
generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]
generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]
generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]
classifier/c_fc1/Matrix:0 (float32_ref 1024x64) [65536, bytes: 262144]
classifier/c_fc1/bias:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/beta:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/gamma:0 (float32_ref 64) [64, bytes: 256]
classifier/c_fc2/Matrix:0 (float32_ref 64x12) [768, bytes: 3072]
classifier/c_fc2/bias:0 (float32_ref 12) [12, bytes: 48]
Total size of variables: 16552792
Total bytes of variables: 66211168
 [*] Reading checkpoints...
 [*] Success to read MultiModalInfoGAN.model-43721
 [*] Load SUCCESS


SAMPLES SIZE=4680,LABELS=299520


SAVED TRAINING SET generated_training_set_mnist_MultivariateGaussianSampler_mu_0.1_sigma_0.3_czrc_czcc_rzcc_rzrc
 [*] Training finished!
 [*] Testing finished!
2018-06-15 19:39:24.188059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:39:24.188257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
model has been loaded from mnist_classifier.pkl
argmax:[4 4 4 ..., 4 4 4]
step 0: accuracy:0.0, confidence:0.9945226311683655, loss:13.377690315246582
Assinging:5
[   0  314    0    0 9686]
argmax:[8 8 8 ..., 8 8 8]
step 0: accuracy:0.0, confidence:0.9971845746040344, loss:18.636470794677734
Assinging:9
[   0  195    0    0    0    0    0    0 9805]
argmax:[5 5 5 ..., 5 5 5]
step 0: accuracy:0.0, confidence:0.988930881023407, loss:11.860736846923828
Assinging:6
[   0  118    0    0    0 9648    0    0  234]
argmax:[7 7 7 ..., 7 7 7]
step 0: accuracy:0.0, confidence:0.9899298548698425, loss:14.013608932495117
Assinging:8
[   0   39    0    0    0    0    0 9921   40]
argmax:[3 3 3 ..., 3 3 3]
step 0: accuracy:0.0, confidence:0.9999291300773621, loss:14.733097076416016
Assinging:4
[    0     0     0 10000]
argmax:[6 6 6 ..., 6 6 6]
step 0: accuracy:0.0, confidence:0.9926935434341431, loss:22.920068740844727
Assinging:7
[   0   40    0    0    0    0 9960]
argmax:[0 0 0 ..., 0 0 0]
step 0: accuracy:0.0, confidence:0.9997321367263794, loss:17.358617782592773
Assinging:1
[10000]
argmax:[2 2 2 ..., 2 2 2]
step 0: accuracy:0.0, confidence:0.9950249791145325, loss:20.76451873779297
Assinging:3
[   0   78 9922]
argmax:[1 1 1 ..., 1 1 1]
step 0: accuracy:0.0, confidence:0.9991695284843445, loss:14.83737850189209
Assinging:2
[    0 10000]
argmax:[9 9 9 ..., 9 9 9]
step 0: accuracy:1.0, confidence:0.9927464723587036, loss:0.007729555480182171
Assinging:10
[    0     0     0     0     0     0     0     0     0 10000]
2018-06-15 19:39:39.506318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:39:39.506531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
model has been loaded from mnist_classifier.pkl
argmax:[3 3 3 ..., 3 3 3]
step 0: accuracy:0.0, confidence:0.9998593926429749, loss:15.565690994262695
Assinging:4
[    0     0     0 10000]
argmax:[0 0 0 ..., 0 0 0]
step 0: accuracy:0.0, confidence:0.9999388456344604, loss:16.627891540527344
Assinging:1
[10000]
argmax:[8 8 8 ..., 8 8 8]
step 0: accuracy:0.0, confidence:0.9999094009399414, loss:20.065271377563477
Assinging:9
[   0    0    0    1    0    0    0    0 9999]
argmax:[1 1 1 ..., 1 1 1]
step 0: accuracy:0.0, confidence:0.9996061325073242, loss:15.46777629852295
Assinging:2
[    0 10000]
argmax:[5 5 5 ..., 5 5 5]
step 0: accuracy:0.0, confidence:0.9972864985466003, loss:14.466840744018555
Assinging:6
[   0    0    0    3    0 9945   49    0    3]
argmax:[4 4 4 ..., 4 4 4]
step 0: accuracy:0.00019999999494757503, confidence:0.99888014793396, loss:15.013501167297363
Assinging:5
[   0    0    0    0 9998    0    0    0    0    2]
argmax:[6 6 6 ..., 6 6 6]
step 0: accuracy:0.0, confidence:0.9995203018188477, loss:22.29735565185547
Assinging:7
[   2    0    0    0    0    0 9998]
argmax:[9 9 9 ..., 9 9 9]
step 0: accuracy:0.9997000098228455, confidence:0.9986317157745361, loss:0.001924404175952077
Assinging:10
[   0    0    0    0    0    0    0    1    2 9997]
argmax:[7 7 7 ..., 7 7 7]
step 0: accuracy:0.0, confidence:0.9995036125183105, loss:10.488227844238281
Assinging:8
[    0     0     0     0     0     0     0 10000]
argmax:[2 2 2 ..., 2 2 2]
step 0: accuracy:0.0, confidence:0.9990468621253967, loss:19.2628116607666
Assinging:3
[    0     0 10000]
2018-06-15 19:39:55.029730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:39:55.029932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
model has been loaded from mnist_classifier.pkl
argmax:[6 6 6 ..., 6 6 6]
step 0: accuracy:0.0, confidence:0.9932654500007629, loss:18.201791763305664
Assinging:7
[  15   20    8    0    0    0 9955    0    2]
argmax:[1 1 1 ..., 1 1 1]
step 0: accuracy:0.0, confidence:0.9963517785072327, loss:14.343603134155273
Assinging:2
[   0 9997    0    0    0    0    1    0    2]
argmax:[7 7 7 ..., 7 7 7]
step 0: accuracy:0.00019999999494757503, confidence:0.9936644434928894, loss:11.39391040802002
Assinging:8
[   0   27    0    4    1    0    0 9965    1    2]
argmax:[2 2 2 ..., 2 2 2]
step 0: accuracy:0.0, confidence:0.995189368724823, loss:17.569538116455078
Assinging:3
[   0    6 9985    1    0    0    0    1    7]
argmax:[5 5 5 ..., 5 5 5]
step 0: accuracy:0.0003000000142492354, confidence:0.9734734296798706, loss:13.55528736114502
Assinging:6
[   4   34    0   23    0 9516  254    0  166    3]
argmax:[3 3 3 ..., 3 3 3]
step 0: accuracy:0.0, confidence:0.9836819171905518, loss:13.362443923950195
Assinging:4
[   2   41   82 9821    0    4    1    7   42]
argmax:[9 9 9 ..., 9 9 9]
step 0: accuracy:0.9957000017166138, confidence:0.9873421788215637, loss:0.020976461470127106
Assinging:10
[  15    3    1    0    1    0    0   12   11 9957]
argmax:[0 0 0 ..., 0 0 0]
step 0: accuracy:0.0, confidence:0.9970636963844299, loss:15.341217041015625
Assinging:1
[9976    0   10    0    0    0    0    0   14]
argmax:[8 8 8 ..., 8 8 8]
step 0: accuracy:0.0, confidence:0.9972327947616577, loss:16.297216415405273
Assinging:9
[   0   10    7    0    0    2    1    0 9980]
argmax:[4 4 4 ..., 4 4 4]
step 0: accuracy:0.007799999788403511, confidence:0.9824578166007996, loss:10.108667373657227
Assinging:5
[   6   12    0    1 9888    0    1    8    6   78]
2018-06-15 19:40:11.100851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:40:11.101046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
model has been loaded from mnist_classifier.pkl
argmax:[8 8 8 ..., 8 8 8]
step 0: accuracy:0.0, confidence:0.9999676942825317, loss:16.267332077026367
Assinging:9
[    0     0     0     0     0     0     0     0 10000]
argmax:[9 9 9 ..., 9 9 9]
step 0: accuracy:1.0, confidence:0.9930931925773621, loss:0.007551013026386499
Assinging:10
[    0     0     0     0     0     0     0     0     0 10000]
argmax:[4 4 4 ..., 4 4 4]
step 0: accuracy:0.0, confidence:0.9975805878639221, loss:9.261672973632812
Assinging:5
[    0     0     0     0 10000]
argmax:[2 2 2 ..., 2 2 2]
step 0: accuracy:0.0, confidence:0.9896156191825867, loss:18.457008361816406
Assinging:3
[   0    0 9883    0    0    0    0    0  117]
argmax:[7 7 7 ..., 7 7 7]
step 0: accuracy:0.0, confidence:0.9996220469474792, loss:11.283448219299316
Assinging:8
[    0     0     0     0     0     0     0 10000]
argmax:[5 5 3 ..., 3 3 3]
step 0: accuracy:0.0, confidence:0.9984591007232666, loss:13.904959678649902
Assinging:4
[   0    0    0 8128    0 1872]
argmax:[6 6 6 ..., 6 6 6]
step 0: accuracy:0.0, confidence:0.9986042976379395, loss:22.514951705932617
Assinging:7
[    0     0     0     0     0     0 10000]
argmax:[5 5 5 ..., 5 5 5]
step 0: accuracy:0.0, confidence:0.9932737350463867, loss:11.13691520690918
Assinging:6
[   0    0    0 1872    0 8128]
argmax:[1 1 1 ..., 1 1 1]
step 0: accuracy:0.0, confidence:0.9974302649497986, loss:13.898530960083008
Assinging:2
[    0 10000]
argmax:[0 0 0 ..., 0 0 0]
step 0: accuracy:0.0, confidence:0.924338161945343, loss:8.771900177001953
Assinging:1
[8947    0    0    0    0    0    0    0 1053]
2018-06-15 19:40:36.279627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:40:36.279775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
step 0: accuracy:0.11500000208616257, confidence:0.8821412920951843, loss:9.400466918945312
epoch0: step0/4680
step 0: accuracy:0.08799999952316284, confidence:1.0, loss:55.50314712524414
epoch0: step500/4680
step 0: accuracy:0.08900000154972076, confidence:1.0, loss:44.55191421508789
epoch0: step1000/4680
step 0: accuracy:0.10300000011920929, confidence:0.9998148083686829, loss:11.135207176208496
epoch0: step1500/4680
step 0: accuracy:0.0860000029206276, confidence:0.9954665303230286, loss:7.337130069732666
epoch0: step2000/4680
step 0: accuracy:0.07999999821186066, confidence:0.9963558316230774, loss:7.767616271972656
epoch0: step2500/4680
step 0: accuracy:0.11299999803304672, confidence:0.9984761476516724, loss:9.098509788513184
epoch0: step3000/4680
step 0: accuracy:0.0949999988079071, confidence:0.9965543746948242, loss:7.4701924324035645
epoch0: step3500/4680
step 0: accuracy:0.11900000274181366, confidence:0.9982547163963318, loss:8.18465518951416
epoch0: step4000/4680
step 0: accuracy:0.11999999731779099, confidence:0.9998731017112732, loss:11.237539291381836
epoch0: step4500/4680
step 0: accuracy:0.11500000208616257, confidence:0.9998490214347839, loss:10.76870346069336
epoch1: step0/4680
step 500: accuracy:0.10400000214576721, confidence:0.9843029379844666, loss:6.686062335968018
epoch1: step500/4680
step 1000: accuracy:0.09600000083446503, confidence:0.9920380711555481, loss:7.65012788772583
epoch1: step1000/4680
step 1500: accuracy:0.08299999684095383, confidence:0.9982342720031738, loss:10.24587631225586
epoch1: step1500/4680
step 2000: accuracy:0.11500000208616257, confidence:0.997909426689148, loss:7.8769755363464355
epoch1: step2000/4680
step 2500: accuracy:0.09799999743700027, confidence:0.9971325397491455, loss:7.3187479972839355
epoch1: step2500/4680
step 3000: accuracy:0.10300000011920929, confidence:0.9990794658660889, loss:8.349020004272461
epoch1: step3000/4680
step 3500: accuracy:0.07000000029802322, confidence:0.9987342357635498, loss:8.367768287658691
epoch1: step3500/4680
step 4000: accuracy:0.10100000351667404, confidence:0.9992902874946594, loss:8.819766998291016
epoch1: step4000/4680
step 4500: accuracy:0.1289999932050705, confidence:0.9994192123413086, loss:8.71294116973877
epoch1: step4500/4680
step 0: accuracy:0.11500000208616257, confidence:0.999455451965332, loss:8.947153091430664
epoch2: step0/4680
step 1000: accuracy:0.10499999672174454, confidence:0.9917638897895813, loss:7.127110958099365
epoch2: step500/4680
step 2000: accuracy:0.09799999743700027, confidence:0.9986514449119568, loss:9.093426704406738
epoch2: step1000/4680
step 3000: accuracy:0.09099999815225601, confidence:0.9939442276954651, loss:7.919090270996094
epoch2: step1500/4680
step 4000: accuracy:0.0989999994635582, confidence:0.9945409297943115, loss:7.221687316894531
epoch2: step2000/4680
step 5000: accuracy:0.09799999743700027, confidence:0.9896665811538696, loss:6.121707439422607
epoch2: step2500/4680
step 6000: accuracy:0.08500000089406967, confidence:0.9971020221710205, loss:7.381009578704834
epoch2: step3000/4680
step 7000: accuracy:0.0949999988079071, confidence:0.9990923404693604, loss:8.436136245727539
epoch2: step3500/4680
step 8000: accuracy:0.10199999809265137, confidence:0.999122142791748, loss:8.845641136169434
epoch2: step4000/4680
step 9000: accuracy:0.11400000005960464, confidence:0.9990800023078918, loss:8.40263843536377
epoch2: step4500/4680
step 0: accuracy:0.10599999874830246, confidence:0.9993618130683899, loss:8.89282512664795
epoch3: step0/4680
step 1500: accuracy:0.10000000149011612, confidence:0.9328490495681763, loss:4.421414375305176
epoch3: step500/4680
step 3000: accuracy:0.10000000149011612, confidence:0.9997243881225586, loss:11.212591171264648
epoch3: step1000/4680
step 4500: accuracy:0.08900000154972076, confidence:0.9942488670349121, loss:8.128416061401367
epoch3: step1500/4680
step 6000: accuracy:0.10199999809265137, confidence:0.9946614503860474, loss:7.262032508850098
epoch3: step2000/4680
step 7500: accuracy:0.09700000286102295, confidence:0.9865913987159729, loss:6.00200891494751
epoch3: step2500/4680
step 9000: accuracy:0.10499999672174454, confidence:0.99718177318573, loss:7.137537956237793
epoch3: step3000/4680
step 10500: accuracy:0.09399999678134918, confidence:0.9938289523124695, loss:6.777640342712402
epoch3: step3500/4680
step 12000: accuracy:0.11299999803304672, confidence:0.997871994972229, loss:7.830864906311035
epoch3: step4000/4680
step 13500: accuracy:0.10300000011920929, confidence:0.9975895881652832, loss:7.423901081085205
epoch3: step4500/4680
step 0: accuracy:0.11400000005960464, confidence:0.9987431764602661, loss:7.8686041831970215
epoch4: step0/4680
step 2000: accuracy:0.10999999940395355, confidence:0.9430537223815918, loss:4.386929512023926
epoch4: step500/4680
step 4000: accuracy:0.10300000011920929, confidence:0.995535135269165, loss:7.37066650390625
epoch4: step1000/4680
step 6000: accuracy:0.10599999874830246, confidence:0.9592515826225281, loss:5.488667964935303
epoch4: step1500/4680
step 8000: accuracy:0.10300000011920929, confidence:0.9447968006134033, loss:5.209578990936279
epoch4: step2000/4680
step 10000: accuracy:0.11100000143051147, confidence:0.9253300428390503, loss:4.86375617980957
epoch4: step2500/4680
step 12000: accuracy:0.0860000029206276, confidence:0.9935855865478516, loss:6.451043605804443
epoch4: step3000/4680
step 14000: accuracy:0.10999999940395355, confidence:0.9880768060684204, loss:6.075948715209961
epoch4: step3500/4680
step 16000: accuracy:0.0989999994635582, confidence:0.993951678276062, loss:7.21402645111084
epoch4: step4000/4680
step 18000: accuracy:0.12399999797344208, confidence:0.9563540816307068, loss:5.148380279541016
epoch4: step4500/4680
step 0: accuracy:0.1289999932050705, confidence:0.9948698878288269, loss:7.402805328369141
epoch5: step0/4680
step 2500: accuracy:0.0989999994635582, confidence:0.9512740969657898, loss:4.978784084320068
epoch5: step500/4680
step 5000: accuracy:0.10899999737739563, confidence:0.9920360445976257, loss:6.432648658752441
epoch5: step1000/4680
step 7500: accuracy:0.09200000017881393, confidence:0.9556922316551208, loss:5.379089832305908
epoch5: step1500/4680
step 10000: accuracy:0.1420000046491623, confidence:0.8270562291145325, loss:3.724949836730957
epoch5: step2000/4680
step 12500: accuracy:0.1340000033378601, confidence:0.9084246158599854, loss:5.055634021759033
epoch5: step2500/4680
step 15000: accuracy:0.10100000351667404, confidence:0.9921660423278809, loss:6.141612529754639
epoch5: step3000/4680
step 17500: accuracy:0.09600000083446503, confidence:0.9768957495689392, loss:5.89840030670166
epoch5: step3500/4680
step 20000: accuracy:0.10599999874830246, confidence:0.9784196615219116, loss:6.299818515777588
epoch5: step4000/4680
step 22500: accuracy:0.16500000655651093, confidence:0.8911994695663452, loss:4.444939613342285
epoch5: step4500/4680
step 0: accuracy:0.1289999932050705, confidence:0.9314159154891968, loss:5.4142255783081055
epoch6: step0/4680
step 3000: accuracy:0.24300000071525574, confidence:0.5956901907920837, loss:2.0671420097351074
epoch6: step500/4680
step 6000: accuracy:0.0860000029206276, confidence:0.9366171956062317, loss:4.442715167999268
epoch6: step1000/4680
step 9000: accuracy:0.1599999964237213, confidence:0.7401496171951294, loss:2.881798267364502
epoch6: step1500/4680
step 12000: accuracy:0.18700000643730164, confidence:0.847158670425415, loss:3.7765157222747803
epoch6: step2000/4680
step 15000: accuracy:0.20600000023841858, confidence:0.8563265800476074, loss:3.990135669708252
epoch6: step2500/4680
step 18000: accuracy:0.13199999928474426, confidence:0.9349284768104553, loss:5.079126358032227
epoch6: step3000/4680
step 21000: accuracy:0.1080000028014183, confidence:0.9359444379806519, loss:5.011984348297119
epoch6: step3500/4680
step 24000: accuracy:0.11299999803304672, confidence:0.9290897250175476, loss:5.970722675323486
epoch6: step4000/4680
step 27000: accuracy:0.24300000071525574, confidence:0.8543347120285034, loss:3.7383205890655518
epoch6: step4500/4680
step 0: accuracy:0.21799999475479126, confidence:0.8880182504653931, loss:4.45759916305542
epoch7: step0/4680
step 3500: accuracy:0.3659999966621399, confidence:0.5861544609069824, loss:1.7914763689041138
epoch7: step500/4680
step 7000: accuracy:0.1889999955892563, confidence:0.8016141653060913, loss:3.1716196537017822
epoch7: step1000/4680
step 10500: accuracy:0.1770000010728836, confidence:0.7471814751625061, loss:2.8135108947753906
epoch7: step1500/4680
step 14000: accuracy:0.20100000500679016, confidence:0.8213165402412415, loss:3.405386209487915
epoch7: step2000/4680
step 17500: accuracy:0.22699999809265137, confidence:0.7969096899032593, loss:3.2457640171051025
epoch7: step2500/4680
step 21000: accuracy:0.1979999989271164, confidence:0.9079123735427856, loss:4.976925849914551
epoch7: step3000/4680
step 24500: accuracy:0.13199999928474426, confidence:0.9177972674369812, loss:4.4932756423950195
epoch7: step3500/4680
step 28000: accuracy:0.1420000046491623, confidence:0.9116962552070618, loss:5.050081729888916
epoch7: step4000/4680
step 31500: accuracy:0.2549999952316284, confidence:0.8507581949234009, loss:3.8702542781829834
epoch7: step4500/4680
step 0: accuracy:0.23399999737739563, confidence:0.8982385396957397, loss:4.534754753112793
epoch8: step0/4680
step 4000: accuracy:0.3540000021457672, confidence:0.602590799331665, loss:1.8079522848129272
epoch8: step500/4680
step 8000: accuracy:0.1899999976158142, confidence:0.8051341772079468, loss:2.896437406539917
epoch8: step1000/4680
step 12000: accuracy:0.20999999344348907, confidence:0.7249216437339783, loss:2.47659969329834
epoch8: step1500/4680
step 16000: accuracy:0.20000000298023224, confidence:0.816103994846344, loss:3.378197193145752
epoch8: step2000/4680
step 20000: accuracy:0.23000000417232513, confidence:0.8077733516693115, loss:3.302335262298584
epoch8: step2500/4680
step 24000: accuracy:0.20399999618530273, confidence:0.8879241347312927, loss:3.908466339111328
epoch8: step3000/4680
step 28000: accuracy:0.2150000035762787, confidence:0.8351426720619202, loss:3.3152499198913574
epoch8: step3500/4680
step 32000: accuracy:0.14000000059604645, confidence:0.9428768157958984, loss:6.123366355895996
epoch8: step4000/4680
step 36000: accuracy:0.22599999606609344, confidence:0.8651509284973145, loss:3.879119634628296
epoch8: step4500/4680
step 0: accuracy:0.2070000022649765, confidence:0.9203256964683533, loss:4.810449600219727
epoch9: step0/4680
step 4500: accuracy:0.328000009059906, confidence:0.6270696520805359, loss:1.9430056810379028
epoch9: step500/4680
step 9000: accuracy:0.23800000548362732, confidence:0.7929859161376953, loss:2.7281370162963867
epoch9: step1000/4680
step 13500: accuracy:0.2370000034570694, confidence:0.7627585530281067, loss:2.4182040691375732
epoch9: step1500/4680
step 18000: accuracy:0.2370000034570694, confidence:0.8441283702850342, loss:3.554757595062256
epoch9: step2000/4680
step 22500: accuracy:0.2529999911785126, confidence:0.7976710796356201, loss:3.2146170139312744
epoch9: step2500/4680
step 27000: accuracy:0.21299999952316284, confidence:0.9072707891464233, loss:5.369478702545166
epoch9: step3000/4680
step 31500: accuracy:0.21400000154972076, confidence:0.8337835669517517, loss:3.582037925720215
epoch9: step3500/4680
step 36000: accuracy:0.14399999380111694, confidence:0.8825554251670837, loss:4.442044258117676
epoch9: step4000/4680
step 40500: accuracy:0.23999999463558197, confidence:0.8441245555877686, loss:3.6992428302764893
epoch9: step4500/4680
step 0: accuracy:0.20000000298023224, confidence:0.8838716149330139, loss:4.5705037117004395
epoch10: step0/4680
step 5000: accuracy:0.3700000047683716, confidence:0.6788058280944824, loss:2.005568027496338
epoch10: step500/4680
step 10000: accuracy:0.10100000351667404, confidence:0.9907540082931519, loss:6.398403167724609
epoch10: step1000/4680
step 15000: accuracy:0.14499999582767487, confidence:0.8595866560935974, loss:3.873145580291748
epoch10: step1500/4680
step 20000: accuracy:0.2639999985694885, confidence:0.807459831237793, loss:2.9187734127044678
epoch10: step2000/4680
step 25000: accuracy:0.20600000023841858, confidence:0.8246831297874451, loss:3.428659439086914
epoch10: step2500/4680
step 30000: accuracy:0.15000000596046448, confidence:0.936775267124176, loss:4.662344932556152
epoch10: step3000/4680
step 35000: accuracy:0.20800000429153442, confidence:0.8487027883529663, loss:3.4957573413848877
epoch10: step3500/4680
step 40000: accuracy:0.09799999743700027, confidence:0.9762856364250183, loss:7.213826656341553
epoch10: step4000/4680
step 45000: accuracy:0.2669999897480011, confidence:0.8557994365692139, loss:3.544053077697754
epoch10: step4500/4680
step 0: accuracy:0.22200000286102295, confidence:0.9023446440696716, loss:4.352470397949219
epoch11: step0/4680
step 5500: accuracy:0.3310000002384186, confidence:0.691990315914154, loss:2.225004196166992
epoch11: step500/4680
step 11000: accuracy:0.3019999861717224, confidence:0.7821115255355835, loss:2.6050314903259277
epoch11: step1000/4680
step 16500: accuracy:0.24300000071525574, confidence:0.8246642351150513, loss:2.912172794342041
epoch11: step1500/4680
step 22000: accuracy:0.22699999809265137, confidence:0.8698298931121826, loss:4.052217483520508
epoch11: step2000/4680
step 27500: accuracy:0.22100000083446503, confidence:0.8544899821281433, loss:4.891924858093262
epoch11: step2500/4680
step 33000: accuracy:0.19300000369548798, confidence:0.9220886826515198, loss:4.421948432922363
epoch11: step3000/4680
step 38500: accuracy:0.2669999897480011, confidence:0.8437873125076294, loss:3.5405592918395996
epoch11: step3500/4680
step 44000: accuracy:0.15800000727176666, confidence:0.8512629270553589, loss:4.243177890777588
epoch11: step4000/4680
step 49500: accuracy:0.27799999713897705, confidence:0.8461481928825378, loss:3.4084689617156982
epoch11: step4500/4680
step 0: accuracy:0.23000000417232513, confidence:0.899804949760437, loss:4.405432224273682
epoch12: step0/4680
step 6000: accuracy:0.4350000023841858, confidence:0.6818335056304932, loss:1.7582629919052124
epoch12: step500/4680
step 12000: accuracy:0.20600000023841858, confidence:0.8336941003799438, loss:3.718076705932617
epoch12: step1000/4680
step 18000: accuracy:0.24199999868869781, confidence:0.7603257894515991, loss:2.3343989849090576
epoch12: step1500/4680
step 24000: accuracy:0.2540000081062317, confidence:0.8178117871284485, loss:2.8830671310424805
epoch12: step2000/4680
step 30000: accuracy:0.30000001192092896, confidence:0.7768241167068481, loss:2.7141799926757812
epoch12: step2500/4680
step 36000: accuracy:0.24300000071525574, confidence:0.8630318641662598, loss:5.061159133911133
epoch12: step3000/4680
step 42000: accuracy:0.296999990940094, confidence:0.8129047751426697, loss:3.026015281677246
epoch12: step3500/4680
step 48000: accuracy:0.1770000010728836, confidence:0.8839464783668518, loss:4.761302471160889
epoch12: step4000/4680
step 54000: accuracy:0.3199999928474426, confidence:0.8063175082206726, loss:2.747929334640503
epoch12: step4500/4680
step 0: accuracy:0.2680000066757202, confidence:0.8579946160316467, loss:3.7783589363098145
epoch13: step0/4680
step 6500: accuracy:0.4090000092983246, confidence:0.6993297934532166, loss:1.8715331554412842
epoch13: step500/4680
step 13000: accuracy:0.39100000262260437, confidence:0.7532919049263, loss:1.8927117586135864
epoch13: step1000/4680
step 19500: accuracy:0.26600000262260437, confidence:0.7976541519165039, loss:2.512932538986206
epoch13: step1500/4680
step 26000: accuracy:0.22499999403953552, confidence:0.8617542386054993, loss:3.724569082260132
epoch13: step2000/4680
step 32500: accuracy:0.2709999978542328, confidence:0.8477466106414795, loss:3.869014263153076
epoch13: step2500/4680
step 39000: accuracy:0.24899999797344208, confidence:0.9089457392692566, loss:3.8333046436309814
epoch13: step3000/4680
step 45500: accuracy:0.3140000104904175, confidence:0.8117737770080566, loss:3.0352978706359863
epoch13: step3500/4680
step 52000: accuracy:0.2070000022649765, confidence:0.8739402890205383, loss:4.078885078430176
epoch13: step4000/4680
step 58500: accuracy:0.3019999861717224, confidence:0.8488212823867798, loss:3.1833994388580322
epoch13: step4500/4680
step 0: accuracy:0.26100000739097595, confidence:0.8797580599784851, loss:3.830017566680908
epoch14: step0/4680
step 7000: accuracy:0.3799999952316284, confidence:0.692222535610199, loss:2.0586745738983154
epoch14: step500/4680
step 14000: accuracy:0.24899999797344208, confidence:0.7994821667671204, loss:2.8789312839508057
epoch14: step1000/4680
step 21000: accuracy:0.23600000143051147, confidence:0.7729594707489014, loss:2.3987560272216797
epoch14: step1500/4680
step 28000: accuracy:0.29499998688697815, confidence:0.815405011177063, loss:2.7264039516448975
epoch14: step2000/4680
step 35000: accuracy:0.2809999883174896, confidence:0.7707167267799377, loss:2.57747745513916
epoch14: step2500/4680
step 42000: accuracy:0.25699999928474426, confidence:0.8566554188728333, loss:4.4837646484375
epoch14: step3000/4680
step 49000: accuracy:0.3240000009536743, confidence:0.7914144396781921, loss:2.5558688640594482
epoch14: step3500/4680
step 56000: accuracy:0.21400000154972076, confidence:0.8314687013626099, loss:3.67172908782959
epoch14: step4000/4680
step 63000: accuracy:0.33899998664855957, confidence:0.7987855672836304, loss:2.7197158336639404
epoch14: step4500/4680
step 0: accuracy:0.2879999876022339, confidence:0.8379018306732178, loss:3.3604698181152344
epoch15: step0/4680
step 7500: accuracy:0.3959999978542328, confidence:0.7493409514427185, loss:2.074420213699341
epoch15: step500/4680
step 15000: accuracy:0.2919999957084656, confidence:0.8169907331466675, loss:3.02274751663208
epoch15: step1000/4680
step 22500: accuracy:0.3449999988079071, confidence:0.8033287525177002, loss:2.3545901775360107
epoch15: step1500/4680
step 30000: accuracy:0.28600001335144043, confidence:0.8498774170875549, loss:3.1930909156799316
epoch15: step2000/4680
step 37500: accuracy:0.30000001192092896, confidence:0.8574637770652771, loss:3.94751238822937
epoch15: step2500/4680
step 45000: accuracy:0.22300000488758087, confidence:0.9347695112228394, loss:4.183923244476318
epoch15: step3000/4680
step 52500: accuracy:0.18700000643730164, confidence:0.8968914151191711, loss:4.379148960113525
epoch15: step3500/4680
step 60000: accuracy:0.18199999630451202, confidence:0.8660938739776611, loss:4.148763179779053
epoch15: step4000/4680
step 67500: accuracy:0.25999999046325684, confidence:0.8551948070526123, loss:3.4472312927246094
epoch15: step4500/4680
step 0: accuracy:0.23899999260902405, confidence:0.8872583508491516, loss:4.009583950042725
epoch16: step0/4680
step 8000: accuracy:0.45899999141693115, confidence:0.651087760925293, loss:1.585098147392273
epoch16: step500/4680
step 16000: accuracy:0.24199999868869781, confidence:0.797595739364624, loss:2.539625644683838
epoch16: step1000/4680
step 24000: accuracy:0.24400000274181366, confidence:0.7487673163414001, loss:2.341043710708618
epoch16: step1500/4680
step 32000: accuracy:0.25600001215934753, confidence:0.8055887222290039, loss:2.6693925857543945
epoch16: step2000/4680
step 40000: accuracy:0.31200000643730164, confidence:0.7640281915664673, loss:2.4223251342773438
epoch16: step2500/4680
step 48000: accuracy:0.26600000262260437, confidence:0.8654168248176575, loss:4.2035040855407715
epoch16: step3000/4680
step 56000: accuracy:0.33899998664855957, confidence:0.7685623168945312, loss:2.364149808883667
epoch16: step3500/4680
step 64000: accuracy:0.2199999988079071, confidence:0.8232183456420898, loss:3.495307683944702
epoch16: step4000/4680
step 72000: accuracy:0.3409999907016754, confidence:0.7917507886886597, loss:2.616137742996216
epoch16: step4500/4680
step 0: accuracy:0.3580000102519989, confidence:0.8310397267341614, loss:2.732611894607544
epoch17: step0/4680
step 8500: accuracy:0.5049999952316284, confidence:0.7247467041015625, loss:1.5115641355514526
epoch17: step500/4680
step 17000: accuracy:0.3790000081062317, confidence:0.7889577150344849, loss:2.282457113265991
epoch17: step1000/4680
step 25500: accuracy:0.2919999957084656, confidence:0.8673562407493591, loss:3.2162702083587646
epoch17: step1500/4680
step 34000: accuracy:0.27900001406669617, confidence:0.856469988822937, loss:3.6168015003204346
epoch17: step2000/4680
step 42500: accuracy:0.27399998903274536, confidence:0.8831995129585266, loss:4.067322254180908
epoch17: step2500/4680
step 51000: accuracy:0.24699999392032623, confidence:0.9173637628555298, loss:3.8440425395965576
epoch17: step3000/4680
step 59500: accuracy:0.22100000083446503, confidence:0.8837104439735413, loss:3.970961570739746
epoch17: step3500/4680
step 68000: accuracy:0.1770000010728836, confidence:0.8867334127426147, loss:4.3037638664245605
epoch17: step4000/4680
step 76500: accuracy:0.2800000011920929, confidence:0.8576301336288452, loss:3.2704787254333496
epoch17: step4500/4680
step 0: accuracy:0.2370000034570694, confidence:0.8695405125617981, loss:3.8876755237579346
epoch18: step0/4680
step 9000: accuracy:0.414000004529953, confidence:0.6830204129219055, loss:1.8907455205917358
epoch18: step500/4680
step 18000: accuracy:0.23600000143051147, confidence:0.8180138468742371, loss:2.7034404277801514
epoch18: step1000/4680
step 27000: accuracy:0.24199999868869781, confidence:0.7750194072723389, loss:2.3776721954345703
epoch18: step1500/4680
step 36000: accuracy:0.28200000524520874, confidence:0.8505372405052185, loss:3.2095110416412354
epoch18: step2000/4680
step 45000: accuracy:0.31200000643730164, confidence:0.7617579102516174, loss:2.448060989379883
epoch18: step2500/4680
step 54000: accuracy:0.2680000066757202, confidence:0.8563194870948792, loss:4.300783157348633
epoch18: step3000/4680
step 63000: accuracy:0.34200000762939453, confidence:0.7744187116622925, loss:2.3838725090026855
epoch18: step3500/4680
step 72000: accuracy:0.22599999606609344, confidence:0.8497806191444397, loss:3.6906962394714355
epoch18: step4000/4680
step 81000: accuracy:0.3959999978542328, confidence:0.7893279194831848, loss:2.2895264625549316
epoch18: step4500/4680
step 0: accuracy:0.3630000054836273, confidence:0.8295983076095581, loss:2.728943347930908
epoch19: step0/4680
step 9500: accuracy:0.45500001311302185, confidence:0.7395479083061218, loss:1.857567310333252
epoch19: step500/4680
step 19000: accuracy:0.4300000071525574, confidence:0.7994394898414612, loss:2.0874507427215576
epoch19: step1000/4680
step 28500: accuracy:0.32600000500679016, confidence:0.866429328918457, loss:3.1824326515197754
epoch19: step1500/4680
step 38000: accuracy:0.30000001192092896, confidence:0.8569122552871704, loss:3.4771695137023926
epoch19: step2000/4680
step 47500: accuracy:0.3370000123977661, confidence:0.8349695205688477, loss:3.3327574729919434
epoch19: step2500/4680
step 57000: accuracy:0.2619999945163727, confidence:0.9233337640762329, loss:3.904857873916626
epoch19: step3000/4680
step 66500: accuracy:0.12399999797344208, confidence:0.9291980266571045, loss:5.164114475250244
epoch19: step3500/4680
step 76000: accuracy:0.15800000727176666, confidence:0.8720500469207764, loss:4.338769912719727
epoch19: step4000/4680
step 85500: accuracy:0.24199999868869781, confidence:0.8580617308616638, loss:3.442690849304199
epoch19: step4500/4680
step 0: accuracy:0.2280000001192093, confidence:0.8882846236228943, loss:4.117093086242676
epoch20: step0/4680
step 10000: accuracy:0.4429999887943268, confidence:0.6708795428276062, loss:1.6559360027313232
epoch20: step500/4680
step 20000: accuracy:0.18199999630451202, confidence:0.8136625289916992, loss:3.134800434112549
epoch20: step1000/4680
step 30000: accuracy:0.28200000524520874, confidence:0.7688085436820984, loss:2.2262556552886963
epoch20: step1500/4680
step 40000: accuracy:0.2630000114440918, confidence:0.8013489842414856, loss:2.856712579727173
epoch20: step2000/4680
step 50000: accuracy:0.3370000123977661, confidence:0.7724267840385437, loss:2.6187617778778076
epoch20: step2500/4680
step 60000: accuracy:0.29899999499320984, confidence:0.8496361374855042, loss:3.682213306427002
epoch20: step3000/4680
step 70000: accuracy:0.3720000088214874, confidence:0.7636234164237976, loss:2.0858521461486816
epoch20: step3500/4680
step 80000: accuracy:0.1599999964237213, confidence:0.8881551027297974, loss:4.515956878662109
epoch20: step4000/4680
step 90000: accuracy:0.43299999833106995, confidence:0.7882636785507202, loss:2.0490055084228516
epoch20: step4500/4680
step 0: accuracy:0.3449999988079071, confidence:0.8313180208206177, loss:2.8791422843933105
epoch21: step0/4680
step 10500: accuracy:0.49399998784065247, confidence:0.7625012397766113, loss:1.6407784223556519
epoch21: step500/4680
step 21000: accuracy:0.5180000066757202, confidence:0.7776890397071838, loss:1.4958295822143555
epoch21: step1000/4680
step 31500: accuracy:0.36000001430511475, confidence:0.8289440870285034, loss:2.3793017864227295
epoch21: step1500/4680
step 42000: accuracy:0.29499998688697815, confidence:0.8823139071464539, loss:3.896648645401001
epoch21: step2000/4680
step 52500: accuracy:0.3109999895095825, confidence:0.8355071544647217, loss:3.629821538925171
epoch21: step2500/4680
step 63000: accuracy:0.30799999833106995, confidence:0.9125859141349792, loss:3.5077924728393555
epoch21: step3000/4680
step 73500: accuracy:0.2809999883174896, confidence:0.8540305495262146, loss:3.435668468475342
epoch21: step3500/4680
step 84000: accuracy:0.23199999332427979, confidence:0.8274659514427185, loss:3.532700538635254
epoch21: step4000/4680
step 94500: accuracy:0.32899999618530273, confidence:0.8255016803741455, loss:2.8881125450134277
epoch21: step4500/4680
step 0: accuracy:0.27799999713897705, confidence:0.8629357814788818, loss:3.519371747970581
epoch22: step0/4680
step 11000: accuracy:0.460999995470047, confidence:0.7109763622283936, loss:1.6984208822250366
epoch22: step500/4680
step 22000: accuracy:0.20900000631809235, confidence:0.8372761011123657, loss:3.7750139236450195
epoch22: step1000/4680
step 33000: accuracy:0.2980000078678131, confidence:0.7687909603118896, loss:2.0837442874908447
epoch22: step1500/4680
step 44000: accuracy:0.3499999940395355, confidence:0.8132596611976624, loss:2.567206382751465
epoch22: step2000/4680
step 55000: accuracy:0.3440000116825104, confidence:0.76927649974823, loss:2.5168864727020264
epoch22: step2500/4680
step 66000: accuracy:0.23499999940395355, confidence:0.8698903918266296, loss:5.137915134429932
epoch22: step3000/4680
step 77000: accuracy:0.3319999873638153, confidence:0.7970073223114014, loss:2.4563848972320557
epoch22: step3500/4680
step 88000: accuracy:0.3019999861717224, confidence:0.8148487210273743, loss:3.059633255004883
epoch22: step4000/4680
step 99000: accuracy:0.4320000112056732, confidence:0.7797117233276367, loss:2.0760316848754883
epoch22: step4500/4680
step 0: accuracy:0.36800000071525574, confidence:0.8201911449432373, loss:2.609459638595581
epoch23: step0/4680
step 11500: accuracy:0.49900001287460327, confidence:0.7493810057640076, loss:1.5334111452102661
epoch23: step500/4680
step 23000: accuracy:0.3700000047683716, confidence:0.8016054034233093, loss:2.517167091369629
epoch23: step1000/4680
step 34500: accuracy:0.3440000116825104, confidence:0.8644841909408569, loss:2.9191501140594482
epoch23: step1500/4680
step 46000: accuracy:0.28600001335144043, confidence:0.8594704866409302, loss:3.6156625747680664
epoch23: step2000/4680
step 57500: accuracy:0.27300000190734863, confidence:0.8649662733078003, loss:4.383392810821533
epoch23: step2500/4680
step 69000: accuracy:0.3269999921321869, confidence:0.9145727753639221, loss:3.487004280090332
epoch23: step3000/4680
step 80500: accuracy:0.22300000488758087, confidence:0.8929060101509094, loss:4.311712741851807
epoch23: step3500/4680
step 92000: accuracy:0.11299999803304672, confidence:0.9558759331703186, loss:6.898504734039307
epoch23: step4000/4680
step 103500: accuracy:0.2770000100135803, confidence:0.8550039529800415, loss:3.1698598861694336
epoch23: step4500/4680
step 0: accuracy:0.2409999966621399, confidence:0.8962609767913818, loss:4.003745079040527
epoch24: step0/4680
step 12000: accuracy:0.5130000114440918, confidence:0.6867501735687256, loss:1.4360085725784302
epoch24: step500/4680
step 24000: accuracy:0.2280000001192093, confidence:0.7919145226478577, loss:2.666435956954956
epoch24: step1000/4680
step 36000: accuracy:0.257999986410141, confidence:0.7682434320449829, loss:2.3204333782196045
epoch24: step1500/4680
step 48000: accuracy:0.2590000033378601, confidence:0.8308021426200867, loss:2.9937193393707275
epoch24: step2000/4680
step 60000: accuracy:0.3499999940395355, confidence:0.7733201384544373, loss:2.35164737701416
epoch24: step2500/4680
step 72000: accuracy:0.25200000405311584, confidence:0.8650391697883606, loss:4.604062557220459
epoch24: step3000/4680
step 84000: accuracy:0.35199999809265137, confidence:0.7583939433097839, loss:2.289799213409424
epoch24: step3500/4680
step 96000: accuracy:0.34299999475479126, confidence:0.7661827206611633, loss:2.660022735595703
epoch24: step4000/4680
step 108000: accuracy:0.39399999380111694, confidence:0.7970092296600342, loss:2.3591508865356445
epoch24: step4500/4680
step 0: accuracy:0.4090000092983246, confidence:0.7988357543945312, loss:2.4055402278900146
epoch25: step0/4680
step 12500: accuracy:0.5740000009536743, confidence:0.7530132532119751, loss:1.2684037685394287
epoch25: step500/4680
step 25000: accuracy:0.27799999713897705, confidence:0.8414896726608276, loss:3.6093857288360596
epoch25: step1000/4680
step 37500: accuracy:0.2759999930858612, confidence:0.8697383403778076, loss:3.2438278198242188
epoch25: step1500/4680
step 50000: accuracy:0.25699999928474426, confidence:0.8710589408874512, loss:4.105998992919922
epoch25: step2000/4680
step 62500: accuracy:0.17100000381469727, confidence:0.9333986639976501, loss:5.972744941711426
epoch25: step2500/4680
step 75000: accuracy:0.2800000011920929, confidence:0.9161900877952576, loss:3.7076423168182373
epoch25: step3000/4680
step 87500: accuracy:0.3269999921321869, confidence:0.8174894452095032, loss:2.9219725131988525
epoch25: step3500/4680
step 100000: accuracy:0.0989999994635582, confidence:0.983868420124054, loss:8.37320327758789
epoch25: step4000/4680
step 112500: accuracy:0.3070000112056732, confidence:0.8372812271118164, loss:2.855421304702759
epoch25: step4500/4680
step 0: accuracy:0.24899999797344208, confidence:0.8719884753227234, loss:3.630653142929077
epoch26: step0/4680
step 13000: accuracy:0.5600000023841858, confidence:0.7023732662200928, loss:1.3221018314361572
epoch26: step500/4680
step 26000: accuracy:0.25, confidence:0.8020669221878052, loss:2.6657586097717285
epoch26: step1000/4680
step 39000: accuracy:0.257999986410141, confidence:0.7721099257469177, loss:2.363328695297241
epoch26: step1500/4680
step 52000: accuracy:0.25200000405311584, confidence:0.828980028629303, loss:3.006256580352783
epoch26: step2000/4680
step 65000: accuracy:0.3659999966621399, confidence:0.7632537484169006, loss:2.3390884399414062
epoch26: step2500/4680
step 78000: accuracy:0.25600001215934753, confidence:0.8754103779792786, loss:4.988155364990234
epoch26: step3000/4680
step 91000: accuracy:0.3440000116825104, confidence:0.772278368473053, loss:2.4365506172180176
epoch26: step3500/4680
step 104000: accuracy:0.3240000009536743, confidence:0.7879315614700317, loss:2.7987759113311768
epoch26: step4000/4680
step 117000: accuracy:0.3959999978542328, confidence:0.8165814280509949, loss:2.4458425045013428
epoch26: step4500/4680
step 0: accuracy:0.3019999861717224, confidence:0.8584579825401306, loss:3.246371030807495
epoch27: step0/4680
step 13500: accuracy:0.5210000276565552, confidence:0.7685694694519043, loss:1.4775301218032837
epoch27: step500/4680
step 27000: accuracy:0.37400001287460327, confidence:0.802410900592804, loss:2.520772933959961
epoch27: step1000/4680
step 40500: accuracy:0.3070000112056732, confidence:0.8663997650146484, loss:3.0643322467803955
epoch27: step1500/4680
step 54000: accuracy:0.2939999997615814, confidence:0.8832246661186218, loss:3.828850507736206
epoch27: step2000/4680
step 67500: accuracy:0.2980000078678131, confidence:0.8626579642295837, loss:4.090134620666504
epoch27: step2500/4680
step 81000: accuracy:0.3019999861717224, confidence:0.9133026599884033, loss:3.588172197341919
epoch27: step3000/4680
step 94500: accuracy:0.34700000286102295, confidence:0.8285800218582153, loss:3.018051862716675
epoch27: step3500/4680
step 108000: accuracy:0.11400000005960464, confidence:0.9628005623817444, loss:6.872386455535889
epoch27: step4000/4680
step 121500: accuracy:0.31200000643730164, confidence:0.8445206880569458, loss:3.0224504470825195
epoch27: step4500/4680
step 0: accuracy:0.22499999403953552, confidence:0.8937691450119019, loss:4.195054531097412
epoch28: step0/4680
step 14000: accuracy:0.4869999885559082, confidence:0.6911692023277283, loss:1.4957468509674072
epoch28: step500/4680
step 28000: accuracy:0.2619999945163727, confidence:0.797985315322876, loss:2.563631772994995
epoch28: step1000/4680
step 42000: accuracy:0.22699999809265137, confidence:0.7660521864891052, loss:2.3613202571868896
epoch28: step1500/4680
step 56000: accuracy:0.257999986410141, confidence:0.8083583116531372, loss:2.8246357440948486
epoch28: step2000/4680
step 70000: accuracy:0.34599998593330383, confidence:0.7650099992752075, loss:2.375307559967041
epoch28: step2500/4680
step 84000: accuracy:0.25600001215934753, confidence:0.8763634562492371, loss:4.717105865478516
epoch28: step3000/4680
step 98000: accuracy:0.3070000112056732, confidence:0.781891942024231, loss:2.5965967178344727
epoch28: step3500/4680
step 112000: accuracy:0.35199999809265137, confidence:0.774252712726593, loss:2.6135568618774414
epoch28: step4000/4680
step 126000: accuracy:0.42100000381469727, confidence:0.8007736206054688, loss:2.3137710094451904
epoch28: step4500/4680
step 0: accuracy:0.3409999907016754, confidence:0.8396703004837036, loss:3.0509326457977295
epoch29: step0/4680
step 14500: accuracy:0.5080000162124634, confidence:0.8030640482902527, loss:1.6118383407592773
epoch29: step500/4680
step 29000: accuracy:0.31700000166893005, confidence:0.8194003701210022, loss:2.9278435707092285
epoch29: step1000/4680
step 43500: accuracy:0.27799999713897705, confidence:0.8694116473197937, loss:3.1642167568206787
epoch29: step1500/4680
step 58000: accuracy:0.3050000071525574, confidence:0.8877325057983398, loss:3.7274324893951416
epoch29: step2000/4680
step 72500: accuracy:0.414000004529953, confidence:0.8091256618499756, loss:2.6775941848754883
epoch29: step2500/4680
step 87000: accuracy:0.36399999260902405, confidence:0.8906711339950562, loss:3.442958116531372
epoch29: step3000/4680
step 101500: accuracy:0.30300000309944153, confidence:0.8471458554267883, loss:3.4244439601898193
epoch29: step3500/4680
step 116000: accuracy:0.10300000011920929, confidence:0.9926502704620361, loss:8.747446060180664
epoch29: step4000/4680
step 130500: accuracy:0.2669999897480011, confidence:0.8591383695602417, loss:3.2179276943206787
epoch29: step4500/4680
2018-06-15 19:49:11.404709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:49:11.404913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
step 0: accuracy:0.05299999937415123, confidence:0.5501519441604614, loss:6.615966320037842
epoch0: step0/4680
step 0: accuracy:0.11500000208616257, confidence:1.0, loss:29.865264892578125
epoch0: step500/4680
step 0: accuracy:0.10700000077486038, confidence:0.9999943971633911, loss:18.11923599243164
epoch0: step1000/4680
step 0: accuracy:0.10199999809265137, confidence:0.9994786381721497, loss:9.05529499053955
epoch0: step1500/4680
step 0: accuracy:0.1340000033378601, confidence:0.998528003692627, loss:8.019097328186035
epoch0: step2000/4680
step 0: accuracy:0.09300000220537186, confidence:0.9997026920318604, loss:10.765586853027344
epoch0: step2500/4680
step 0: accuracy:0.07900000363588333, confidence:0.9992592334747314, loss:8.999239921569824
epoch0: step3000/4680
step 0: accuracy:0.07900000363588333, confidence:0.9984508752822876, loss:8.853843688964844
epoch0: step3500/4680
step 0: accuracy:0.10199999809265137, confidence:0.9998926520347595, loss:10.507180213928223
epoch0: step4000/4680
step 0: accuracy:0.10499999672174454, confidence:0.9999839067459106, loss:12.884786605834961
epoch0: step4500/4680
step 0: accuracy:0.1080000028014183, confidence:0.9999527931213379, loss:11.635003089904785
epoch1: step0/4680
step 500: accuracy:0.09000000357627869, confidence:0.999930739402771, loss:11.606931686401367
epoch1: step500/4680
step 1000: accuracy:0.10700000077486038, confidence:0.9954637885093689, loss:7.877140045166016
epoch1: step1000/4680
step 1500: accuracy:0.10499999672174454, confidence:0.9986155033111572, loss:8.230690956115723
epoch1: step1500/4680
step 2000: accuracy:0.14000000059604645, confidence:0.9983705878257751, loss:7.49485445022583
epoch1: step2000/4680
step 2500: accuracy:0.09200000017881393, confidence:0.9995521306991577, loss:9.311918258666992
epoch1: step2500/4680
step 3000: accuracy:0.10400000214576721, confidence:0.9999934434890747, loss:13.609755516052246
epoch1: step3000/4680
step 3500: accuracy:0.11400000005960464, confidence:0.9989970326423645, loss:8.73969554901123
epoch1: step3500/4680
step 4000: accuracy:0.10100000351667404, confidence:0.9995786547660828, loss:9.332174301147461
epoch1: step4000/4680
step 4500: accuracy:0.08699999749660492, confidence:0.9994602203369141, loss:9.460967063903809
epoch1: step4500/4680
step 0: accuracy:0.09600000083446503, confidence:0.9995986223220825, loss:9.576981544494629
epoch2: step0/4680
step 1000: accuracy:0.10700000077486038, confidence:0.9774883985519409, loss:5.361879348754883
epoch2: step500/4680
step 2000: accuracy:0.09700000286102295, confidence:0.9936474561691284, loss:7.605167388916016
epoch2: step1000/4680
step 3000: accuracy:0.09399999678134918, confidence:0.9983964562416077, loss:8.189862251281738
epoch2: step1500/4680
step 4000: accuracy:0.11699999868869781, confidence:0.9954907894134521, loss:6.80539083480835
epoch2: step2000/4680
step 5000: accuracy:0.07800000160932541, confidence:0.9931097030639648, loss:7.065523624420166
epoch2: step2500/4680
step 6000: accuracy:0.0949999988079071, confidence:0.9999880194664001, loss:13.428788185119629
epoch2: step3000/4680
step 7000: accuracy:0.1080000028014183, confidence:0.9970483183860779, loss:7.394381046295166
epoch2: step3500/4680
step 8000: accuracy:0.08399999886751175, confidence:0.9995582699775696, loss:9.70695686340332
epoch2: step4000/4680
step 9000: accuracy:0.10599999874830246, confidence:0.998436689376831, loss:8.291778564453125
epoch2: step4500/4680
step 0: accuracy:0.09600000083446503, confidence:0.999358594417572, loss:9.178125381469727
epoch3: step0/4680
step 1500: accuracy:0.08699999749660492, confidence:0.9904846549034119, loss:6.234416961669922
epoch3: step500/4680
step 3000: accuracy:0.10300000011920929, confidence:0.9613472819328308, loss:5.13675594329834
epoch3: step1000/4680
step 4500: accuracy:0.11699999868869781, confidence:0.9964684247970581, loss:7.260156631469727
epoch3: step1500/4680
step 6000: accuracy:0.1289999932050705, confidence:0.9999502897262573, loss:11.248848915100098
epoch3: step2000/4680
step 7500: accuracy:0.08900000154972076, confidence:0.9953807592391968, loss:6.865624904632568
epoch3: step2500/4680
step 9000: accuracy:0.0989999994635582, confidence:0.9995949864387512, loss:9.926900863647461
epoch3: step3000/4680
step 10500: accuracy:0.11500000208616257, confidence:0.9983637928962708, loss:7.816512107849121
epoch3: step3500/4680
step 12000: accuracy:0.09000000357627869, confidence:0.9997256994247437, loss:9.682339668273926
epoch3: step4000/4680
step 13500: accuracy:0.1080000028014183, confidence:0.9990766048431396, loss:9.083906173706055
epoch3: step4500/4680
step 0: accuracy:0.10100000351667404, confidence:0.9995856285095215, loss:9.815277099609375
epoch4: step0/4680
step 2000: accuracy:0.10400000214576721, confidence:0.9786781668663025, loss:5.1698503494262695
epoch4: step500/4680
step 4000: accuracy:0.10199999809265137, confidence:0.8048864603042603, loss:3.3976175785064697
epoch4: step1000/4680
step 6000: accuracy:0.0989999994635582, confidence:0.9981972575187683, loss:8.292802810668945
epoch4: step1500/4680
step 8000: accuracy:0.10400000214576721, confidence:0.9992469549179077, loss:8.818912506103516
epoch4: step2000/4680
step 10000: accuracy:0.07400000095367432, confidence:0.9842516183853149, loss:6.567200183868408
epoch4: step2500/4680
step 12000: accuracy:0.09600000083446503, confidence:0.993867814540863, loss:7.12923002243042
epoch4: step3000/4680
step 14000: accuracy:0.10999999940395355, confidence:0.9792419672012329, loss:5.580802917480469
epoch4: step3500/4680
step 16000: accuracy:0.10199999809265137, confidence:0.9978781938552856, loss:8.026833534240723
epoch4: step4000/4680
step 18000: accuracy:0.10599999874830246, confidence:0.9940351247787476, loss:7.080545902252197
epoch4: step4500/4680
step 0: accuracy:0.10499999672174454, confidence:0.9982492327690125, loss:8.328330039978027
epoch5: step0/4680
step 2500: accuracy:0.1340000033378601, confidence:0.7597653269767761, loss:2.882136583328247
epoch5: step500/4680
step 5000: accuracy:0.26600000262260437, confidence:0.5768266320228577, loss:2.2113003730773926
epoch5: step1000/4680
step 7500: accuracy:0.09300000220537186, confidence:0.9424802660942078, loss:4.5252885818481445
epoch5: step1500/4680
step 10000: accuracy:0.1899999976158142, confidence:0.7463262677192688, loss:2.8448867797851562
epoch5: step2000/4680
step 12500: accuracy:0.1770000010728836, confidence:0.9331463575363159, loss:7.419864654541016
epoch5: step2500/4680
step 15000: accuracy:0.11800000071525574, confidence:0.9049619436264038, loss:4.568240165710449
epoch5: step3000/4680
step 17500: accuracy:0.16599999368190765, confidence:0.8275839686393738, loss:3.5858652591705322
epoch5: step3500/4680
step 20000: accuracy:0.0860000029206276, confidence:0.992347776889801, loss:8.37745189666748
epoch5: step4000/4680
step 22500: accuracy:0.09399999678134918, confidence:0.9875416159629822, loss:6.944595813751221
epoch5: step4500/4680
step 0: accuracy:0.10599999874830246, confidence:0.9964809417724609, loss:7.9797797203063965
epoch6: step0/4680
step 3000: accuracy:0.257999986410141, confidence:0.6919271945953369, loss:2.360316514968872
epoch6: step500/4680
step 6000: accuracy:0.35600000619888306, confidence:0.5934260487556458, loss:1.9641247987747192
epoch6: step1000/4680
step 9000: accuracy:0.16200000047683716, confidence:0.8262540102005005, loss:3.1167001724243164
epoch6: step1500/4680
step 12000: accuracy:0.21899999678134918, confidence:0.694198727607727, loss:2.493109703063965
epoch6: step2000/4680
step 15000: accuracy:0.16200000047683716, confidence:0.9066704511642456, loss:6.299095153808594
epoch6: step2500/4680
step 18000: accuracy:0.14499999582767487, confidence:0.906259298324585, loss:4.83266544342041
epoch6: step3000/4680
step 21000: accuracy:0.2150000035762787, confidence:0.8224592208862305, loss:3.3931281566619873
epoch6: step3500/4680
step 24000: accuracy:0.08900000154972076, confidence:0.9830997586250305, loss:7.0810346603393555
epoch6: step4000/4680
step 27000: accuracy:0.10100000351667404, confidence:0.9671988487243652, loss:6.237280368804932
epoch6: step4500/4680
step 0: accuracy:0.11299999803304672, confidence:0.9868966341018677, loss:7.127256393432617
epoch7: step0/4680
step 3500: accuracy:0.367000013589859, confidence:0.6448235511779785, loss:2.0878937244415283
epoch7: step500/4680
step 7000: accuracy:0.4230000078678131, confidence:0.6060568690299988, loss:1.7807910442352295
epoch7: step1000/4680
step 10500: accuracy:0.25699999928474426, confidence:0.761534571647644, loss:2.4868478775024414
epoch7: step1500/4680
step 14000: accuracy:0.23000000417232513, confidence:0.705613374710083, loss:2.4245615005493164
epoch7: step2000/4680
step 17500: accuracy:0.19900000095367432, confidence:0.9035608172416687, loss:5.618891716003418
epoch7: step2500/4680
step 21000: accuracy:0.1509999930858612, confidence:0.9156759977340698, loss:4.9795098304748535
epoch7: step3000/4680
step 24500: accuracy:0.23600000143051147, confidence:0.8363398909568787, loss:3.236966371536255
epoch7: step3500/4680
step 28000: accuracy:0.09399999678134918, confidence:0.9739900231361389, loss:7.458521366119385
epoch7: step4000/4680
step 31500: accuracy:0.16200000047683716, confidence:0.9300412535667419, loss:5.3299360275268555
epoch7: step4500/4680
step 0: accuracy:0.10199999809265137, confidence:0.9682615995407104, loss:6.665317535400391
epoch8: step0/4680
step 4000: accuracy:0.3540000021457672, confidence:0.7006934881210327, loss:2.2500460147857666
epoch8: step500/4680
step 8000: accuracy:0.421999990940094, confidence:0.6678138971328735, loss:1.9066643714904785
epoch8: step1000/4680
step 12000: accuracy:0.30799999833106995, confidence:0.7265790700912476, loss:2.295008897781372
epoch8: step1500/4680
step 16000: accuracy:0.29899999499320984, confidence:0.7207413911819458, loss:2.221081256866455
epoch8: step2000/4680
step 20000: accuracy:0.20800000429153442, confidence:0.8755866885185242, loss:5.158483982086182
epoch8: step2500/4680
step 24000: accuracy:0.164000004529953, confidence:0.8552829623222351, loss:4.394954204559326
epoch8: step3000/4680
step 28000: accuracy:0.27799999713897705, confidence:0.7889682650566101, loss:2.823364496231079
epoch8: step3500/4680
step 32000: accuracy:0.10400000214576721, confidence:0.9628541469573975, loss:7.196288108825684
epoch8: step4000/4680
step 36000: accuracy:0.19599999487400055, confidence:0.9033759832382202, loss:4.557869911193848
epoch8: step4500/4680
step 0: accuracy:0.14100000262260437, confidence:0.9574624300003052, loss:6.0352864265441895
epoch9: step0/4680
step 4500: accuracy:0.3610000014305115, confidence:0.7138589024543762, loss:2.4218997955322266
epoch9: step500/4680
step 9000: accuracy:0.45500001311302185, confidence:0.687833309173584, loss:1.8466570377349854
epoch9: step1000/4680
step 13500: accuracy:0.375, confidence:0.7125661969184875, loss:2.0636439323425293
epoch9: step1500/4680
step 18000: accuracy:0.328000009059906, confidence:0.7514235377311707, loss:2.2508153915405273
epoch9: step2000/4680
step 22500: accuracy:0.1850000023841858, confidence:0.8795844316482544, loss:4.973668098449707
epoch9: step2500/4680
step 27000: accuracy:0.16899999976158142, confidence:0.8962428569793701, loss:4.989941120147705
epoch9: step3000/4680
step 31500: accuracy:0.2370000034570694, confidence:0.8513314723968506, loss:3.574517011642456
epoch9: step3500/4680
step 36000: accuracy:0.0989999994635582, confidence:0.9585806727409363, loss:7.134385585784912
epoch9: step4000/4680
step 40500: accuracy:0.2160000056028366, confidence:0.9196876287460327, loss:4.8533477783203125
epoch9: step4500/4680
step 0: accuracy:0.13500000536441803, confidence:0.9541881084442139, loss:6.270068168640137
epoch10: step0/4680
step 5000: accuracy:0.335999995470047, confidence:0.7508257031440735, loss:2.621983528137207
epoch10: step500/4680
step 10000: accuracy:0.421999990940094, confidence:0.7045099139213562, loss:2.0583622455596924
epoch10: step1000/4680
step 15000: accuracy:0.33500000834465027, confidence:0.747146725654602, loss:2.294785737991333
epoch10: step1500/4680
step 20000: accuracy:0.3019999861717224, confidence:0.7579621076583862, loss:2.4254279136657715
epoch10: step2000/4680
step 25000: accuracy:0.2409999966621399, confidence:0.8711516261100769, loss:4.4651031494140625
epoch10: step2500/4680
step 30000: accuracy:0.1850000023841858, confidence:0.8535495400428772, loss:4.489377021789551
epoch10: step3000/4680
step 35000: accuracy:0.2770000100135803, confidence:0.7973942756652832, loss:2.9182145595550537
epoch10: step3500/4680
step 40000: accuracy:0.09700000286102295, confidence:0.954857349395752, loss:7.567173004150391
epoch10: step4000/4680
step 45000: accuracy:0.1889999955892563, confidence:0.8984774351119995, loss:4.446777820587158
epoch10: step4500/4680
step 0: accuracy:0.15600000321865082, confidence:0.9472218155860901, loss:5.772310256958008
epoch11: step0/4680
step 5500: accuracy:0.37400001287460327, confidence:0.7227962017059326, loss:2.4105191230773926
epoch11: step500/4680
step 11000: accuracy:0.46799999475479126, confidence:0.7062422633171082, loss:1.9339313507080078
epoch11: step1000/4680
step 16500: accuracy:0.43799999356269836, confidence:0.7370366454124451, loss:2.212770700454712
epoch11: step1500/4680
step 22000: accuracy:0.375, confidence:0.734992265701294, loss:2.1086905002593994
epoch11: step2000/4680
step 27500: accuracy:0.23199999332427979, confidence:0.8513392210006714, loss:4.2088775634765625
epoch11: step2500/4680
step 33000: accuracy:0.2280000001192093, confidence:0.8492545485496521, loss:4.021554470062256
epoch11: step3000/4680
step 38500: accuracy:0.2930000126361847, confidence:0.8209906816482544, loss:3.0126819610595703
epoch11: step3500/4680
step 44000: accuracy:0.10999999940395355, confidence:0.9522237777709961, loss:7.271815299987793
epoch11: step4000/4680
step 49500: accuracy:0.22599999606609344, confidence:0.8836305141448975, loss:4.153636932373047
epoch11: step4500/4680
step 0: accuracy:0.18000000715255737, confidence:0.9342339038848877, loss:5.54990816116333
epoch12: step0/4680
step 6000: accuracy:0.35499998927116394, confidence:0.7819259762763977, loss:2.8563573360443115
epoch12: step500/4680
step 12000: accuracy:0.42800000309944153, confidence:0.7441002130508423, loss:2.322195291519165
epoch12: step1000/4680
step 18000: accuracy:0.3160000145435333, confidence:0.782377302646637, loss:2.5405473709106445
epoch12: step1500/4680
step 24000: accuracy:0.36399999260902405, confidence:0.760274350643158, loss:2.201561689376831
epoch12: step2000/4680
step 30000: accuracy:0.23199999332427979, confidence:0.8438950181007385, loss:4.011249542236328
epoch12: step2500/4680
step 36000: accuracy:0.23600000143051147, confidence:0.8238033056259155, loss:3.9684486389160156
epoch12: step3000/4680
step 42000: accuracy:0.32499998807907104, confidence:0.7931561470031738, loss:2.8592143058776855
epoch12: step3500/4680
step 48000: accuracy:0.10300000011920929, confidence:0.9639731049537659, loss:8.227996826171875
epoch12: step4000/4680
step 54000: accuracy:0.210999995470047, confidence:0.901439905166626, loss:4.090174198150635
epoch12: step4500/4680
step 0: accuracy:0.16599999368190765, confidence:0.9389203786849976, loss:5.465459823608398
epoch13: step0/4680
step 6500: accuracy:0.36000001430511475, confidence:0.748945415019989, loss:2.564434051513672
epoch13: step500/4680
step 13000: accuracy:0.49300000071525574, confidence:0.7362287640571594, loss:1.881201148033142
epoch13: step1000/4680
step 19500: accuracy:0.3959999978542328, confidence:0.750516414642334, loss:2.4127578735351562
epoch13: step1500/4680
step 26000: accuracy:0.4300000071525574, confidence:0.7538782358169556, loss:1.9821370840072632
epoch13: step2000/4680
step 32500: accuracy:0.21799999475479126, confidence:0.862553596496582, loss:4.372990608215332
epoch13: step2500/4680
step 39000: accuracy:0.289000004529953, confidence:0.8547651767730713, loss:3.9031856060028076
epoch13: step3000/4680
step 45500: accuracy:0.3009999990463257, confidence:0.854733407497406, loss:3.350919485092163
epoch13: step3500/4680
step 52000: accuracy:0.12099999934434891, confidence:0.9531716704368591, loss:7.177305221557617
epoch13: step4000/4680
step 58500: accuracy:0.24400000274181366, confidence:0.8841032385826111, loss:4.441455841064453
epoch13: step4500/4680
step 0: accuracy:0.1589999943971634, confidence:0.931547999382019, loss:5.767968654632568
epoch14: step0/4680
step 7000: accuracy:0.3190000057220459, confidence:0.8199920058250427, loss:3.2632393836975098
epoch14: step500/4680
step 14000: accuracy:0.32899999618530273, confidence:0.7919427752494812, loss:2.8587112426757812
epoch14: step1000/4680
step 21000: accuracy:0.2939999997615814, confidence:0.8309533596038818, loss:2.996077060699463
epoch14: step1500/4680
step 28000: accuracy:0.36800000071525574, confidence:0.7682799100875854, loss:2.161890983581543
epoch14: step2000/4680
step 35000: accuracy:0.2370000034570694, confidence:0.855044424533844, loss:4.006869792938232
epoch14: step2500/4680
step 42000: accuracy:0.24899999797344208, confidence:0.8491503000259399, loss:4.0501389503479
epoch14: step3000/4680
step 49000: accuracy:0.36899998784065247, confidence:0.8065177202224731, loss:2.6303954124450684
epoch14: step3500/4680
step 56000: accuracy:0.10599999874830246, confidence:0.9616780877113342, loss:7.786674976348877
epoch14: step4000/4680
step 63000: accuracy:0.19699999690055847, confidence:0.893581748008728, loss:3.971959114074707
epoch14: step4500/4680
step 0: accuracy:0.17900000512599945, confidence:0.9399125576019287, loss:5.382706642150879
epoch15: step0/4680
step 7500: accuracy:0.3919999897480011, confidence:0.768714189529419, loss:2.5333433151245117
epoch15: step500/4680
step 15000: accuracy:0.460999995470047, confidence:0.7591938376426697, loss:2.165325880050659
epoch15: step1000/4680
step 22500: accuracy:0.3919999897480011, confidence:0.750959038734436, loss:2.453556537628174
epoch15: step1500/4680
step 30000: accuracy:0.3630000054836273, confidence:0.7763731479644775, loss:2.328275203704834
epoch15: step2000/4680
step 37500: accuracy:0.22699999809265137, confidence:0.8709128499031067, loss:4.759328842163086
epoch15: step2500/4680
step 45000: accuracy:0.24300000071525574, confidence:0.8301084637641907, loss:4.1819376945495605
epoch15: step3000/4680
step 52500: accuracy:0.3440000116825104, confidence:0.8258430361747742, loss:2.7751495838165283
epoch15: step3500/4680
step 60000: accuracy:0.10599999874830246, confidence:0.9602692723274231, loss:7.987163066864014
epoch15: step4000/4680
step 67500: accuracy:0.26100000739097595, confidence:0.8971126675605774, loss:4.472073554992676
epoch15: step4500/4680
step 0: accuracy:0.1899999976158142, confidence:0.9325985312461853, loss:5.87543249130249
epoch16: step0/4680
step 8000: accuracy:0.3190000057220459, confidence:0.8525161147117615, loss:3.329695224761963
epoch16: step500/4680
step 16000: accuracy:0.29100000858306885, confidence:0.858678936958313, loss:3.5923852920532227
epoch16: step1000/4680
step 24000: accuracy:0.28299999237060547, confidence:0.8531094193458557, loss:3.218386650085449
epoch16: step1500/4680
step 32000: accuracy:0.4009999930858612, confidence:0.7811247110366821, loss:2.151461362838745
epoch16: step2000/4680
step 40000: accuracy:0.21799999475479126, confidence:0.8508299589157104, loss:4.031363487243652
epoch16: step2500/4680
step 48000: accuracy:0.25999999046325684, confidence:0.8491515517234802, loss:3.9774980545043945
epoch16: step3000/4680
step 56000: accuracy:0.34299999475479126, confidence:0.8302834630012512, loss:2.986661195755005
epoch16: step3500/4680
step 64000: accuracy:0.08699999749660492, confidence:0.9725489616394043, loss:8.804059028625488
epoch16: step4000/4680
step 72000: accuracy:0.20900000631809235, confidence:0.8894366025924683, loss:4.392826557159424
epoch16: step4500/4680
step 0: accuracy:0.16500000655651093, confidence:0.9435036778450012, loss:5.755624294281006
epoch17: step0/4680
step 8500: accuracy:0.3310000002384186, confidence:0.8025001287460327, loss:3.2842555046081543
epoch17: step500/4680
step 17000: accuracy:0.382999986410141, confidence:0.7675377726554871, loss:2.8776726722717285
epoch17: step1000/4680
step 25500: accuracy:0.4000000059604645, confidence:0.7716242074966431, loss:2.4843852519989014
epoch17: step1500/4680
step 34000: accuracy:0.38100001215934753, confidence:0.7755259871482849, loss:2.504526376724243
epoch17: step2000/4680
step 42500: accuracy:0.22300000488758087, confidence:0.8649114370346069, loss:4.542879581451416
epoch17: step2500/4680
step 51000: accuracy:0.23199999332427979, confidence:0.8347249031066895, loss:4.159936428070068
epoch17: step3000/4680
step 59500: accuracy:0.26100000739097595, confidence:0.8479821681976318, loss:3.4848599433898926
epoch17: step3500/4680
step 68000: accuracy:0.11999999731779099, confidence:0.9481963515281677, loss:8.0134859085083
epoch17: step4000/4680
step 76500: accuracy:0.2460000067949295, confidence:0.8813352584838867, loss:4.44803524017334
epoch17: step4500/4680
step 0: accuracy:0.20100000500679016, confidence:0.9168509244918823, loss:5.317180156707764
epoch18: step0/4680
step 9000: accuracy:0.34299999475479126, confidence:0.8388422727584839, loss:3.0307881832122803
epoch18: step500/4680
step 18000: accuracy:0.3400000035762787, confidence:0.8312168717384338, loss:3.03402042388916
epoch18: step1000/4680
step 27000: accuracy:0.3569999933242798, confidence:0.8044811487197876, loss:2.4937496185302734
epoch18: step1500/4680
step 36000: accuracy:0.43799999356269836, confidence:0.7457276582717896, loss:1.8885703086853027
epoch18: step2000/4680
step 45000: accuracy:0.2639999985694885, confidence:0.8420746326446533, loss:3.580939531326294
epoch18: step2500/4680
step 54000: accuracy:0.18799999356269836, confidence:0.8747563362121582, loss:4.83153772354126
epoch18: step3000/4680
step 63000: accuracy:0.3889999985694885, confidence:0.7969075441360474, loss:2.5259785652160645
epoch18: step3500/4680
step 72000: accuracy:0.09700000286102295, confidence:0.9556024074554443, loss:7.907690048217773
epoch18: step4000/4680
step 81000: accuracy:0.18799999356269836, confidence:0.9043387174606323, loss:4.313018321990967
epoch18: step4500/4680
step 0: accuracy:0.16200000047683716, confidence:0.9521422386169434, loss:5.947358131408691
epoch19: step0/4680
step 9500: accuracy:0.39500001072883606, confidence:0.7764681577682495, loss:2.728290319442749
epoch19: step500/4680
step 19000: accuracy:0.4449999928474426, confidence:0.74293053150177, loss:2.3452043533325195
epoch19: step1000/4680
step 28500: accuracy:0.3869999945163727, confidence:0.7487386465072632, loss:2.3917505741119385
epoch19: step1500/4680
step 38000: accuracy:0.4000000059604645, confidence:0.7842978835105896, loss:2.4022011756896973
epoch19: step2000/4680
step 47500: accuracy:0.2240000069141388, confidence:0.8747139573097229, loss:4.348607540130615
epoch19: step2500/4680
step 57000: accuracy:0.3149999976158142, confidence:0.8487535715103149, loss:3.6537530422210693
epoch19: step3000/4680
step 66500: accuracy:0.28200000524520874, confidence:0.8652172684669495, loss:3.484050750732422
epoch19: step3500/4680
step 76000: accuracy:0.12999999523162842, confidence:0.9355635046958923, loss:7.547504901885986
epoch19: step4000/4680
step 85500: accuracy:0.25099998712539673, confidence:0.8701857924461365, loss:4.202229022979736
epoch19: step4500/4680
step 0: accuracy:0.1850000023841858, confidence:0.920646607875824, loss:5.427680015563965
epoch20: step0/4680
step 10000: accuracy:0.44699999690055847, confidence:0.7882359623908997, loss:2.4610555171966553
epoch20: step500/4680
step 20000: accuracy:0.35100001096725464, confidence:0.8371977806091309, loss:3.1645679473876953
epoch20: step1000/4680
step 30000: accuracy:0.41200000047683716, confidence:0.8099639415740967, loss:2.5718071460723877
epoch20: step1500/4680
step 40000: accuracy:0.42800000309944153, confidence:0.7860856652259827, loss:2.0726590156555176
epoch20: step2000/4680
step 50000: accuracy:0.26499998569488525, confidence:0.8560881018638611, loss:3.7312817573547363
epoch20: step2500/4680
step 60000: accuracy:0.13600000739097595, confidence:0.969875156879425, loss:7.14259672164917
epoch20: step3000/4680
step 70000: accuracy:0.3440000116825104, confidence:0.8153013586997986, loss:2.944221019744873
epoch20: step3500/4680
step 80000: accuracy:0.13300000131130219, confidence:0.9721609354019165, loss:7.1811933517456055
epoch20: step4000/4680
step 90000: accuracy:0.18199999630451202, confidence:0.8922867774963379, loss:4.068132400512695
epoch20: step4500/4680
step 0: accuracy:0.17399999499320984, confidence:0.9528142213821411, loss:5.725236415863037
epoch21: step0/4680
step 10500: accuracy:0.3709999918937683, confidence:0.7705204486846924, loss:2.7152254581451416
epoch21: step500/4680
step 21000: accuracy:0.4449999928474426, confidence:0.7447078824043274, loss:2.3508808612823486
epoch21: step1000/4680
step 31500: accuracy:0.4169999957084656, confidence:0.7563397288322449, loss:2.2780182361602783
epoch21: step1500/4680
step 42000: accuracy:0.4009999930858612, confidence:0.7386671304702759, loss:2.073928117752075
epoch21: step2000/4680
step 52500: accuracy:0.2639999985694885, confidence:0.8546481132507324, loss:3.5128440856933594
epoch21: step2500/4680
step 63000: accuracy:0.3160000145435333, confidence:0.8450144529342651, loss:3.4725916385650635
epoch21: step3000/4680
step 73500: accuracy:0.30300000309944153, confidence:0.8678346872329712, loss:3.4017183780670166
epoch21: step3500/4680
step 84000: accuracy:0.11999999731779099, confidence:0.9418661594390869, loss:8.101242065429688
epoch21: step4000/4680
step 94500: accuracy:0.23600000143051147, confidence:0.882044792175293, loss:4.445260524749756
epoch21: step4500/4680
step 0: accuracy:0.21299999952316284, confidence:0.9214619994163513, loss:5.520862102508545
epoch22: step0/4680
step 11000: accuracy:0.4880000054836273, confidence:0.8037280440330505, loss:2.4203102588653564
epoch22: step500/4680
step 22000: accuracy:0.34700000286102295, confidence:0.8543217778205872, loss:3.3979809284210205
epoch22: step1000/4680
step 33000: accuracy:0.3889999985694885, confidence:0.8336867690086365, loss:2.9347808361053467
epoch22: step1500/4680
step 44000: accuracy:0.44200000166893005, confidence:0.7968300580978394, loss:2.190249443054199
epoch22: step2000/4680
step 55000: accuracy:0.28299999237060547, confidence:0.8598695993423462, loss:3.493267297744751
epoch22: step2500/4680
step 66000: accuracy:0.15399999916553497, confidence:0.940370500087738, loss:6.768661022186279
epoch22: step3000/4680
step 77000: accuracy:0.3269999921321869, confidence:0.8323304653167725, loss:3.224430799484253
epoch22: step3500/4680
step 88000: accuracy:0.11299999803304672, confidence:0.9532696008682251, loss:6.726929664611816
epoch22: step4000/4680
step 99000: accuracy:0.20000000298023224, confidence:0.8718649744987488, loss:3.754908800125122
epoch22: step4500/4680
step 0: accuracy:0.16099999845027924, confidence:0.9468657374382019, loss:5.43229341506958
epoch23: step0/4680
step 11500: accuracy:0.38100001215934753, confidence:0.7705971002578735, loss:2.552253007888794
epoch23: step500/4680
step 23000: accuracy:0.5139999985694885, confidence:0.7231011390686035, loss:1.8072437047958374
epoch23: step1000/4680
step 34500: accuracy:0.40400001406669617, confidence:0.7526276707649231, loss:2.2649002075195312
epoch23: step1500/4680
step 46000: accuracy:0.4129999876022339, confidence:0.7647354602813721, loss:2.0894429683685303
epoch23: step2000/4680
step 57500: accuracy:0.2930000126361847, confidence:0.8398709893226624, loss:3.3203742504119873
epoch23: step2500/4680
step 69000: accuracy:0.34700000286102295, confidence:0.817622184753418, loss:3.128709554672241
epoch23: step3000/4680
step 80500: accuracy:0.2800000011920929, confidence:0.8623687624931335, loss:3.557694673538208
epoch23: step3500/4680
step 92000: accuracy:0.15000000596046448, confidence:0.9225691556930542, loss:7.44950532913208
epoch23: step4000/4680
step 103500: accuracy:0.24899999797344208, confidence:0.8653368949890137, loss:4.245322227478027
epoch23: step4500/4680
step 0: accuracy:0.21799999475479126, confidence:0.9204354882240295, loss:5.209668159484863
epoch24: step0/4680
step 12000: accuracy:0.4390000104904175, confidence:0.7891455292701721, loss:2.646376848220825
epoch24: step500/4680
step 24000: accuracy:0.35199999809265137, confidence:0.8890684247016907, loss:3.7111587524414062
epoch24: step1000/4680
step 36000: accuracy:0.4020000100135803, confidence:0.8415165543556213, loss:2.825839042663574
epoch24: step1500/4680
step 48000: accuracy:0.42500001192092896, confidence:0.8049337863922119, loss:2.255648136138916
epoch24: step2000/4680
step 60000: accuracy:0.2800000011920929, confidence:0.8627405166625977, loss:3.7752366065979004
epoch24: step2500/4680
step 72000: accuracy:0.1860000044107437, confidence:0.8936521410942078, loss:5.099637508392334
epoch24: step3000/4680
step 84000: accuracy:0.36000001430511475, confidence:0.8316933512687683, loss:3.004861831665039
epoch24: step3500/4680
step 96000: accuracy:0.1289999932050705, confidence:0.954267144203186, loss:6.374128341674805
epoch24: step4000/4680
step 108000: accuracy:0.18700000643730164, confidence:0.8789055943489075, loss:3.8386552333831787
epoch24: step4500/4680
step 0: accuracy:0.1550000011920929, confidence:0.943293571472168, loss:5.436097145080566
epoch25: step0/4680
step 12500: accuracy:0.4339999854564667, confidence:0.734677255153656, loss:2.14253306388855
epoch25: step500/4680
step 25000: accuracy:0.49300000071525574, confidence:0.710307240486145, loss:1.8765228986740112
epoch25: step1000/4680
step 37500: accuracy:0.3970000147819519, confidence:0.7589231133460999, loss:2.302119731903076
epoch25: step1500/4680
step 50000: accuracy:0.44200000166893005, confidence:0.7527111172676086, loss:2.0131914615631104
epoch25: step2000/4680
step 62500: accuracy:0.2590000033378601, confidence:0.8550795912742615, loss:3.7276391983032227
epoch25: step2500/4680
step 75000: accuracy:0.3370000123977661, confidence:0.8384061455726624, loss:3.304622173309326
epoch25: step3000/4680
step 87500: accuracy:0.27000001072883606, confidence:0.8580998182296753, loss:3.509617805480957
epoch25: step3500/4680
step 100000: accuracy:0.1459999978542328, confidence:0.9245926141738892, loss:7.726166248321533
epoch25: step4000/4680
step 112500: accuracy:0.27000001072883606, confidence:0.8543699979782104, loss:3.895164966583252
epoch25: step4500/4680
step 0: accuracy:0.20800000429153442, confidence:0.9172184467315674, loss:5.452128887176514
epoch26: step0/4680
step 13000: accuracy:0.296999990940094, confidence:0.8569698929786682, loss:4.425681114196777
epoch26: step500/4680
step 26000: accuracy:0.2849999964237213, confidence:0.891019880771637, loss:4.3416547775268555
epoch26: step1000/4680
step 39000: accuracy:0.3140000104904175, confidence:0.8716286420822144, loss:3.409055709838867
epoch26: step1500/4680
step 52000: accuracy:0.4180000126361847, confidence:0.7947545647621155, loss:2.2780072689056396
epoch26: step2000/4680
step 65000: accuracy:0.25999999046325684, confidence:0.8609893321990967, loss:3.746584415435791
epoch26: step2500/4680
step 78000: accuracy:0.11699999868869781, confidence:0.966262698173523, loss:7.7769904136657715
epoch26: step3000/4680
step 91000: accuracy:0.3230000138282776, confidence:0.8520019054412842, loss:3.329035758972168
epoch26: step3500/4680
step 104000: accuracy:0.09700000286102295, confidence:0.9750975966453552, loss:7.499553680419922
epoch26: step4000/4680
step 117000: accuracy:0.17499999701976776, confidence:0.9048230051994324, loss:4.27166748046875
epoch26: step4500/4680
step 0: accuracy:0.15800000727176666, confidence:0.9499987363815308, loss:5.674537658691406
epoch27: step0/4680
step 13500: accuracy:0.375, confidence:0.7545349597930908, loss:2.417680025100708
epoch27: step500/4680
step 27000: accuracy:0.4699999988079071, confidence:0.7002622485160828, loss:1.8984249830245972
epoch27: step1000/4680
step 40500: accuracy:0.39100000262260437, confidence:0.7649708390235901, loss:2.3531343936920166
epoch27: step1500/4680
step 54000: accuracy:0.3970000147819519, confidence:0.7534385919570923, loss:2.1063497066497803
epoch27: step2000/4680
step 67500: accuracy:0.23499999940395355, confidence:0.8574931025505066, loss:3.7945268154144287
epoch27: step2500/4680
step 81000: accuracy:0.3409999907016754, confidence:0.8212021589279175, loss:3.1594974994659424
epoch27: step3000/4680
step 94500: accuracy:0.23800000548362732, confidence:0.8665369749069214, loss:3.658021926879883
epoch27: step3500/4680
step 108000: accuracy:0.13099999725818634, confidence:0.9241302609443665, loss:7.818714141845703
epoch27: step4000/4680
step 121500: accuracy:0.2680000066757202, confidence:0.8493456244468689, loss:3.8536298274993896
epoch27: step4500/4680
step 0: accuracy:0.20900000631809235, confidence:0.922978401184082, loss:5.293053150177002
epoch28: step0/4680
step 14000: accuracy:0.3269999921321869, confidence:0.8489557504653931, loss:3.828185558319092
epoch28: step500/4680
step 28000: accuracy:0.2800000011920929, confidence:0.8967199921607971, loss:4.329124927520752
epoch28: step1000/4680
step 42000: accuracy:0.38999998569488525, confidence:0.8573674559593201, loss:2.867326259613037
epoch28: step1500/4680
step 56000: accuracy:0.37700000405311584, confidence:0.7989541888237, loss:2.4764041900634766
epoch28: step2000/4680
step 70000: accuracy:0.2879999876022339, confidence:0.848543107509613, loss:3.5355217456817627
epoch28: step2500/4680
step 84000: accuracy:0.12600000202655792, confidence:0.9606479406356812, loss:7.183354377746582
epoch28: step3000/4680
step 98000: accuracy:0.33399999141693115, confidence:0.8275834918022156, loss:2.9838154315948486
epoch28: step3500/4680
step 112000: accuracy:0.11400000005960464, confidence:0.9590909481048584, loss:6.986398696899414
epoch28: step4000/4680
step 126000: accuracy:0.20000000298023224, confidence:0.89189612865448, loss:4.00108528137207
epoch28: step4500/4680
step 0: accuracy:0.17900000512599945, confidence:0.9430490732192993, loss:5.237818717956543
epoch29: step0/4680
step 14500: accuracy:0.3840000033378601, confidence:0.7520812749862671, loss:2.420280694961548
epoch29: step500/4680
step 29000: accuracy:0.47200000286102295, confidence:0.7062996625900269, loss:2.0216541290283203
epoch29: step1000/4680
step 43500: accuracy:0.4009999930858612, confidence:0.7674760818481445, loss:2.453388214111328
epoch29: step1500/4680
step 58000: accuracy:0.4339999854564667, confidence:0.7581420540809631, loss:2.0163514614105225
epoch29: step2000/4680
step 72500: accuracy:0.23100000619888306, confidence:0.8695093393325806, loss:4.13735294342041
epoch29: step2500/4680
step 87000: accuracy:0.3319999873638153, confidence:0.807174801826477, loss:3.056893825531006
epoch29: step3000/4680
step 101500: accuracy:0.22599999606609344, confidence:0.8572334051132202, loss:3.7114901542663574
epoch29: step3500/4680
step 116000: accuracy:0.11599999666213989, confidence:0.9506329894065857, loss:8.130290985107422
epoch29: step4000/4680
step 130500: accuracy:0.24199999868869781, confidence:0.8535069227218628, loss:4.042893886566162
epoch29: step4500/4680
2018-06-15 19:57:43.963488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:57:43.963702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
step 0: accuracy:0.10100000351667404, confidence:0.6496676206588745, loss:6.786223411560059
epoch0: step0/4680
step 0: accuracy:0.09000000357627869, confidence:1.0, loss:35.82161331176758
epoch0: step500/4680
step 0: accuracy:0.08799999952316284, confidence:0.9999918341636658, loss:16.810565948486328
epoch0: step1000/4680
step 0: accuracy:0.10400000214576721, confidence:0.9998592138290405, loss:10.595843315124512
epoch0: step1500/4680
step 0: accuracy:0.10000000149011612, confidence:0.999911904335022, loss:11.291315078735352
epoch0: step2000/4680
step 0: accuracy:0.09300000220537186, confidence:0.9999980330467224, loss:17.552040100097656
epoch0: step2500/4680
step 0: accuracy:0.10700000077486038, confidence:0.9994674921035767, loss:9.7343168258667
epoch0: step3000/4680
step 0: accuracy:0.11299999803304672, confidence:0.9999980330467224, loss:15.188220977783203
epoch0: step3500/4680
step 0: accuracy:0.10100000351667404, confidence:0.9999611973762512, loss:12.837677001953125
epoch0: step4000/4680
step 0: accuracy:0.08900000154972076, confidence:0.999364972114563, loss:9.600884437561035
epoch0: step4500/4680
step 0: accuracy:0.1120000034570694, confidence:0.9992884397506714, loss:9.09238338470459
epoch1: step0/4680
step 500: accuracy:0.09600000083446503, confidence:0.9998399615287781, loss:11.099326133728027
epoch1: step500/4680
step 1000: accuracy:0.1120000034570694, confidence:0.999995768070221, loss:15.681558609008789
epoch1: step1000/4680
step 1500: accuracy:0.0860000029206276, confidence:0.9984357953071594, loss:8.193704605102539
epoch1: step1500/4680
step 2000: accuracy:0.10400000214576721, confidence:0.9999716877937317, loss:12.139257431030273
epoch1: step2000/4680
step 2500: accuracy:0.09200000017881393, confidence:0.9999898672103882, loss:13.175000190734863
epoch1: step2500/4680
step 3000: accuracy:0.08100000023841858, confidence:0.9998539090156555, loss:10.99496078491211
epoch1: step3000/4680
step 3500: accuracy:0.11599999666213989, confidence:0.9997474551200867, loss:9.744983673095703
epoch1: step3500/4680
step 4000: accuracy:0.10400000214576721, confidence:0.9997732043266296, loss:11.289097785949707
epoch1: step4000/4680
step 4500: accuracy:0.08900000154972076, confidence:0.9996047616004944, loss:10.333617210388184
epoch1: step4500/4680
step 0: accuracy:0.09000000357627869, confidence:0.9995514154434204, loss:10.01786994934082
epoch2: step0/4680
step 1000: accuracy:0.08900000154972076, confidence:0.9999939203262329, loss:15.136051177978516
epoch2: step500/4680
step 2000: accuracy:0.11500000208616257, confidence:0.9999741911888123, loss:13.134905815124512
epoch2: step1000/4680
step 3000: accuracy:0.09799999743700027, confidence:0.9999111890792847, loss:10.864282608032227
epoch2: step1500/4680
step 4000: accuracy:0.09200000017881393, confidence:0.9999014735221863, loss:11.341797828674316
epoch2: step2000/4680
step 5000: accuracy:0.07800000160932541, confidence:0.9999379515647888, loss:11.438421249389648
epoch2: step2500/4680
step 6000: accuracy:0.08500000089406967, confidence:0.9998211860656738, loss:10.58598804473877
epoch2: step3000/4680
step 7000: accuracy:0.09000000357627869, confidence:0.9999123811721802, loss:10.905163764953613
epoch2: step3500/4680
step 8000: accuracy:0.10199999809265137, confidence:0.9995933771133423, loss:9.941329956054688
epoch2: step4000/4680
step 9000: accuracy:0.09799999743700027, confidence:0.999399721622467, loss:9.466171264648438
epoch2: step4500/4680
step 0: accuracy:0.10700000077486038, confidence:0.9993690848350525, loss:9.276616096496582
epoch3: step0/4680
step 1500: accuracy:0.09799999743700027, confidence:0.997551441192627, loss:7.764490127563477
epoch3: step500/4680
step 3000: accuracy:0.09200000017881393, confidence:0.9999992847442627, loss:16.011159896850586
epoch3: step1000/4680
step 4500: accuracy:0.11500000208616257, confidence:0.999805212020874, loss:10.479722023010254
epoch3: step1500/4680
step 6000: accuracy:0.11100000143051147, confidence:0.999741792678833, loss:9.959087371826172
epoch3: step2000/4680
step 7500: accuracy:0.08900000154972076, confidence:0.9995502233505249, loss:9.676553726196289
epoch3: step2500/4680
step 9000: accuracy:0.11800000071525574, confidence:0.9998365044593811, loss:10.45179271697998
epoch3: step3000/4680
step 10500: accuracy:0.0989999994635582, confidence:0.9994755387306213, loss:9.241643905639648
epoch3: step3500/4680
step 12000: accuracy:0.11299999803304672, confidence:0.998307466506958, loss:8.217283248901367
epoch3: step4000/4680
step 13500: accuracy:0.08799999952316284, confidence:0.9988602995872498, loss:8.647150993347168
epoch3: step4500/4680
step 0: accuracy:0.09799999743700027, confidence:0.9987246990203857, loss:8.42537784576416
epoch4: step0/4680
step 2000: accuracy:0.09300000220537186, confidence:0.9806357026100159, loss:5.82314920425415
epoch4: step500/4680
step 4000: accuracy:0.12099999934434891, confidence:0.9999501705169678, loss:11.22085952758789
epoch4: step1000/4680
step 6000: accuracy:0.10100000351667404, confidence:0.9995601773262024, loss:10.66598129272461
epoch4: step1500/4680
step 8000: accuracy:0.1080000028014183, confidence:0.9990826845169067, loss:8.97684383392334
epoch4: step2000/4680
step 10000: accuracy:0.07400000095367432, confidence:0.9994555711746216, loss:9.744617462158203
epoch4: step2500/4680
step 12000: accuracy:0.09799999743700027, confidence:0.9990493655204773, loss:9.138249397277832
epoch4: step3000/4680
step 14000: accuracy:0.09799999743700027, confidence:0.9996302723884583, loss:9.606341361999512
epoch4: step3500/4680
step 16000: accuracy:0.09399999678134918, confidence:0.997905969619751, loss:8.325552940368652
epoch4: step4000/4680
step 18000: accuracy:0.10000000149011612, confidence:0.998187780380249, loss:8.28501033782959
epoch4: step4500/4680
step 0: accuracy:0.09700000286102295, confidence:0.998410701751709, loss:8.275999069213867
epoch5: step0/4680
step 2500: accuracy:0.0989999994635582, confidence:0.97801274061203, loss:5.947751522064209
epoch5: step500/4680
step 5000: accuracy:0.12600000202655792, confidence:0.9994927048683167, loss:8.937433242797852
epoch5: step1000/4680
step 7500: accuracy:0.0949999988079071, confidence:0.9991021752357483, loss:9.235154151916504
epoch5: step1500/4680
step 10000: accuracy:0.1120000034570694, confidence:0.998506486415863, loss:8.517904281616211
epoch5: step2000/4680
step 12500: accuracy:0.07800000160932541, confidence:0.998779296875, loss:8.894603729248047
epoch5: step2500/4680
step 15000: accuracy:0.09600000083446503, confidence:0.998853325843811, loss:9.101089477539062
epoch5: step3000/4680
step 17500: accuracy:0.10899999737739563, confidence:0.9993566274642944, loss:9.26124095916748
epoch5: step3500/4680
step 20000: accuracy:0.09200000017881393, confidence:0.9951091408729553, loss:7.471714496612549
epoch5: step4000/4680
step 22500: accuracy:0.0860000029206276, confidence:0.9980826377868652, loss:8.271085739135742
epoch5: step4500/4680
step 0: accuracy:0.08500000089406967, confidence:0.9985196590423584, loss:8.421496391296387
epoch6: step0/4680
step 3000: accuracy:0.09799999743700027, confidence:0.9329796433448792, loss:4.8342509269714355
epoch6: step500/4680
step 6000: accuracy:0.11500000208616257, confidence:0.9970267415046692, loss:7.207032680511475
epoch6: step1000/4680
step 9000: accuracy:0.09300000220537186, confidence:0.9907678365707397, loss:6.751547336578369
epoch6: step1500/4680
step 12000: accuracy:0.10199999809265137, confidence:0.9981338977813721, loss:8.443184852600098
epoch6: step2000/4680
step 15000: accuracy:0.07999999821186066, confidence:0.9966461062431335, loss:7.899328231811523
epoch6: step2500/4680
step 18000: accuracy:0.09600000083446503, confidence:0.9976991415023804, loss:8.679849624633789
epoch6: step3000/4680
step 21000: accuracy:0.10199999809265137, confidence:0.9987581372261047, loss:8.965920448303223
epoch6: step3500/4680
step 24000: accuracy:0.10199999809265137, confidence:0.9897275567054749, loss:6.71414852142334
epoch6: step4000/4680
step 27000: accuracy:0.0860000029206276, confidence:0.9975681900978088, loss:8.068119049072266
epoch6: step4500/4680
step 0: accuracy:0.09200000017881393, confidence:0.9979143142700195, loss:8.145058631896973
epoch7: step0/4680
step 3500: accuracy:0.10000000149011612, confidence:0.9016462564468384, loss:4.2845587730407715
epoch7: step500/4680
step 7000: accuracy:0.1080000028014183, confidence:0.9952430129051208, loss:6.9491801261901855
epoch7: step1000/4680
step 10500: accuracy:0.10000000149011612, confidence:0.9698013067245483, loss:5.488366603851318
epoch7: step1500/4680
step 14000: accuracy:0.125, confidence:0.9981120824813843, loss:7.83683967590332
epoch7: step2000/4680
step 17500: accuracy:0.07900000363588333, confidence:0.9943103790283203, loss:7.6887288093566895
epoch7: step2500/4680
step 21000: accuracy:0.10300000011920929, confidence:0.997492253780365, loss:8.424345970153809
epoch7: step3000/4680
step 24500: accuracy:0.0860000029206276, confidence:0.9979049563407898, loss:7.919347286224365
epoch7: step3500/4680
step 28000: accuracy:0.10300000011920929, confidence:0.9845690727233887, loss:6.45866584777832
epoch7: step4000/4680
step 31500: accuracy:0.11599999666213989, confidence:0.9965654015541077, loss:7.631410121917725
epoch7: step4500/4680
step 0: accuracy:0.10700000077486038, confidence:0.9973153471946716, loss:7.770829200744629
epoch8: step0/4680
step 4000: accuracy:0.10100000351667404, confidence:0.8031553030014038, loss:3.3626906871795654
epoch8: step500/4680
step 8000: accuracy:0.10499999672174454, confidence:0.9142778515815735, loss:4.2101545333862305
epoch8: step1000/4680
step 12000: accuracy:0.16200000047683716, confidence:0.8704459071159363, loss:3.915412187576294
epoch8: step1500/4680
step 16000: accuracy:0.09000000357627869, confidence:0.9912972450256348, loss:6.85219144821167
epoch8: step2000/4680
step 20000: accuracy:0.09200000017881393, confidence:0.9914185404777527, loss:7.141997814178467
epoch8: step2500/4680
step 24000: accuracy:0.10499999672174454, confidence:0.9930680990219116, loss:8.306953430175781
epoch8: step3000/4680
step 28000: accuracy:0.09099999815225601, confidence:0.9944995641708374, loss:7.287104606628418
epoch8: step3500/4680
step 32000: accuracy:0.09200000017881393, confidence:0.9731175303459167, loss:6.349348545074463
epoch8: step4000/4680
step 36000: accuracy:0.09099999815225601, confidence:0.9963602423667908, loss:7.830376625061035
epoch8: step4500/4680
step 0: accuracy:0.10499999672174454, confidence:0.9973419308662415, loss:7.84417200088501
epoch9: step0/4680
step 4500: accuracy:0.1469999998807907, confidence:0.7265397906303406, loss:2.8446030616760254
epoch9: step500/4680
step 9000: accuracy:0.15299999713897705, confidence:0.846899151802063, loss:3.5774624347686768
epoch9: step1000/4680
step 13500: accuracy:0.20600000023841858, confidence:0.7163975834846497, loss:2.592039108276367
epoch9: step1500/4680
step 18000: accuracy:0.10300000011920929, confidence:0.9561719298362732, loss:5.830971717834473
epoch9: step2000/4680
step 22500: accuracy:0.0949999988079071, confidence:0.9948521256446838, loss:7.389183521270752
epoch9: step2500/4680
step 27000: accuracy:0.09200000017881393, confidence:0.9947757720947266, loss:8.362268447875977
epoch9: step3000/4680
step 31500: accuracy:0.09700000286102295, confidence:0.9925540685653687, loss:7.151120662689209
epoch9: step3500/4680
step 36000: accuracy:0.11900000274181366, confidence:0.9683603048324585, loss:6.24268102645874
epoch9: step4000/4680
step 40500: accuracy:0.09799999743700027, confidence:0.9917128682136536, loss:7.5205254554748535
epoch9: step4500/4680
step 0: accuracy:0.11400000005960464, confidence:0.9923645853996277, loss:7.5081787109375
epoch10: step0/4680
step 5000: accuracy:0.23499999940395355, confidence:0.7173842191696167, loss:2.4505414962768555
epoch10: step500/4680
step 10000: accuracy:0.21199999749660492, confidence:0.8021762371063232, loss:2.976959228515625
epoch10: step1000/4680
step 15000: accuracy:0.28600001335144043, confidence:0.7208681106567383, loss:2.4086780548095703
epoch10: step1500/4680
step 20000: accuracy:0.15399999916553497, confidence:0.9242923855781555, loss:4.921354293823242
epoch10: step2000/4680
step 25000: accuracy:0.10000000149011612, confidence:0.9821134805679321, loss:6.640814304351807
epoch10: step2500/4680
step 30000: accuracy:0.09600000083446503, confidence:0.9939675331115723, loss:8.3817777633667
epoch10: step3000/4680
step 35000: accuracy:0.10700000077486038, confidence:0.9924741387367249, loss:7.118021488189697
epoch10: step3500/4680
step 40000: accuracy:0.10300000011920929, confidence:0.9239431023597717, loss:5.47304105758667
epoch10: step4000/4680
step 45000: accuracy:0.10100000351667404, confidence:0.9806162714958191, loss:7.13189172744751
epoch10: step4500/4680
step 0: accuracy:0.10599999874830246, confidence:0.9791279435157776, loss:7.196103572845459
epoch11: step0/4680
step 5500: accuracy:0.2980000078678131, confidence:0.7168726921081543, loss:2.261733293533325
epoch11: step500/4680
step 11000: accuracy:0.2669999897480011, confidence:0.75301194190979, loss:2.4805209636688232
epoch11: step1000/4680
step 16500: accuracy:0.35199999809265137, confidence:0.6958109140396118, loss:2.003542184829712
epoch11: step1500/4680
step 22000: accuracy:0.15299999713897705, confidence:0.9243068099021912, loss:5.071469306945801
epoch11: step2000/4680
step 27500: accuracy:0.08500000089406967, confidence:0.9666072130203247, loss:6.548497200012207
epoch11: step2500/4680
step 33000: accuracy:0.09700000286102295, confidence:0.9893700480461121, loss:8.296628952026367
epoch11: step3000/4680
step 38500: accuracy:0.10000000149011612, confidence:0.9851558208465576, loss:7.117379665374756
epoch11: step3500/4680
step 44000: accuracy:0.09000000357627869, confidence:0.9254897832870483, loss:5.737412929534912
epoch11: step4000/4680
step 49500: accuracy:0.11599999666213989, confidence:0.9621782898902893, loss:6.8305768966674805
epoch11: step4500/4680
step 0: accuracy:0.10999999940395355, confidence:0.9677769541740417, loss:6.795456409454346
epoch12: step0/4680
step 6000: accuracy:0.32199999690055847, confidence:0.7155346870422363, loss:2.121098518371582
epoch12: step500/4680
step 12000: accuracy:0.33899998664855957, confidence:0.7411788105964661, loss:2.12518310546875
epoch12: step1000/4680
step 18000: accuracy:0.38499999046325684, confidence:0.704818069934845, loss:2.0019137859344482
epoch12: step1500/4680
step 24000: accuracy:0.14300000667572021, confidence:0.9032543897628784, loss:4.310234069824219
epoch12: step2000/4680
step 30000: accuracy:0.0989999994635582, confidence:0.9830859303474426, loss:7.203498840332031
epoch12: step2500/4680
step 36000: accuracy:0.11400000005960464, confidence:0.9882640838623047, loss:7.6961517333984375
epoch12: step3000/4680
step 42000: accuracy:0.0989999994635582, confidence:0.99090576171875, loss:7.618393421173096
epoch12: step3500/4680
step 48000: accuracy:0.11299999803304672, confidence:0.9347623586654663, loss:5.397666931152344
epoch12: step4000/4680
step 54000: accuracy:0.1120000034570694, confidence:0.9549291729927063, loss:6.528824329376221
epoch12: step4500/4680
step 0: accuracy:0.13600000739097595, confidence:0.9690290093421936, loss:6.629530906677246
epoch13: step0/4680
step 6500: accuracy:0.35100001096725464, confidence:0.7165036201477051, loss:2.0536468029022217
epoch13: step500/4680
step 13000: accuracy:0.367000013589859, confidence:0.7430384755134583, loss:1.9512757062911987
epoch13: step1000/4680
step 19500: accuracy:0.4189999997615814, confidence:0.7308390736579895, loss:1.9823291301727295
epoch13: step1500/4680
step 26000: accuracy:0.15700000524520874, confidence:0.9297273755073547, loss:5.133504867553711
epoch13: step2000/4680
step 32500: accuracy:0.11800000071525574, confidence:0.9682298898696899, loss:7.076326847076416
epoch13: step2500/4680
step 39000: accuracy:0.10199999809265137, confidence:0.9719714522361755, loss:7.081786632537842
epoch13: step3000/4680
step 45500: accuracy:0.07699999958276749, confidence:0.9819953441619873, loss:7.462100505828857
epoch13: step3500/4680
step 52000: accuracy:0.12399999797344208, confidence:0.9200378656387329, loss:5.252776145935059
epoch13: step4000/4680
step 58500: accuracy:0.14100000262260437, confidence:0.9524674415588379, loss:6.293928146362305
epoch13: step4500/4680
step 0: accuracy:0.12099999934434891, confidence:0.9538005590438843, loss:6.442086696624756
epoch14: step0/4680
step 7000: accuracy:0.34599998593330383, confidence:0.7369357347488403, loss:2.1124937534332275
epoch14: step500/4680
step 14000: accuracy:0.3970000147819519, confidence:0.7315595149993896, loss:1.8108015060424805
epoch14: step1000/4680
step 21000: accuracy:0.40799999237060547, confidence:0.7153193950653076, loss:1.981166124343872
epoch14: step1500/4680
step 28000: accuracy:0.1599999964237213, confidence:0.9199181199073792, loss:4.387872695922852
epoch14: step2000/4680
step 35000: accuracy:0.11599999666213989, confidence:0.955680251121521, loss:5.9987921714782715
epoch14: step2500/4680
step 42000: accuracy:0.0949999988079071, confidence:0.9698241353034973, loss:7.383398532867432
epoch14: step3000/4680
step 49000: accuracy:0.09200000017881393, confidence:0.9560574293136597, loss:6.293227672576904
epoch14: step3500/4680
step 56000: accuracy:0.1120000034570694, confidence:0.9247069358825684, loss:5.763726711273193
epoch14: step4000/4680
step 63000: accuracy:0.1469999998807907, confidence:0.9408603310585022, loss:5.703495025634766
epoch14: step4500/4680
step 0: accuracy:0.1340000033378601, confidence:0.9460483193397522, loss:6.367494106292725
epoch15: step0/4680
step 7500: accuracy:0.30000001192092896, confidence:0.7870665192604065, loss:2.520279884338379
epoch15: step500/4680
step 15000: accuracy:0.39800000190734863, confidence:0.7349991202354431, loss:1.7870148420333862
epoch15: step1000/4680
step 22500: accuracy:0.414000004529953, confidence:0.7582368850708008, loss:2.0792315006256104
epoch15: step1500/4680
step 30000: accuracy:0.14300000667572021, confidence:0.9225192070007324, loss:4.971739292144775
epoch15: step2000/4680
step 37500: accuracy:0.12700000405311584, confidence:0.951301634311676, loss:6.228227615356445
epoch15: step2500/4680
step 45000: accuracy:0.10199999809265137, confidence:0.9784408807754517, loss:7.901287078857422
epoch15: step3000/4680
step 52500: accuracy:0.10700000077486038, confidence:0.9797106981277466, loss:7.065706253051758
epoch15: step3500/4680
step 60000: accuracy:0.08799999952316284, confidence:0.9471473097801208, loss:6.152650356292725
epoch15: step4000/4680
step 67500: accuracy:0.1599999964237213, confidence:0.94827800989151, loss:6.158407688140869
epoch15: step4500/4680
step 0: accuracy:0.13600000739097595, confidence:0.9516273736953735, loss:6.219508647918701
epoch16: step0/4680
step 8000: accuracy:0.2919999957084656, confidence:0.7648909091949463, loss:2.5359747409820557
epoch16: step500/4680
step 16000: accuracy:0.37400001287460327, confidence:0.7386678457260132, loss:1.837817668914795
epoch16: step1000/4680
step 24000: accuracy:0.4519999921321869, confidence:0.7270981073379517, loss:1.8278541564941406
epoch16: step1500/4680
step 32000: accuracy:0.16300000250339508, confidence:0.8973613381385803, loss:4.292357444763184
epoch16: step2000/4680
step 40000: accuracy:0.12200000137090683, confidence:0.9613014459609985, loss:6.850373268127441
epoch16: step2500/4680
step 48000: accuracy:0.1080000028014183, confidence:0.9669921398162842, loss:7.126153469085693
epoch16: step3000/4680
step 56000: accuracy:0.09799999743700027, confidence:0.9835710525512695, loss:7.721443176269531
epoch16: step3500/4680
step 64000: accuracy:0.1379999965429306, confidence:0.9169678688049316, loss:5.156290054321289
epoch16: step4000/4680
step 72000: accuracy:0.1679999977350235, confidence:0.9458234906196594, loss:6.011108875274658
epoch16: step4500/4680
step 0: accuracy:0.1379999965429306, confidence:0.9315984845161438, loss:6.074138641357422
epoch17: step0/4680
step 8500: accuracy:0.2939999997615814, confidence:0.7952226400375366, loss:2.732515573501587
epoch17: step500/4680
step 17000: accuracy:0.42399999499320984, confidence:0.7598249912261963, loss:1.7681447267532349
epoch17: step1000/4680
step 25500: accuracy:0.4560000002384186, confidence:0.7459439635276794, loss:1.8597162961959839
epoch17: step1500/4680
step 34000: accuracy:0.2070000022649765, confidence:0.8635421395301819, loss:3.6660513877868652
epoch17: step2000/4680
step 42500: accuracy:0.14499999582767487, confidence:0.9444278478622437, loss:5.776346206665039
epoch17: step2500/4680
step 51000: accuracy:0.11299999803304672, confidence:0.9796584844589233, loss:7.83870267868042
epoch17: step3000/4680
step 59500: accuracy:0.12200000137090683, confidence:0.9333412647247314, loss:6.0105977058410645
epoch17: step3500/4680
step 68000: accuracy:0.12800000607967377, confidence:0.9373396039009094, loss:5.682915210723877
epoch17: step4000/4680
step 76500: accuracy:0.1459999978542328, confidence:0.9412578344345093, loss:5.87114143371582
epoch17: step4500/4680
step 0: accuracy:0.1459999978542328, confidence:0.9449341893196106, loss:5.815880298614502
epoch18: step0/4680
step 9000: accuracy:0.29899999499320984, confidence:0.802679181098938, loss:2.675229072570801
epoch18: step500/4680
step 18000: accuracy:0.44200000166893005, confidence:0.7448064684867859, loss:1.728226661682129
epoch18: step1000/4680
step 27000: accuracy:0.4560000002384186, confidence:0.7391958236694336, loss:1.8446965217590332
epoch18: step1500/4680
step 36000: accuracy:0.20200000703334808, confidence:0.8767871260643005, loss:3.7849948406219482
epoch18: step2000/4680
step 45000: accuracy:0.12600000202655792, confidence:0.9519049525260925, loss:6.317999839782715
epoch18: step2500/4680
step 54000: accuracy:0.1289999932050705, confidence:0.9700294137001038, loss:7.220472812652588
epoch18: step3000/4680
step 63000: accuracy:0.0989999994635582, confidence:0.976865291595459, loss:7.277802467346191
epoch18: step3500/4680
step 72000: accuracy:0.14300000667572021, confidence:0.9219419360160828, loss:5.353400230407715
epoch18: step4000/4680
step 81000: accuracy:0.16899999976158142, confidence:0.9341951012611389, loss:5.562972068786621
epoch18: step4500/4680
step 0: accuracy:0.1899999976158142, confidence:0.9262292385101318, loss:5.634889602661133
epoch19: step0/4680
step 9500: accuracy:0.2840000092983246, confidence:0.8304398059844971, loss:3.042865753173828
epoch19: step500/4680
step 19000: accuracy:0.4729999899864197, confidence:0.7624601721763611, loss:1.6201155185699463
epoch19: step1000/4680
step 28500: accuracy:0.43700000643730164, confidence:0.7467179298400879, loss:1.896709680557251
epoch19: step1500/4680
step 38000: accuracy:0.1720000058412552, confidence:0.8690747022628784, loss:3.7770090103149414
epoch19: step2000/4680
step 47500: accuracy:0.12800000607967377, confidence:0.956617534160614, loss:6.387833118438721
epoch19: step2500/4680
step 57000: accuracy:0.13500000536441803, confidence:0.9489296674728394, loss:6.622925758361816
epoch19: step3000/4680
step 66500: accuracy:0.10300000011920929, confidence:0.9552945494651794, loss:6.8885931968688965
epoch19: step3500/4680
step 76000: accuracy:0.12099999934434891, confidence:0.9268561601638794, loss:5.659653663635254
epoch19: step4000/4680
step 85500: accuracy:0.16500000655651093, confidence:0.9337019920349121, loss:5.569478988647461
epoch19: step4500/4680
step 0: accuracy:0.15399999916553497, confidence:0.9341796636581421, loss:5.532957077026367
epoch20: step0/4680
step 10000: accuracy:0.2849999964237213, confidence:0.8304355144500732, loss:2.9859626293182373
epoch20: step500/4680
step 20000: accuracy:0.4339999854564667, confidence:0.7604430913925171, loss:1.7968884706497192
epoch20: step1000/4680
step 30000: accuracy:0.5070000290870667, confidence:0.7685919404029846, loss:1.6867326498031616
epoch20: step1500/4680
step 40000: accuracy:0.1459999978542328, confidence:0.9343665242195129, loss:5.032983303070068
epoch20: step2000/4680
step 50000: accuracy:0.13500000536441803, confidence:0.9395508170127869, loss:5.7153706550598145
epoch20: step2500/4680
step 60000: accuracy:0.13699999451637268, confidence:0.9705015420913696, loss:7.338960647583008
epoch20: step3000/4680
step 70000: accuracy:0.125, confidence:0.9399625658988953, loss:6.406374454498291
epoch20: step3500/4680
step 80000: accuracy:0.13699999451637268, confidence:0.9038589000701904, loss:4.891687870025635
epoch20: step4000/4680
step 90000: accuracy:0.18799999356269836, confidence:0.922705888748169, loss:5.416487216949463
epoch20: step4500/4680
step 0: accuracy:0.18400000035762787, confidence:0.9203512072563171, loss:5.415194511413574
epoch21: step0/4680
step 10500: accuracy:0.2669999897480011, confidence:0.8593937158584595, loss:3.5687601566314697
epoch21: step500/4680
step 21000: accuracy:0.4269999861717224, confidence:0.7823327779769897, loss:1.9126461744308472
epoch21: step1000/4680
step 31500: accuracy:0.4699999988079071, confidence:0.7732999920845032, loss:1.8126145601272583
epoch21: step1500/4680
step 42000: accuracy:0.16500000655651093, confidence:0.9164313673973083, loss:4.976027488708496
epoch21: step2000/4680
step 52500: accuracy:0.1340000033378601, confidence:0.9456049203872681, loss:6.031671524047852
epoch21: step2500/4680
step 63000: accuracy:0.1420000046491623, confidence:0.9595186114311218, loss:6.931753158569336
epoch21: step3000/4680
step 73500: accuracy:0.10499999672174454, confidence:0.9481815099716187, loss:6.72515344619751
epoch21: step3500/4680
step 84000: accuracy:0.0989999994635582, confidence:0.921989917755127, loss:5.431727409362793
epoch21: step4000/4680
step 94500: accuracy:0.18400000035762787, confidence:0.9328849911689758, loss:5.409107685089111
epoch21: step4500/4680
step 0: accuracy:0.16099999845027924, confidence:0.9238101840019226, loss:5.680978298187256
epoch22: step0/4680
step 11000: accuracy:0.2709999978542328, confidence:0.8543537259101868, loss:3.506126642227173
epoch22: step500/4680
step 22000: accuracy:0.4650000035762787, confidence:0.7877365946769714, loss:1.8569148778915405
epoch22: step1000/4680
step 33000: accuracy:0.44999998807907104, confidence:0.7650049924850464, loss:1.9567089080810547
epoch22: step1500/4680
step 44000: accuracy:0.14100000262260437, confidence:0.9309762120246887, loss:5.152238845825195
epoch22: step2000/4680
step 55000: accuracy:0.1379999965429306, confidence:0.9443358778953552, loss:5.746590614318848
epoch22: step2500/4680
step 66000: accuracy:0.14800000190734863, confidence:0.935846209526062, loss:6.008881568908691
epoch22: step3000/4680
step 77000: accuracy:0.14100000262260437, confidence:0.9334928393363953, loss:6.53047513961792
epoch22: step3500/4680
step 88000: accuracy:0.125, confidence:0.9209780097007751, loss:5.155261039733887
epoch22: step4000/4680
step 99000: accuracy:0.17900000512599945, confidence:0.9187204241752625, loss:5.409540176391602
epoch22: step4500/4680
step 0: accuracy:0.1720000058412552, confidence:0.92290198802948, loss:5.434223651885986
epoch23: step0/4680
step 11500: accuracy:0.25600001215934753, confidence:0.8512429594993591, loss:3.554263114929199
epoch23: step500/4680
step 23000: accuracy:0.4449999928474426, confidence:0.7885518074035645, loss:1.8509585857391357
epoch23: step1000/4680
step 34500: accuracy:0.4950000047683716, confidence:0.7692325711250305, loss:1.8068615198135376
epoch23: step1500/4680
step 46000: accuracy:0.19900000095367432, confidence:0.8923537731170654, loss:4.180780410766602
epoch23: step2000/4680
step 57500: accuracy:0.14900000393390656, confidence:0.9312036037445068, loss:5.533999919891357
epoch23: step2500/4680
step 69000: accuracy:0.16300000250339508, confidence:0.9442209601402283, loss:6.019362926483154
epoch23: step3000/4680
step 80500: accuracy:0.1469999998807907, confidence:0.9507299065589905, loss:6.6601667404174805
epoch23: step3500/4680
step 92000: accuracy:0.15399999916553497, confidence:0.9057055115699768, loss:4.872709274291992
epoch23: step4000/4680
step 103500: accuracy:0.17100000381469727, confidence:0.9204660058021545, loss:5.245713233947754
epoch23: step4500/4680
step 0: accuracy:0.18299999833106995, confidence:0.9241118431091309, loss:5.436444282531738
epoch24: step0/4680
step 12000: accuracy:0.2980000078678131, confidence:0.8468708395957947, loss:3.3140439987182617
epoch24: step500/4680
step 24000: accuracy:0.46799999475479126, confidence:0.7866998314857483, loss:1.7671363353729248
epoch24: step1000/4680
step 36000: accuracy:0.46799999475479126, confidence:0.7840587496757507, loss:1.9753092527389526
epoch24: step1500/4680
step 48000: accuracy:0.16899999976158142, confidence:0.9286573529243469, loss:5.249433994293213
epoch24: step2000/4680
step 60000: accuracy:0.14100000262260437, confidence:0.9416408538818359, loss:6.151087760925293
epoch24: step2500/4680
step 72000: accuracy:0.20600000023841858, confidence:0.918643593788147, loss:5.753570079803467
epoch24: step3000/4680
step 84000: accuracy:0.15800000727176666, confidence:0.9434792399406433, loss:6.888798713684082
epoch24: step3500/4680
step 96000: accuracy:0.12700000405311584, confidence:0.9247479438781738, loss:5.390260219573975
epoch24: step4000/4680
step 108000: accuracy:0.1550000011920929, confidence:0.9321619272232056, loss:5.683353424072266
epoch24: step4500/4680
step 0: accuracy:0.17299999296665192, confidence:0.9216858744621277, loss:5.220656394958496
epoch25: step0/4680
step 12500: accuracy:0.2750000059604645, confidence:0.8696687817573547, loss:3.8536460399627686
epoch25: step500/4680
step 25000: accuracy:0.4259999990463257, confidence:0.7994476556777954, loss:2.002201795578003
epoch25: step1000/4680
step 37500: accuracy:0.4909999966621399, confidence:0.7774592041969299, loss:1.876125454902649
epoch25: step1500/4680
step 50000: accuracy:0.17599999904632568, confidence:0.9041759967803955, loss:4.774521827697754
epoch25: step2000/4680
step 62500: accuracy:0.12999999523162842, confidence:0.936691164970398, loss:5.6678667068481445
epoch25: step2500/4680
step 75000: accuracy:0.16899999976158142, confidence:0.9221764802932739, loss:5.5339674949646
epoch25: step3000/4680
step 87500: accuracy:0.14499999582767487, confidence:0.940282940864563, loss:6.706127166748047
epoch25: step3500/4680
step 100000: accuracy:0.12800000607967377, confidence:0.9381301403045654, loss:5.546876907348633
epoch25: step4000/4680
step 112500: accuracy:0.18299999833106995, confidence:0.9313673377037048, loss:5.469648838043213
epoch25: step4500/4680
step 0: accuracy:0.1599999964237213, confidence:0.9160330891609192, loss:5.417829990386963
epoch26: step0/4680
step 13000: accuracy:0.29100000858306885, confidence:0.8403708934783936, loss:3.2466824054718018
epoch26: step500/4680
step 26000: accuracy:0.460999995470047, confidence:0.7822707295417786, loss:1.7154229879379272
epoch26: step1000/4680
step 39000: accuracy:0.43700000643730164, confidence:0.7795530557632446, loss:2.055229902267456
epoch26: step1500/4680
step 52000: accuracy:0.17299999296665192, confidence:0.9158143401145935, loss:4.672292709350586
epoch26: step2000/4680
step 65000: accuracy:0.13699999451637268, confidence:0.9258120656013489, loss:5.478913307189941
epoch26: step2500/4680
step 78000: accuracy:0.1899999976158142, confidence:0.9305725693702698, loss:5.837445259094238
epoch26: step3000/4680
step 91000: accuracy:0.14100000262260437, confidence:0.9433112740516663, loss:6.463897228240967
epoch26: step3500/4680
step 104000: accuracy:0.12800000607967377, confidence:0.9269443154335022, loss:5.284294128417969
epoch26: step4000/4680
step 117000: accuracy:0.16500000655651093, confidence:0.9308116436004639, loss:5.498536109924316
epoch26: step4500/4680
step 0: accuracy:0.16300000250339508, confidence:0.924702525138855, loss:5.57325553894043
epoch27: step0/4680
step 13500: accuracy:0.26499998569488525, confidence:0.8488694429397583, loss:3.5609614849090576
epoch27: step500/4680
step 27000: accuracy:0.46700000762939453, confidence:0.7839443683624268, loss:1.8517308235168457
epoch27: step1000/4680
step 40500: accuracy:0.45100000500679016, confidence:0.7831973433494568, loss:1.9732788801193237
epoch27: step1500/4680
step 54000: accuracy:0.15399999916553497, confidence:0.9171359539031982, loss:5.061875820159912
epoch27: step2000/4680
step 67500: accuracy:0.16099999845027924, confidence:0.9323835372924805, loss:5.580703258514404
epoch27: step2500/4680
step 81000: accuracy:0.1809999942779541, confidence:0.9251514077186584, loss:5.626552581787109
epoch27: step3000/4680
step 94500: accuracy:0.12399999797344208, confidence:0.9404943585395813, loss:6.817040920257568
epoch27: step3500/4680
step 108000: accuracy:0.1679999977350235, confidence:0.9080982208251953, loss:4.61438512802124
epoch27: step4000/4680
step 121500: accuracy:0.1599999964237213, confidence:0.919759213924408, loss:5.344180583953857
epoch27: step4500/4680
step 0: accuracy:0.17900000512599945, confidence:0.9129317402839661, loss:5.147977352142334
epoch28: step0/4680
step 14000: accuracy:0.2720000147819519, confidence:0.844924807548523, loss:3.385356903076172
epoch28: step500/4680
step 28000: accuracy:0.4320000112056732, confidence:0.7766034007072449, loss:1.8400241136550903
epoch28: step1000/4680
step 42000: accuracy:0.48100000619888306, confidence:0.7872944474220276, loss:1.8925867080688477
epoch28: step1500/4680
step 56000: accuracy:0.1770000010728836, confidence:0.9066517949104309, loss:4.846299648284912
epoch28: step2000/4680
step 70000: accuracy:0.1509999930858612, confidence:0.9373820424079895, loss:5.8810224533081055
epoch28: step2500/4680
step 84000: accuracy:0.2150000035762787, confidence:0.9092842936515808, loss:5.03009557723999
epoch28: step3000/4680
step 98000: accuracy:0.15199999511241913, confidence:0.942933976650238, loss:6.68621301651001
epoch28: step3500/4680
step 112000: accuracy:0.15700000524520874, confidence:0.9150293469429016, loss:4.897517204284668
epoch28: step4000/4680
step 126000: accuracy:0.15800000727176666, confidence:0.9137479066848755, loss:5.526549339294434
epoch28: step4500/4680
step 0: accuracy:0.17900000512599945, confidence:0.9075105786323547, loss:5.28866720199585
epoch29: step0/4680
step 14500: accuracy:0.27900001406669617, confidence:0.8502868413925171, loss:3.6483402252197266
epoch29: step500/4680
step 29000: accuracy:0.4690000116825104, confidence:0.7788592576980591, loss:1.8514857292175293
epoch29: step1000/4680
step 43500: accuracy:0.4790000021457672, confidence:0.7862041592597961, loss:1.8937876224517822
epoch29: step1500/4680
step 58000: accuracy:0.18299999833106995, confidence:0.88543701171875, loss:4.115093231201172
epoch29: step2000/4680
step 72500: accuracy:0.15600000321865082, confidence:0.9200724959373474, loss:5.496223449707031
epoch29: step2500/4680
step 87000: accuracy:0.1889999955892563, confidence:0.9141659736633301, loss:5.202126502990723
epoch29: step3000/4680
step 101500: accuracy:0.13500000536441803, confidence:0.9445620179176331, loss:6.592380523681641
epoch29: step3500/4680
step 116000: accuracy:0.13300000131130219, confidence:0.9104788899421692, loss:4.882174015045166
epoch29: step4000/4680
step 130500: accuracy:0.17399999499320984, confidence:0.9149143099784851, loss:5.42897367477417
epoch29: step4500/4680
2018-06-15 20:06:17.233387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 20:06:17.233588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
step 0: accuracy:0.09200000017881393, confidence:0.8276244401931763, loss:6.958163738250732
epoch0: step0/4680
step 0: accuracy:0.10000000149011612, confidence:1.0, loss:42.351600646972656
epoch0: step500/4680
step 0: accuracy:0.10599999874830246, confidence:1.0, loss:26.322025299072266
epoch0: step1000/4680
step 0: accuracy:0.09000000357627869, confidence:0.9998111724853516, loss:10.333373069763184
epoch0: step1500/4680
step 0: accuracy:0.10000000149011612, confidence:0.9999999403953552, loss:18.247167587280273
epoch0: step2000/4680
step 0: accuracy:0.09099999815225601, confidence:0.9982050657272339, loss:8.231184959411621
epoch0: step2500/4680
step 0: accuracy:0.10700000077486038, confidence:0.9999984502792358, loss:15.571494102478027
epoch0: step3000/4680
step 0: accuracy:0.07900000363588333, confidence:0.999869704246521, loss:11.167543411254883
epoch0: step3500/4680
step 0: accuracy:0.09700000286102295, confidence:0.9999980926513672, loss:15.284722328186035
epoch0: step4000/4680
step 0: accuracy:0.11999999731779099, confidence:0.9999995827674866, loss:17.55998420715332
epoch0: step4500/4680
step 0: accuracy:0.11500000208616257, confidence:0.9999980926513672, loss:15.805129051208496
epoch1: step0/4680
step 500: accuracy:0.08399999886751175, confidence:0.9999566078186035, loss:11.696503639221191
epoch1: step500/4680
step 1000: accuracy:0.11100000143051147, confidence:0.999997079372406, loss:15.209511756896973
epoch1: step1000/4680
step 1500: accuracy:0.11400000005960464, confidence:0.999665379524231, loss:10.606963157653809
epoch1: step1500/4680
step 2000: accuracy:0.10400000214576721, confidence:0.9999993443489075, loss:16.07036018371582
epoch1: step2000/4680
step 2500: accuracy:0.08299999684095383, confidence:0.9997230172157288, loss:10.33646297454834
epoch1: step2500/4680
step 3000: accuracy:0.08100000023841858, confidence:0.9992193579673767, loss:8.969378471374512
epoch1: step3000/4680
step 3500: accuracy:0.11400000005960464, confidence:0.9999780058860779, loss:12.542787551879883
epoch1: step3500/4680
step 4000: accuracy:0.09200000017881393, confidence:0.9999171495437622, loss:12.102530479431152
epoch1: step4000/4680
step 4500: accuracy:0.1289999932050705, confidence:0.9999991655349731, loss:16.1603946685791
epoch1: step4500/4680
step 0: accuracy:0.11500000208616257, confidence:0.9999952912330627, loss:14.376683235168457
epoch2: step0/4680
step 1000: accuracy:0.11100000143051147, confidence:0.9998490214347839, loss:10.752880096435547
epoch2: step500/4680
step 2000: accuracy:0.10400000214576721, confidence:0.999982476234436, loss:13.20167064666748
epoch2: step1000/4680
step 3000: accuracy:0.10300000011920929, confidence:0.9998696446418762, loss:10.73550033569336
epoch2: step1500/4680
step 4000: accuracy:0.09200000017881393, confidence:0.9999905824661255, loss:14.0027494430542
epoch2: step2000/4680
step 5000: accuracy:0.0860000029206276, confidence:0.9998842477798462, loss:10.795998573303223
epoch2: step2500/4680
step 6000: accuracy:0.08500000089406967, confidence:0.9993710517883301, loss:9.88967227935791
epoch2: step3000/4680
step 7000: accuracy:0.1080000028014183, confidence:0.9994701147079468, loss:9.244976043701172
epoch2: step3500/4680
step 8000: accuracy:0.10499999672174454, confidence:0.999761700630188, loss:10.34485912322998
epoch2: step4000/4680
step 9000: accuracy:0.11400000005960464, confidence:0.9999761581420898, loss:12.431681632995605
epoch2: step4500/4680
step 0: accuracy:0.10599999874830246, confidence:0.9999346733093262, loss:11.439309120178223
epoch3: step0/4680
step 1500: accuracy:0.10499999672174454, confidence:0.9992334842681885, loss:9.369185447692871
epoch3: step500/4680
step 3000: accuracy:0.09700000286102295, confidence:0.9999904632568359, loss:13.591822624206543
epoch3: step1000/4680
step 4500: accuracy:0.10700000077486038, confidence:0.9998322129249573, loss:11.279417991638184
epoch3: step1500/4680
step 6000: accuracy:0.11100000143051147, confidence:0.9998130202293396, loss:10.717710494995117
epoch3: step2000/4680
step 7500: accuracy:0.1120000034570694, confidence:0.9997010231018066, loss:9.763031005859375
epoch3: step2500/4680
step 9000: accuracy:0.11800000071525574, confidence:0.999594509601593, loss:9.408594131469727
epoch3: step3000/4680
step 10500: accuracy:0.11500000208616257, confidence:0.9995177984237671, loss:9.649401664733887
epoch3: step3500/4680
step 12000: accuracy:0.08399999886751175, confidence:0.9995113611221313, loss:9.62111759185791
epoch3: step4000/4680
step 13500: accuracy:0.10300000011920929, confidence:0.999821126461029, loss:10.263718605041504
epoch3: step4500/4680
step 0: accuracy:0.11400000005960464, confidence:0.9998167753219604, loss:10.030656814575195
epoch4: step0/4680
step 2000: accuracy:0.10400000214576721, confidence:0.9995416402816772, loss:10.306180953979492
epoch4: step500/4680
step 4000: accuracy:0.09600000083446503, confidence:0.9998826384544373, loss:10.945047378540039
epoch4: step1000/4680
step 6000: accuracy:0.10400000214576721, confidence:0.9998419880867004, loss:11.936891555786133
epoch4: step1500/4680
step 8000: accuracy:0.1080000028014183, confidence:0.9992714524269104, loss:9.373634338378906
epoch4: step2000/4680
step 10000: accuracy:0.11599999666213989, confidence:0.9996955990791321, loss:10.346807479858398
epoch4: step2500/4680
step 12000: accuracy:0.09799999743700027, confidence:0.9983993768692017, loss:8.747000694274902
epoch4: step3000/4680
step 14000: accuracy:0.10999999940395355, confidence:0.9987131953239441, loss:8.561393737792969
epoch4: step3500/4680
step 16000: accuracy:0.09200000017881393, confidence:0.9977784156799316, loss:8.39643383026123
epoch4: step4000/4680
step 18000: accuracy:0.12099999934434891, confidence:0.9998539686203003, loss:10.431365966796875
epoch4: step4500/4680
step 0: accuracy:0.1289999932050705, confidence:0.9998445510864258, loss:10.237414360046387
epoch5: step0/4680
step 2500: accuracy:0.10000000149011612, confidence:0.9984199404716492, loss:8.604605674743652
epoch5: step500/4680
step 5000: accuracy:0.12099999934434891, confidence:0.999783992767334, loss:10.359565734863281
epoch5: step1000/4680
step 7500: accuracy:0.09600000083446503, confidence:0.9986684322357178, loss:9.203853607177734
epoch5: step1500/4680
step 10000: accuracy:0.1120000034570694, confidence:0.9995843768119812, loss:9.808560371398926
epoch5: step2000/4680
step 12500: accuracy:0.09399999678134918, confidence:0.9996743202209473, loss:10.03311824798584
epoch5: step2500/4680
step 15000: accuracy:0.09600000083446503, confidence:0.9990150332450867, loss:9.303269386291504
epoch5: step3000/4680
step 17500: accuracy:0.09799999743700027, confidence:0.9980100989341736, loss:8.645092964172363
epoch5: step3500/4680
step 20000: accuracy:0.09300000220537186, confidence:0.9985453486442566, loss:8.76913833618164
epoch5: step4000/4680
step 22500: accuracy:0.1120000034570694, confidence:0.9997556805610657, loss:9.954837799072266
epoch5: step4500/4680
step 0: accuracy:0.09600000083446503, confidence:0.9997828602790833, loss:10.068926811218262
epoch6: step0/4680
step 3000: accuracy:0.09700000286102295, confidence:0.996006429195404, loss:7.992340564727783
epoch6: step500/4680
step 6000: accuracy:0.11299999803304672, confidence:0.9989562630653381, loss:8.780633926391602
epoch6: step1000/4680
step 9000: accuracy:0.11299999803304672, confidence:0.9996088743209839, loss:10.17664623260498
epoch6: step1500/4680
step 12000: accuracy:0.10199999809265137, confidence:0.9990115165710449, loss:9.329208374023438
epoch6: step2000/4680
step 15000: accuracy:0.10100000351667404, confidence:0.9994088411331177, loss:9.525903701782227
epoch6: step2500/4680
step 18000: accuracy:0.09600000083446503, confidence:0.9984299540519714, loss:8.688140869140625
epoch6: step3000/4680
step 21000: accuracy:0.10599999874830246, confidence:0.9978330135345459, loss:8.330259323120117
epoch6: step3500/4680
step 24000: accuracy:0.0989999994635582, confidence:0.9984878897666931, loss:8.865775108337402
epoch6: step4000/4680
step 27000: accuracy:0.12700000405311584, confidence:0.9997567534446716, loss:9.674470901489258
epoch6: step4500/4680
step 0: accuracy:0.10300000011920929, confidence:0.9997527003288269, loss:9.77914047241211
epoch7: step0/4680
step 3500: accuracy:0.07900000363588333, confidence:0.985604465007782, loss:6.7255706787109375
epoch7: step500/4680
step 7000: accuracy:0.11100000143051147, confidence:0.9995849132537842, loss:9.846662521362305
epoch7: step1000/4680
step 10500: accuracy:0.1120000034570694, confidence:0.9984082579612732, loss:8.820714950561523
epoch7: step1500/4680
step 14000: accuracy:0.125, confidence:0.9987382292747498, loss:8.526016235351562
epoch7: step2000/4680
step 17500: accuracy:0.1080000028014183, confidence:0.9992814660072327, loss:9.492953300476074
epoch7: step2500/4680
step 21000: accuracy:0.10300000011920929, confidence:0.9970052242279053, loss:8.255114555358887
epoch7: step3000/4680
step 24500: accuracy:0.11100000143051147, confidence:0.9974809288978577, loss:8.187304496765137
epoch7: step3500/4680
step 28000: accuracy:0.09399999678134918, confidence:0.9967755675315857, loss:8.407844543457031
epoch7: step4000/4680
step 31500: accuracy:0.11599999666213989, confidence:0.9998818635940552, loss:11.197613716125488
epoch7: step4500/4680
step 0: accuracy:0.13600000739097595, confidence:0.999919056892395, loss:11.363201141357422
epoch8: step0/4680
step 4000: accuracy:0.09200000017881393, confidence:0.9870003461837769, loss:6.994466781616211
epoch8: step500/4680
step 8000: accuracy:0.08900000154972076, confidence:0.9998552799224854, loss:11.109160423278809
epoch8: step1000/4680
step 12000: accuracy:0.09600000083446503, confidence:0.9991248250007629, loss:9.722901344299316
epoch8: step1500/4680
step 16000: accuracy:0.09000000357627869, confidence:0.9984360933303833, loss:8.808243751525879
epoch8: step2000/4680
step 20000: accuracy:0.1120000034570694, confidence:0.999077558517456, loss:8.986875534057617
epoch8: step2500/4680
step 24000: accuracy:0.10499999672174454, confidence:0.995792806148529, loss:8.009058952331543
epoch8: step3000/4680
step 28000: accuracy:0.09000000357627869, confidence:0.9960408806800842, loss:8.210294723510742
epoch8: step3500/4680
step 32000: accuracy:0.09600000083446503, confidence:0.9957079291343689, loss:7.876091003417969
epoch8: step4000/4680
step 36000: accuracy:0.09600000083446503, confidence:0.9997336268424988, loss:10.058354377746582
epoch8: step4500/4680
step 0: accuracy:0.11800000071525574, confidence:0.9997445940971375, loss:9.742061614990234
epoch9: step0/4680
step 4500: accuracy:0.10499999672174454, confidence:0.9861658215522766, loss:6.725829124450684
epoch9: step500/4680
step 9000: accuracy:0.10700000077486038, confidence:0.9992141723632812, loss:9.529804229736328
epoch9: step1000/4680
step 13500: accuracy:0.10199999809265137, confidence:0.9991263151168823, loss:9.951088905334473
epoch9: step1500/4680
step 18000: accuracy:0.09000000357627869, confidence:0.9970569014549255, loss:7.985879421234131
epoch9: step2000/4680
step 22500: accuracy:0.09799999743700027, confidence:0.9989768266677856, loss:9.083808898925781
epoch9: step2500/4680
step 27000: accuracy:0.09200000017881393, confidence:0.9958517551422119, loss:7.930260181427002
epoch9: step3000/4680
step 31500: accuracy:0.10499999672174454, confidence:0.9914171099662781, loss:7.526764869689941
epoch9: step3500/4680
step 36000: accuracy:0.09000000357627869, confidence:0.9934357404708862, loss:7.974180698394775
epoch9: step4000/4680
step 40500: accuracy:0.09700000286102295, confidence:0.9997842311859131, loss:10.340093612670898
epoch9: step4500/4680
step 0: accuracy:0.09600000083446503, confidence:0.9997915625572205, loss:10.300649642944336
epoch10: step0/4680
step 5000: accuracy:0.10400000214576721, confidence:0.9829709529876709, loss:6.513418197631836
epoch10: step500/4680
step 10000: accuracy:0.1120000034570694, confidence:0.9981712102890015, loss:10.061506271362305
epoch10: step1000/4680
step 15000: accuracy:0.10100000351667404, confidence:0.9988248944282532, loss:9.730916023254395
epoch10: step1500/4680
step 20000: accuracy:0.09300000220537186, confidence:0.9985700845718384, loss:8.826798439025879
epoch10: step2000/4680
step 25000: accuracy:0.10999999940395355, confidence:0.9983901977539062, loss:8.581101417541504
epoch10: step2500/4680
step 30000: accuracy:0.09600000083446503, confidence:0.996118426322937, loss:7.889344692230225
epoch10: step3000/4680
step 35000: accuracy:0.09799999743700027, confidence:0.9946187138557434, loss:7.9768385887146
epoch10: step3500/4680
step 40000: accuracy:0.0860000029206276, confidence:0.9912727475166321, loss:7.660211563110352
epoch10: step4000/4680
step 45000: accuracy:0.1120000034570694, confidence:0.9998669028282166, loss:11.017247200012207
epoch10: step4500/4680
step 0: accuracy:0.09099999815225601, confidence:0.9999765753746033, loss:13.511809349060059
epoch11: step0/4680
step 5500: accuracy:0.09399999678134918, confidence:0.9832342863082886, loss:6.426917552947998
epoch11: step500/4680
step 11000: accuracy:0.1080000028014183, confidence:0.999244213104248, loss:9.479852676391602
epoch11: step1000/4680
step 16500: accuracy:0.1080000028014183, confidence:0.9991719722747803, loss:9.913626670837402
epoch11: step1500/4680
step 22000: accuracy:0.09700000286102295, confidence:0.9899830222129822, loss:6.622193336486816
epoch11: step2000/4680
step 27500: accuracy:0.09099999815225601, confidence:0.9877435564994812, loss:7.377617359161377
epoch11: step2500/4680
step 33000: accuracy:0.09700000286102295, confidence:0.9849583506584167, loss:7.3439741134643555
epoch11: step3000/4680
step 38500: accuracy:0.09300000220537186, confidence:0.9777473211288452, loss:7.300824165344238
epoch11: step3500/4680
step 44000: accuracy:0.10300000011920929, confidence:0.9831377267837524, loss:6.71497917175293
epoch11: step4000/4680
step 49500: accuracy:0.11500000208616257, confidence:0.9996660947799683, loss:9.584112167358398
epoch11: step4500/4680
step 0: accuracy:0.11299999803304672, confidence:0.9996998310089111, loss:9.5836820602417
epoch12: step0/4680
step 6000: accuracy:0.10499999672174454, confidence:0.9837761521339417, loss:6.212564945220947
epoch12: step500/4680
step 12000: accuracy:0.10100000351667404, confidence:0.9990093111991882, loss:9.050821304321289
epoch12: step1000/4680
step 18000: accuracy:0.09099999815225601, confidence:0.9980718493461609, loss:9.308266639709473
epoch12: step1500/4680
step 24000: accuracy:0.08799999952316284, confidence:0.9938005208969116, loss:8.154056549072266
epoch12: step2000/4680
step 30000: accuracy:0.10700000077486038, confidence:0.9856483340263367, loss:7.202672004699707
epoch12: step2500/4680
step 36000: accuracy:0.11400000005960464, confidence:0.967894434928894, loss:6.9750261306762695
epoch12: step3000/4680
step 42000: accuracy:0.08900000154972076, confidence:0.9651596546173096, loss:7.064846038818359
epoch12: step3500/4680
step 48000: accuracy:0.0949999988079071, confidence:0.9797683358192444, loss:6.618375778198242
epoch12: step4000/4680
step 54000: accuracy:0.10400000214576721, confidence:0.9994905591011047, loss:8.881627082824707
epoch12: step4500/4680
step 0: accuracy:0.1080000028014183, confidence:0.9996159076690674, loss:9.115852355957031
epoch13: step0/4680
step 6500: accuracy:0.10300000011920929, confidence:0.9821174144744873, loss:6.43126916885376
epoch13: step500/4680
step 13000: accuracy:0.09799999743700027, confidence:0.9980100393295288, loss:8.052041053771973
epoch13: step1000/4680
step 19500: accuracy:0.11100000143051147, confidence:0.9898923635482788, loss:7.048539638519287
epoch13: step1500/4680
step 26000: accuracy:0.10300000011920929, confidence:0.9700284004211426, loss:5.9895453453063965
epoch13: step2000/4680
step 32500: accuracy:0.1080000028014183, confidence:0.9572350382804871, loss:6.406289577484131
epoch13: step2500/4680
step 39000: accuracy:0.09399999678134918, confidence:0.9587816596031189, loss:6.7042694091796875
epoch13: step3000/4680
step 45500: accuracy:0.11500000208616257, confidence:0.9253839254379272, loss:6.535579204559326
epoch13: step3500/4680
step 52000: accuracy:0.07900000363588333, confidence:0.9454104900360107, loss:6.10358190536499
epoch13: step4000/4680
step 58500: accuracy:0.10700000077486038, confidence:0.9996069669723511, loss:9.011592864990234
epoch13: step4500/4680
step 0: accuracy:0.1120000034570694, confidence:0.9996584057807922, loss:8.9718656539917
epoch14: step0/4680
step 7000: accuracy:0.10100000351667404, confidence:0.9818201065063477, loss:6.372943878173828
epoch14: step500/4680
step 14000: accuracy:0.09300000220537186, confidence:0.9968980550765991, loss:7.78268575668335
epoch14: step1000/4680
step 21000: accuracy:0.09700000286102295, confidence:0.8990358710289001, loss:4.667585849761963
epoch14: step1500/4680
step 28000: accuracy:0.164000004529953, confidence:0.8796816468238831, loss:4.7499589920043945
epoch14: step2000/4680
step 35000: accuracy:0.17900000512599945, confidence:0.896484375, loss:5.72119665145874
epoch14: step2500/4680
step 42000: accuracy:0.09000000357627869, confidence:0.8721083998680115, loss:5.072462558746338
epoch14: step3000/4680
step 49000: accuracy:0.13099999725818634, confidence:0.8787073493003845, loss:5.7516279220581055
epoch14: step3500/4680
step 56000: accuracy:0.09799999743700027, confidence:0.8919428586959839, loss:5.247941970825195
epoch14: step4000/4680
step 63000: accuracy:0.1120000034570694, confidence:0.9992969036102295, loss:8.552526473999023
epoch14: step4500/4680
step 0: accuracy:0.10000000149011612, confidence:0.9994724988937378, loss:8.872397422790527
epoch15: step0/4680
step 7500: accuracy:0.09000000357627869, confidence:0.9927868247032166, loss:7.126467227935791
epoch15: step500/4680
step 15000: accuracy:0.08699999749660492, confidence:0.9926521182060242, loss:6.562898635864258
epoch15: step1000/4680
step 22500: accuracy:0.13300000131130219, confidence:0.7628873586654663, loss:3.571974515914917
epoch15: step1500/4680
step 30000: accuracy:0.1899999976158142, confidence:0.8579056262969971, loss:4.175196647644043
epoch15: step2000/4680
step 37500: accuracy:0.23100000619888306, confidence:0.8697260618209839, loss:4.976998329162598
epoch15: step2500/4680
step 45000: accuracy:0.13300000131130219, confidence:0.8544531464576721, loss:4.846471786499023
epoch15: step3000/4680
step 52500: accuracy:0.11599999666213989, confidence:0.8349423408508301, loss:5.4218974113464355
epoch15: step3500/4680
step 60000: accuracy:0.14399999380111694, confidence:0.833660364151001, loss:4.702984809875488
epoch15: step4000/4680
step 67500: accuracy:0.10199999809265137, confidence:0.9951038360595703, loss:7.155919075012207
epoch15: step4500/4680
step 0: accuracy:0.10700000077486038, confidence:0.9966590404510498, loss:7.265304088592529
epoch16: step0/4680
step 8000: accuracy:0.10199999809265137, confidence:0.9675545692443848, loss:5.81387996673584
epoch16: step500/4680
step 16000: accuracy:0.09099999815225601, confidence:0.9982241988182068, loss:7.954090595245361
epoch16: step1000/4680
step 24000: accuracy:0.1940000057220459, confidence:0.6837736964225769, loss:3.027935266494751
epoch16: step1500/4680
step 32000: accuracy:0.2160000056028366, confidence:0.845893144607544, loss:3.781586170196533
epoch16: step2000/4680
step 40000: accuracy:0.2720000147819519, confidence:0.8672484159469604, loss:4.523799419403076
epoch16: step2500/4680
step 48000: accuracy:0.16099999845027924, confidence:0.8138825297355652, loss:4.42563009262085
epoch16: step3000/4680
step 56000: accuracy:0.13699999451637268, confidence:0.8540432453155518, loss:5.478155136108398
epoch16: step3500/4680
step 64000: accuracy:0.1770000010728836, confidence:0.8223250508308411, loss:4.62653923034668
epoch16: step4000/4680
step 72000: accuracy:0.11599999666213989, confidence:0.983581006526947, loss:6.92013692855835
epoch16: step4500/4680
step 0: accuracy:0.1289999932050705, confidence:0.9920129179954529, loss:7.420743465423584
epoch17: step0/4680
step 8500: accuracy:0.09799999743700027, confidence:0.9236133694648743, loss:4.876789569854736
epoch17: step500/4680
step 17000: accuracy:0.09799999743700027, confidence:0.9969289302825928, loss:7.608328819274902
epoch17: step1000/4680
step 25500: accuracy:0.20600000023841858, confidence:0.6806544661521912, loss:3.098174810409546
epoch17: step1500/4680
step 34000: accuracy:0.2669999897480011, confidence:0.8487268090248108, loss:3.6906728744506836
epoch17: step2000/4680
step 42500: accuracy:0.28700000047683716, confidence:0.8508744239807129, loss:4.0177001953125
epoch17: step2500/4680
step 51000: accuracy:0.2290000021457672, confidence:0.8224196434020996, loss:4.402261257171631
epoch17: step3000/4680
step 59500: accuracy:0.17399999499320984, confidence:0.851977527141571, loss:5.504326820373535
epoch17: step3500/4680
step 68000: accuracy:0.20900000631809235, confidence:0.8244746923446655, loss:4.938693523406982
epoch17: step4000/4680
step 76500: accuracy:0.11800000071525574, confidence:0.9938930869102478, loss:7.424935340881348
epoch17: step4500/4680
step 0: accuracy:0.10999999940395355, confidence:0.9944730997085571, loss:7.464222431182861
epoch18: step0/4680
step 9000: accuracy:0.11599999666213989, confidence:0.9157679677009583, loss:4.851824760437012
epoch18: step500/4680
step 18000: accuracy:0.10000000149011612, confidence:0.9635144472122192, loss:5.990510940551758
epoch18: step1000/4680
step 27000: accuracy:0.3190000057220459, confidence:0.6382037997245789, loss:2.530866861343384
epoch18: step1500/4680
step 36000: accuracy:0.2630000114440918, confidence:0.8484219908714294, loss:3.5674118995666504
epoch18: step2000/4680
step 45000: accuracy:0.2680000066757202, confidence:0.8330655097961426, loss:3.6903910636901855
epoch18: step2500/4680
step 54000: accuracy:0.20399999618530273, confidence:0.8417869806289673, loss:4.5282301902771
epoch18: step3000/4680
step 63000: accuracy:0.17900000512599945, confidence:0.8302569389343262, loss:5.240696907043457
epoch18: step3500/4680
step 72000: accuracy:0.20499999821186066, confidence:0.8237832188606262, loss:5.121968746185303
epoch18: step4000/4680
step 81000: accuracy:0.10000000149011612, confidence:0.974457859992981, loss:6.255068302154541
epoch18: step4500/4680
step 0: accuracy:0.13199999928474426, confidence:0.9783793091773987, loss:6.170939922332764
epoch19: step0/4680
step 9500: accuracy:0.10700000077486038, confidence:0.9036538600921631, loss:4.791801929473877
epoch19: step500/4680
step 19000: accuracy:0.09099999815225601, confidence:0.96494060754776, loss:5.406703948974609
epoch19: step1000/4680
step 28500: accuracy:0.4569999873638153, confidence:0.6028308272361755, loss:2.0891048908233643
epoch19: step1500/4680
step 38000: accuracy:0.23000000417232513, confidence:0.8291183114051819, loss:3.806291341781616
epoch19: step2000/4680
step 47500: accuracy:0.26600000262260437, confidence:0.8616665005683899, loss:3.886767625808716
epoch19: step2500/4680
step 57000: accuracy:0.2639999985694885, confidence:0.8268192410469055, loss:4.085559368133545
epoch19: step3000/4680
step 66500: accuracy:0.20800000429153442, confidence:0.842629075050354, loss:5.059029579162598
epoch19: step3500/4680
step 76000: accuracy:0.27000001072883606, confidence:0.8163193464279175, loss:4.8232855796813965
epoch19: step4000/4680
step 85500: accuracy:0.1080000028014183, confidence:0.9809849262237549, loss:7.979372501373291
epoch19: step4500/4680
step 0: accuracy:0.12300000339746475, confidence:0.983481764793396, loss:7.723494052886963
epoch20: step0/4680
step 10000: accuracy:0.11699999868869781, confidence:0.9203195571899414, loss:4.59248161315918
epoch20: step500/4680
step 20000: accuracy:0.11500000208616257, confidence:0.94014972448349, loss:4.756991386413574
epoch20: step1000/4680
step 30000: accuracy:0.4869999885559082, confidence:0.6276342868804932, loss:1.8976507186889648
epoch20: step1500/4680
step 40000: accuracy:0.29100000858306885, confidence:0.8031051158905029, loss:3.2025821208953857
epoch20: step2000/4680
step 50000: accuracy:0.28200000524520874, confidence:0.8460218906402588, loss:3.9710216522216797
epoch20: step2500/4680
step 60000: accuracy:0.3199999928474426, confidence:0.8404451608657837, loss:3.7422854900360107
epoch20: step3000/4680
step 70000: accuracy:0.2280000001192093, confidence:0.8610262274742126, loss:5.087235450744629
epoch20: step3500/4680
step 80000: accuracy:0.24500000476837158, confidence:0.8238030672073364, loss:5.023304462432861
epoch20: step4000/4680
step 90000: accuracy:0.1120000034570694, confidence:0.9812638163566589, loss:8.056432723999023
epoch20: step4500/4680
step 0: accuracy:0.1080000028014183, confidence:0.985689640045166, loss:7.936647415161133
epoch21: step0/4680
step 10500: accuracy:0.10700000077486038, confidence:0.9397692680358887, loss:5.300676345825195
epoch21: step500/4680
step 21000: accuracy:0.11900000274181366, confidence:0.9452664256095886, loss:4.824979782104492
epoch21: step1000/4680
step 31500: accuracy:0.5149999856948853, confidence:0.6216853260993958, loss:1.8695098161697388
epoch21: step1500/4680
step 42000: accuracy:0.3009999990463257, confidence:0.7924658060073853, loss:3.2123398780822754
epoch21: step2000/4680
step 52500: accuracy:0.2800000011920929, confidence:0.8383917212486267, loss:3.7011735439300537
epoch21: step2500/4680
step 63000: accuracy:0.35600000619888306, confidence:0.8174840211868286, loss:3.6248667240142822
epoch21: step3000/4680
step 73500: accuracy:0.2409999966621399, confidence:0.8459638953208923, loss:4.592750072479248
epoch21: step3500/4680
step 84000: accuracy:0.2939999997615814, confidence:0.8187403678894043, loss:4.38664436340332
epoch21: step4000/4680
step 94500: accuracy:0.11500000208616257, confidence:0.9850260019302368, loss:8.42483139038086
epoch21: step4500/4680
step 0: accuracy:0.12099999934434891, confidence:0.993570864200592, loss:8.81456470489502
epoch22: step0/4680
step 11000: accuracy:0.08500000089406967, confidence:0.9750530123710632, loss:6.777286052703857
epoch22: step500/4680
step 22000: accuracy:0.10400000214576721, confidence:0.9445844888687134, loss:4.603515625
epoch22: step1000/4680
step 33000: accuracy:0.4860000014305115, confidence:0.6548477411270142, loss:1.9716277122497559
epoch22: step1500/4680
step 44000: accuracy:0.26499998569488525, confidence:0.8046612739562988, loss:3.307935953140259
epoch22: step2000/4680
step 55000: accuracy:0.2879999876022339, confidence:0.8089017271995544, loss:3.1973628997802734
epoch22: step2500/4680
step 66000: accuracy:0.32899999618530273, confidence:0.8318696618080139, loss:3.839235544204712
epoch22: step3000/4680
step 77000: accuracy:0.2370000034570694, confidence:0.8511086106300354, loss:4.873713493347168
epoch22: step3500/4680
step 88000: accuracy:0.23499999940395355, confidence:0.8500461578369141, loss:5.29121208190918
epoch22: step4000/4680
step 99000: accuracy:0.11800000071525574, confidence:0.9797253012657166, loss:8.049166679382324
epoch22: step4500/4680
step 0: accuracy:0.11500000208616257, confidence:0.9858282208442688, loss:8.223801612854004
epoch23: step0/4680
step 11500: accuracy:0.10100000351667404, confidence:0.9413677453994751, loss:5.364240646362305
epoch23: step500/4680
step 23000: accuracy:0.11699999868869781, confidence:0.9371479153633118, loss:4.645114421844482
epoch23: step1000/4680
step 34500: accuracy:0.5, confidence:0.6722274422645569, loss:2.0115859508514404
epoch23: step1500/4680
step 46000: accuracy:0.3149999976158142, confidence:0.7823976278305054, loss:2.86757230758667
epoch23: step2000/4680
step 57500: accuracy:0.31299999356269836, confidence:0.8452103137969971, loss:3.4303863048553467
epoch23: step2500/4680
step 69000: accuracy:0.3540000021457672, confidence:0.8230116367340088, loss:3.546823740005493
epoch23: step3000/4680
step 80500: accuracy:0.24899999797344208, confidence:0.8420024514198303, loss:4.686716079711914
epoch23: step3500/4680
step 92000: accuracy:0.28600001335144043, confidence:0.8322422504425049, loss:4.81011962890625
epoch23: step4000/4680
step 103500: accuracy:0.10999999940395355, confidence:0.9705134034156799, loss:7.9402384757995605
epoch23: step4500/4680
step 0: accuracy:0.12399999797344208, confidence:0.9803336262702942, loss:7.988895416259766
epoch24: step0/4680
step 12000: accuracy:0.10000000149011612, confidence:0.971401035785675, loss:7.126798629760742
epoch24: step500/4680
step 24000: accuracy:0.10199999809265137, confidence:0.9420902132987976, loss:4.498050689697266
epoch24: step1000/4680
step 36000: accuracy:0.5320000052452087, confidence:0.6778135299682617, loss:1.8853037357330322
epoch24: step1500/4680
step 48000: accuracy:0.33000001311302185, confidence:0.7747091054916382, loss:2.6229138374328613
epoch24: step2000/4680
step 60000: accuracy:0.3149999976158142, confidence:0.8163802623748779, loss:3.1669702529907227
epoch24: step2500/4680
step 72000: accuracy:0.3529999852180481, confidence:0.832552433013916, loss:3.6889543533325195
epoch24: step3000/4680
step 84000: accuracy:0.23999999463558197, confidence:0.8388638496398926, loss:4.411604404449463
epoch24: step3500/4680
step 96000: accuracy:0.26899999380111694, confidence:0.836377739906311, loss:4.972850799560547
epoch24: step4000/4680
step 108000: accuracy:0.1340000033378601, confidence:0.9545219540596008, loss:7.346435070037842
epoch24: step4500/4680
step 0: accuracy:0.12600000202655792, confidence:0.9457560181617737, loss:7.067286968231201
epoch25: step0/4680
step 12500: accuracy:0.10000000149011612, confidence:0.9416385293006897, loss:5.965025901794434
epoch25: step500/4680
step 25000: accuracy:0.12399999797344208, confidence:0.9420203566551208, loss:4.645570278167725
epoch25: step1000/4680
step 37500: accuracy:0.5550000071525574, confidence:0.7026704549789429, loss:1.7623748779296875
epoch25: step1500/4680
step 50000: accuracy:0.33399999141693115, confidence:0.7876555919647217, loss:2.7557497024536133
epoch25: step2000/4680
step 62500: accuracy:0.3330000042915344, confidence:0.818195641040802, loss:3.0659382343292236
epoch25: step2500/4680
step 75000: accuracy:0.38999998569488525, confidence:0.8192877173423767, loss:3.6404058933258057
epoch25: step3000/4680
step 87500: accuracy:0.23100000619888306, confidence:0.844057023525238, loss:4.783087730407715
epoch25: step3500/4680
step 100000: accuracy:0.2759999930858612, confidence:0.8384938836097717, loss:4.8810319900512695
epoch25: step4000/4680
step 112500: accuracy:0.13099999725818634, confidence:0.9716469049453735, loss:8.527366638183594
epoch25: step4500/4680
step 0: accuracy:0.11100000143051147, confidence:0.9774245619773865, loss:8.505175590515137
epoch26: step0/4680
step 13000: accuracy:0.1420000046491623, confidence:0.9119921922683716, loss:5.169516563415527
epoch26: step500/4680
step 26000: accuracy:0.10700000077486038, confidence:0.9631279110908508, loss:5.189157485961914
epoch26: step1000/4680
step 39000: accuracy:0.5550000071525574, confidence:0.6955621242523193, loss:1.785815954208374
epoch26: step1500/4680
step 52000: accuracy:0.31299999356269836, confidence:0.803128719329834, loss:2.900716781616211
epoch26: step2000/4680
step 65000: accuracy:0.32499998807907104, confidence:0.8130777478218079, loss:3.008497476577759
epoch26: step2500/4680
step 78000: accuracy:0.382999986410141, confidence:0.8416677713394165, loss:3.6538188457489014
epoch26: step3000/4680
step 91000: accuracy:0.23999999463558197, confidence:0.8332563042640686, loss:4.439399242401123
epoch26: step3500/4680
step 104000: accuracy:0.26899999380111694, confidence:0.8422979116439819, loss:5.101685523986816
epoch26: step4000/4680
step 117000: accuracy:0.125, confidence:0.9831174612045288, loss:8.610299110412598
epoch26: step4500/4680
step 0: accuracy:0.12300000339746475, confidence:0.9873166680335999, loss:8.99507999420166
epoch27: step0/4680
step 13500: accuracy:0.17499999701976776, confidence:0.8974742889404297, loss:4.6907477378845215
epoch27: step500/4680
step 27000: accuracy:0.12800000607967377, confidence:0.9660382270812988, loss:5.231620788574219
epoch27: step1000/4680
step 40500: accuracy:0.5529999732971191, confidence:0.7037246227264404, loss:1.725134015083313
epoch27: step1500/4680
step 54000: accuracy:0.32600000500679016, confidence:0.814301609992981, loss:3.0214779376983643
epoch27: step2000/4680
step 67500: accuracy:0.3630000054836273, confidence:0.8246795535087585, loss:2.7892754077911377
epoch27: step2500/4680
step 81000: accuracy:0.39500001072883606, confidence:0.8321882486343384, loss:3.501272439956665
epoch27: step3000/4680
step 94500: accuracy:0.25200000405311584, confidence:0.858474612236023, loss:4.497155666351318
epoch27: step3500/4680
step 108000: accuracy:0.2980000078678131, confidence:0.8436514735221863, loss:4.755487442016602
epoch27: step4000/4680
step 121500: accuracy:0.10999999940395355, confidence:0.9915672540664673, loss:9.338336944580078
epoch27: step4500/4680
step 0: accuracy:0.11299999803304672, confidence:0.9917973279953003, loss:9.032670021057129
epoch28: step0/4680
step 14000: accuracy:0.1459999978542328, confidence:0.9112662076950073, loss:5.554776191711426
epoch28: step500/4680
step 28000: accuracy:0.12399999797344208, confidence:0.9808782339096069, loss:5.696372032165527
epoch28: step1000/4680
step 42000: accuracy:0.5860000252723694, confidence:0.7078409194946289, loss:1.5883052349090576
epoch28: step1500/4680
step 56000: accuracy:0.31200000643730164, confidence:0.7950994968414307, loss:2.8996639251708984
epoch28: step2000/4680
step 70000: accuracy:0.32899999618530273, confidence:0.8200799822807312, loss:2.9648613929748535
epoch28: step2500/4680
step 84000: accuracy:0.38499999046325684, confidence:0.823328971862793, loss:3.434741735458374
epoch28: step3000/4680
step 98000: accuracy:0.23800000548362732, confidence:0.8510501980781555, loss:4.460042476654053
epoch28: step3500/4680
step 112000: accuracy:0.25600001215934753, confidence:0.842820942401886, loss:4.840557098388672
epoch28: step4000/4680
step 126000: accuracy:0.10499999672174454, confidence:0.9798285961151123, loss:8.491593360900879
epoch28: step4500/4680
step 0: accuracy:0.10400000214576721, confidence:0.9844861626625061, loss:8.338186264038086
epoch29: step0/4680
step 14500: accuracy:0.18000000715255737, confidence:0.8961247801780701, loss:4.696427345275879
epoch29: step500/4680
step 29000: accuracy:0.10499999672174454, confidence:0.9851812720298767, loss:6.148111343383789
epoch29: step1000/4680
step 43500: accuracy:0.5720000267028809, confidence:0.7157716751098633, loss:1.6960216760635376
epoch29: step1500/4680
step 58000: accuracy:0.3580000102519989, confidence:0.7838586568832397, loss:2.545933485031128
epoch29: step2000/4680
step 72500: accuracy:0.36000001430511475, confidence:0.8206725120544434, loss:2.801450490951538
epoch29: step2500/4680
step 87000: accuracy:0.4090000092983246, confidence:0.8280249238014221, loss:3.763211250305176
epoch29: step3000/4680
step 101500: accuracy:0.26100000739097595, confidence:0.8532745242118835, loss:4.481173038482666
epoch29: step3500/4680
step 116000: accuracy:0.289000004529953, confidence:0.8509146571159363, loss:4.960219860076904
epoch29: step4000/4680
step 130500: accuracy:0.12399999797344208, confidence:0.9847117066383362, loss:9.12110710144043
epoch29: step4500/4680
