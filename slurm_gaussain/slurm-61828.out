2018-06-15 17:39:21.426408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 17:39:21.426696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
2018-06-15 17:39:26.630862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
CHEKPOINT DIR: uniform
model has been loaded from mnist_classifier.pkl
---------
Variables: name (type shape) [size]
---------
Variable:0 (float32_ref 5x5x1x32) [800, bytes: 3200]
Variable_1:0 (float32_ref 32) [32, bytes: 128]
Variable_2:0 (float32_ref 5x5x32x64) [51200, bytes: 204800]
Variable_3:0 (float32_ref 64) [64, bytes: 256]
Variable_4:0 (float32_ref 3136x1024) [3211264, bytes: 12845056]
Variable_5:0 (float32_ref 1024) [1024, bytes: 4096]
Variable_6:0 (float32_ref 1024x10) [10240, bytes: 40960]
Variable_7:0 (float32_ref 10) [10, bytes: 40]
discriminator/d_conv1/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]
discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]
discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]
discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]
generator/g_fc1/Matrix:0 (float32_ref 74x1024) [75776, bytes: 303104]
generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]
generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]
generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]
classifier/c_fc1/Matrix:0 (float32_ref 1024x64) [65536, bytes: 262144]
classifier/c_fc1/bias:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/beta:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/gamma:0 (float32_ref 64) [64, bytes: 256]
classifier/c_fc2/Matrix:0 (float32_ref 64x12) [768, bytes: 3072]
classifier/c_fc2/bias:0 (float32_ref 12) [12, bytes: 48]
Total size of variables: 16552792
Total bytes of variables: 66211168
 [*] Reading checkpoints...
 [*] Success to read MultiModalInfoGAN.model-43721
 [*] Load SUCCESS


SAMPLES SIZE=4680,LABELS=299520


SAVED TRAINING SET generated_training_set_mnist_UniformSample_mu_0_sigma_0.15_czcc_czrc_rzcc_rzrc
 [*] Training finished!
 [*] Testing finished!
2018-06-15 17:40:18.000180: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 17:40:18.000370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
model has been loaded from mnist_classifier.pkl
argmax:[7 7 7 ..., 7 7 7]
step 0: accuracy:0.0, confidence:0.9975712895393372, loss:14.716750144958496
Assinging:8
[   0   39    0    0    0    0    0 9961]
argmax:[5 5 5 ..., 5 5 5]
step 0: accuracy:0.0, confidence:0.9875743389129639, loss:18.34659767150879
Assinging:6
[   0    0    0    0    0 9883    0    0  117]
argmax:[9 9 9 ..., 9 9 9]
step 0: accuracy:0.9961000084877014, confidence:0.9937304854393005, loss:0.016177073121070862
Assinging:10
[   0   39    0    0    0    0    0    0    0 9961]
argmax:[1 1 1 ..., 1 1 1]
step 0: accuracy:0.0, confidence:0.9996072053909302, loss:14.714092254638672
Assinging:2
[    0 10000]
argmax:[3 3 3 ..., 3 3 3]
step 0: accuracy:0.0, confidence:0.9994745850563049, loss:11.704439163208008
Assinging:4
[    0     0     0 10000]
argmax:[6 6 6 ..., 6 6 6]
step 0: accuracy:0.0, confidence:0.9994109272956848, loss:22.491304397583008
Assinging:7
[    0     0     0     0     0     0 10000]
argmax:[2 2 2 ..., 2 2 2]
step 0: accuracy:0.0, confidence:0.9987232685089111, loss:16.472150802612305
Assinging:3
[    0     0 10000]
argmax:[8 8 8 ..., 8 8 8]
step 0: accuracy:0.0, confidence:0.9983187317848206, loss:15.355284690856934
Assinging:9
[    0     0     0     0     0     0     0     0 10000]
argmax:[0 0 0 ..., 0 0 0]
step 0: accuracy:0.0, confidence:0.9998655319213867, loss:17.982446670532227
Assinging:1
[10000]
argmax:[4 4 4 ..., 4 4 4]
step 0: accuracy:0.0, confidence:0.9926775097846985, loss:15.980181694030762
Assinging:5
[    0     0     0     0 10000]
2018-06-15 17:40:37.214567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 17:40:37.214812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
step 0: accuracy:0.10700000077486038, confidence:0.965938150882721, loss:9.051644325256348
epoch0: step0/4680
step 0: accuracy:0.09200000017881393, confidence:1.0, loss:61.11001968383789
epoch0: step500/4680
step 0: accuracy:0.09099999815225601, confidence:1.0, loss:39.760536193847656
epoch0: step1000/4680
step 0: accuracy:0.10000000149011612, confidence:0.9999759197235107, loss:13.803627967834473
epoch0: step1500/4680
step 0: accuracy:0.1340000033378601, confidence:0.9992741942405701, loss:8.355018615722656
epoch0: step2000/4680
step 0: accuracy:0.07999999821186066, confidence:0.9967389702796936, loss:7.881389617919922
epoch0: step2500/4680
step 0: accuracy:0.11299999803304672, confidence:0.9926072359085083, loss:7.309549808502197
epoch0: step3000/4680
step 0: accuracy:0.1080000028014183, confidence:0.9987834095954895, loss:9.22793960571289
epoch0: step3500/4680
step 0: accuracy:0.10599999874830246, confidence:0.997612476348877, loss:8.291827201843262
epoch0: step4000/4680
step 0: accuracy:0.08500000089406967, confidence:0.9958709478378296, loss:7.400993347167969
epoch0: step4500/4680
step 0: accuracy:0.08299999684095383, confidence:0.998359203338623, loss:8.354931831359863
epoch1: step0/4680
step 500: accuracy:0.10899999737739563, confidence:0.8628392219543457, loss:4.174520015716553
epoch1: step500/4680
step 1000: accuracy:0.0820000022649765, confidence:0.9949749708175659, loss:9.346284866333008
epoch1: step1000/4680
step 1500: accuracy:0.09300000220537186, confidence:0.9953711628913879, loss:7.380262851715088
epoch1: step1500/4680
step 2000: accuracy:0.14000000059604645, confidence:0.9935176968574524, loss:6.354498863220215
epoch1: step2000/4680
step 2500: accuracy:0.09799999743700027, confidence:0.9953211545944214, loss:7.451554775238037
epoch1: step2500/4680
step 3000: accuracy:0.10300000011920929, confidence:0.995622456073761, loss:7.068591117858887
epoch1: step3000/4680
step 3500: accuracy:0.10499999672174454, confidence:0.99676114320755, loss:7.260831832885742
epoch1: step3500/4680
step 4000: accuracy:0.08500000089406967, confidence:0.9975600242614746, loss:7.867041110992432
epoch1: step4000/4680
step 4500: accuracy:0.09000000357627869, confidence:0.9878100156784058, loss:6.821755886077881
epoch1: step4500/4680
step 0: accuracy:0.10100000351667404, confidence:0.9967622756958008, loss:7.915467739105225
epoch2: step0/4680
step 1000: accuracy:0.0949999988079071, confidence:0.968589723110199, loss:5.630405426025391
epoch2: step500/4680
step 2000: accuracy:0.07599999755620956, confidence:0.9865440726280212, loss:7.529423713684082
epoch2: step1000/4680
step 3000: accuracy:0.0820000022649765, confidence:0.9764014482498169, loss:7.7959442138671875
epoch2: step1500/4680
step 4000: accuracy:0.11699999868869781, confidence:0.9688181281089783, loss:5.298768043518066
epoch2: step2000/4680
step 5000: accuracy:0.09799999743700027, confidence:0.9945741891860962, loss:7.163300037384033
epoch2: step2500/4680
step 6000: accuracy:0.08500000089406967, confidence:0.9885168075561523, loss:6.604922294616699
epoch2: step3000/4680
step 7000: accuracy:0.09399999678134918, confidence:0.9980262517929077, loss:7.835658073425293
epoch2: step3500/4680
step 8000: accuracy:0.0860000029206276, confidence:0.9988475441932678, loss:8.175117492675781
epoch2: step4000/4680
step 9000: accuracy:0.09700000286102295, confidence:0.9773276448249817, loss:6.236828804016113
epoch2: step4500/4680
step 0: accuracy:0.09799999743700027, confidence:0.9925822019577026, loss:7.63491678237915
epoch3: step0/4680
step 1500: accuracy:0.08299999684095383, confidence:0.8845284581184387, loss:4.061100482940674
epoch3: step500/4680
step 3000: accuracy:0.11900000274181366, confidence:0.9966225624084473, loss:8.71389102935791
epoch3: step1000/4680
step 4500: accuracy:0.11999999731779099, confidence:0.9770461916923523, loss:8.017407417297363
epoch3: step1500/4680
step 6000: accuracy:0.12999999523162842, confidence:0.9399241209030151, loss:5.228081226348877
epoch3: step2000/4680
step 7500: accuracy:0.09700000286102295, confidence:0.9969035387039185, loss:8.199348449707031
epoch3: step2500/4680
step 9000: accuracy:0.10499999672174454, confidence:0.9941049814224243, loss:7.700524806976318
epoch3: step3000/4680
step 10500: accuracy:0.08699999749660492, confidence:0.9979761838912964, loss:8.017483711242676
epoch3: step3500/4680
step 12000: accuracy:0.09200000017881393, confidence:0.9967305660247803, loss:7.203671455383301
epoch3: step4000/4680
step 13500: accuracy:0.13600000739097595, confidence:0.952630877494812, loss:5.6773858070373535
epoch3: step4500/4680
step 0: accuracy:0.10199999809265137, confidence:0.9770075082778931, loss:6.964880466461182
epoch4: step0/4680
step 2000: accuracy:0.10000000149011612, confidence:0.8365635275840759, loss:4.092592716217041
epoch4: step500/4680
step 4000: accuracy:0.17000000178813934, confidence:0.9180379509925842, loss:6.693210601806641
epoch4: step1000/4680
step 6000: accuracy:0.15299999713897705, confidence:0.968269944190979, loss:6.638612747192383
epoch4: step1500/4680
step 8000: accuracy:0.14499999582767487, confidence:0.8008719682693481, loss:3.82890248298645
epoch4: step2000/4680
step 10000: accuracy:0.08699999749660492, confidence:0.9995645880699158, loss:9.773959159851074
epoch4: step2500/4680
step 12000: accuracy:0.13099999725818634, confidence:0.9423678517341614, loss:5.0504150390625
epoch4: step3000/4680
step 14000: accuracy:0.08900000154972076, confidence:0.9984780550003052, loss:8.761611938476562
epoch4: step3500/4680
step 16000: accuracy:0.09300000220537186, confidence:0.9983510375022888, loss:8.05864143371582
epoch4: step4000/4680
step 18000: accuracy:0.12700000405311584, confidence:0.9535878300666809, loss:5.747037887573242
epoch4: step4500/4680
step 0: accuracy:0.09600000083446503, confidence:0.9791868329048157, loss:7.06464147567749
epoch5: step0/4680
step 2500: accuracy:0.1379999965429306, confidence:0.821914792060852, loss:3.669710159301758
epoch5: step500/4680
step 5000: accuracy:0.15399999916553497, confidence:0.8999530076980591, loss:5.142202377319336
epoch5: step1000/4680
step 7500: accuracy:0.210999995470047, confidence:0.810034990310669, loss:3.6418471336364746
epoch5: step1500/4680
step 10000: accuracy:0.20499999821186066, confidence:0.7478671669960022, loss:3.1520795822143555
epoch5: step2000/4680
step 12500: accuracy:0.09099999815225601, confidence:0.9975574016571045, loss:8.485661506652832
epoch5: step2500/4680
step 15000: accuracy:0.1589999943971634, confidence:0.9363707304000854, loss:4.690882205963135
epoch5: step3000/4680
step 17500: accuracy:0.15000000596046448, confidence:0.9414156079292297, loss:5.256836891174316
epoch5: step3500/4680
step 20000: accuracy:0.09700000286102295, confidence:0.9563717246055603, loss:5.164040565490723
epoch5: step4000/4680
step 22500: accuracy:0.17399999499320984, confidence:0.924469530582428, loss:5.104655742645264
epoch5: step4500/4680
step 0: accuracy:0.14100000262260437, confidence:0.9425001740455627, loss:5.752679824829102
epoch6: step0/4680
step 3000: accuracy:0.2639999985694885, confidence:0.7519029378890991, loss:2.706524133682251
epoch6: step500/4680
step 6000: accuracy:0.1979999989271164, confidence:0.9244497418403625, loss:5.626152515411377
epoch6: step1000/4680
step 9000: accuracy:0.210999995470047, confidence:0.8222869634628296, loss:3.9790074825286865
epoch6: step1500/4680
step 12000: accuracy:0.26100000739097595, confidence:0.7336544990539551, loss:2.8009440898895264
epoch6: step2000/4680
step 15000: accuracy:0.1289999932050705, confidence:0.9913322329521179, loss:8.111781120300293
epoch6: step2500/4680
step 18000: accuracy:0.17399999499320984, confidence:0.8655151128768921, loss:4.320127010345459
epoch6: step3000/4680
step 21000: accuracy:0.11800000071525574, confidence:0.9383896589279175, loss:5.983940601348877
epoch6: step3500/4680
step 24000: accuracy:0.10700000077486038, confidence:0.9640856385231018, loss:5.363439559936523
epoch6: step4000/4680
step 27000: accuracy:0.21400000154972076, confidence:0.8839569091796875, loss:4.178552627563477
epoch6: step4500/4680
step 0: accuracy:0.16300000250339508, confidence:0.918718695640564, loss:5.069079875946045
epoch7: step0/4680
step 3500: accuracy:0.2619999945163727, confidence:0.7631921768188477, loss:2.903938055038452
epoch7: step500/4680
step 7000: accuracy:0.20200000703334808, confidence:0.9158110022544861, loss:4.809431552886963
epoch7: step1000/4680
step 10500: accuracy:0.1720000058412552, confidence:0.9126660227775574, loss:4.990006446838379
epoch7: step1500/4680
step 14000: accuracy:0.16599999368190765, confidence:0.7968977689743042, loss:3.940645217895508
epoch7: step2000/4680
step 17500: accuracy:0.11599999666213989, confidence:0.9399982690811157, loss:5.0863118171691895
epoch7: step2500/4680
step 21000: accuracy:0.1850000023841858, confidence:0.8751929998397827, loss:4.342177867889404
epoch7: step3000/4680
step 24500: accuracy:0.1809999942779541, confidence:0.9138771891593933, loss:4.735917091369629
epoch7: step3500/4680
step 28000: accuracy:0.11400000005960464, confidence:0.9296143054962158, loss:4.90291690826416
epoch7: step4000/4680
step 31500: accuracy:0.22300000488758087, confidence:0.8741517066955566, loss:4.0978007316589355
epoch7: step4500/4680
step 0: accuracy:0.20100000500679016, confidence:0.8934224247932434, loss:4.644949436187744
epoch8: step0/4680
step 4000: accuracy:0.25600001215934753, confidence:0.812865138053894, loss:4.084444046020508
epoch8: step500/4680
step 8000: accuracy:0.19099999964237213, confidence:0.9262603521347046, loss:5.559876441955566
epoch8: step1000/4680
step 12000: accuracy:0.23199999332427979, confidence:0.7993189096450806, loss:3.493570566177368
epoch8: step1500/4680
step 16000: accuracy:0.3089999854564667, confidence:0.7336366176605225, loss:2.7549026012420654
epoch8: step2000/4680
step 20000: accuracy:0.11400000005960464, confidence:0.9977937936782837, loss:9.960813522338867
epoch8: step2500/4680
step 24000: accuracy:0.16599999368190765, confidence:0.8705652952194214, loss:4.282551288604736
epoch8: step3000/4680
step 28000: accuracy:0.16200000047683716, confidence:0.88322913646698, loss:4.802692890167236
epoch8: step3500/4680
step 32000: accuracy:0.10700000077486038, confidence:0.9207830429077148, loss:4.854769706726074
epoch8: step4000/4680
step 36000: accuracy:0.2460000067949295, confidence:0.8279891014099121, loss:3.4520199298858643
epoch8: step4500/4680
step 0: accuracy:0.21299999952316284, confidence:0.8637259602546692, loss:4.3239922523498535
epoch9: step0/4680
step 4500: accuracy:0.26100000739097595, confidence:0.7860745787620544, loss:3.52868914604187
epoch9: step500/4680
step 9000: accuracy:0.20000000298023224, confidence:0.9073613882064819, loss:4.488134384155273
epoch9: step1000/4680
step 13500: accuracy:0.17100000381469727, confidence:0.8218470215797424, loss:5.713244915008545
epoch9: step1500/4680
step 18000: accuracy:0.23899999260902405, confidence:0.7708812355995178, loss:3.614190101623535
epoch9: step2000/4680
step 22500: accuracy:0.1120000034570694, confidence:0.9796820878982544, loss:7.023437023162842
epoch9: step2500/4680
step 27000: accuracy:0.2199999988079071, confidence:0.8707714676856995, loss:4.238696575164795
epoch9: step3000/4680
step 31500: accuracy:0.2070000022649765, confidence:0.8993785977363586, loss:4.4390459060668945
epoch9: step3500/4680
step 36000: accuracy:0.18199999630451202, confidence:0.8949283957481384, loss:4.281400680541992
epoch9: step4000/4680
step 40500: accuracy:0.2199999988079071, confidence:0.859683096408844, loss:4.039112091064453
epoch9: step4500/4680
step 0: accuracy:0.20499999821186066, confidence:0.8836877942085266, loss:4.417578220367432
epoch10: step0/4680
step 5000: accuracy:0.26499998569488525, confidence:0.8041400909423828, loss:3.7324514389038086
epoch10: step500/4680
step 10000: accuracy:0.1770000010728836, confidence:0.9187580943107605, loss:5.387463569641113
epoch10: step1000/4680
step 15000: accuracy:0.28999999165534973, confidence:0.7427123785018921, loss:2.9491608142852783
epoch10: step1500/4680
step 20000: accuracy:0.2939999997615814, confidence:0.7450140118598938, loss:2.7515363693237305
epoch10: step2000/4680
step 25000: accuracy:0.09300000220537186, confidence:0.9988447427749634, loss:11.929692268371582
epoch10: step2500/4680
step 30000: accuracy:0.20399999618530273, confidence:0.8609085083007812, loss:4.169136047363281
epoch10: step3000/4680
step 35000: accuracy:0.20000000298023224, confidence:0.8893374800682068, loss:4.606541156768799
epoch10: step3500/4680
step 40000: accuracy:0.1379999965429306, confidence:0.886500895023346, loss:4.532125473022461
epoch10: step4000/4680
step 45000: accuracy:0.29899999499320984, confidence:0.8043205142021179, loss:2.9684536457061768
epoch10: step4500/4680
step 0: accuracy:0.2639999985694885, confidence:0.8640680909156799, loss:3.9744980335235596
epoch11: step0/4680
step 5500: accuracy:0.2720000147819519, confidence:0.8147728443145752, loss:3.629409074783325
epoch11: step500/4680
step 11000: accuracy:0.1889999955892563, confidence:0.9207236170768738, loss:5.676517486572266
epoch11: step1000/4680
step 16500: accuracy:0.13300000131130219, confidence:0.8773923516273499, loss:7.306037425994873
epoch11: step1500/4680
step 22000: accuracy:0.26499998569488525, confidence:0.7800524234771729, loss:3.4051284790039062
epoch11: step2000/4680
step 27500: accuracy:0.10899999737739563, confidence:0.9692174792289734, loss:6.992277145385742
epoch11: step2500/4680
step 33000: accuracy:0.20800000429153442, confidence:0.8644651174545288, loss:4.431211471557617
epoch11: step3000/4680
step 38500: accuracy:0.20600000023841858, confidence:0.8886786103248596, loss:4.1978864669799805
epoch11: step3500/4680
step 44000: accuracy:0.17599999904632568, confidence:0.8982720375061035, loss:4.4380950927734375
epoch11: step4000/4680
step 49500: accuracy:0.2770000100135803, confidence:0.8330789804458618, loss:3.2743778228759766
epoch11: step4500/4680
step 0: accuracy:0.2669999897480011, confidence:0.8536730408668518, loss:3.759567975997925
epoch12: step0/4680
step 6000: accuracy:0.2720000147819519, confidence:0.8367205262184143, loss:4.473466396331787
epoch12: step500/4680
step 12000: accuracy:0.20600000023841858, confidence:0.9183613657951355, loss:5.195845603942871
epoch12: step1000/4680
step 18000: accuracy:0.32899999618530273, confidence:0.7270971536636353, loss:2.5002291202545166
epoch12: step1500/4680
step 24000: accuracy:0.2849999964237213, confidence:0.7555028796195984, loss:2.7701313495635986
epoch12: step2000/4680
step 30000: accuracy:0.10899999737739563, confidence:0.9828311204910278, loss:8.32108211517334
epoch12: step2500/4680
step 36000: accuracy:0.18700000643730164, confidence:0.8819853663444519, loss:4.78395414352417
epoch12: step3000/4680
step 42000: accuracy:0.19900000095367432, confidence:0.8724099397659302, loss:4.042446136474609
epoch12: step3500/4680
step 48000: accuracy:0.13600000739097595, confidence:0.8920836448669434, loss:4.701389789581299
epoch12: step4000/4680
step 54000: accuracy:0.3449999988079071, confidence:0.7972550988197327, loss:2.7945430278778076
epoch12: step4500/4680
step 0: accuracy:0.25200000405311584, confidence:0.8600624203681946, loss:4.08016300201416
epoch13: step0/4680
step 6500: accuracy:0.2840000092983246, confidence:0.8056639432907104, loss:2.9639174938201904
epoch13: step500/4680
step 13000: accuracy:0.21400000154972076, confidence:0.8599530458450317, loss:4.332723140716553
epoch13: step1000/4680
step 19500: accuracy:0.2879999876022339, confidence:0.8273793458938599, loss:3.7729458808898926
epoch13: step1500/4680
step 26000: accuracy:0.38100001215934753, confidence:0.783835768699646, loss:2.5163638591766357
epoch13: step2000/4680
step 32500: accuracy:0.10199999809265137, confidence:0.9905110001564026, loss:8.804603576660156
epoch13: step2500/4680
step 39000: accuracy:0.2160000056028366, confidence:0.8542796969413757, loss:4.081490516662598
epoch13: step3000/4680
step 45500: accuracy:0.23000000417232513, confidence:0.8939507603645325, loss:3.864600419998169
epoch13: step3500/4680
step 52000: accuracy:0.19599999487400055, confidence:0.8775845766067505, loss:4.20325231552124
epoch13: step4000/4680
step 58500: accuracy:0.3109999895095825, confidence:0.7919064164161682, loss:2.912921905517578
epoch13: step4500/4680
step 0: accuracy:0.26499998569488525, confidence:0.8425374627113342, loss:3.896117687225342
epoch14: step0/4680
step 7000: accuracy:0.2590000033378601, confidence:0.8194844722747803, loss:4.02645206451416
epoch14: step500/4680
step 14000: accuracy:0.20800000429153442, confidence:0.9289814233779907, loss:5.604475021362305
epoch14: step1000/4680
step 21000: accuracy:0.2840000092983246, confidence:0.7781678438186646, loss:3.005561590194702
epoch14: step1500/4680
step 28000: accuracy:0.28999999165534973, confidence:0.7566980123519897, loss:2.713935375213623
epoch14: step2000/4680
step 35000: accuracy:0.12200000137090683, confidence:0.9463576078414917, loss:6.073732852935791
epoch14: step2500/4680
step 42000: accuracy:0.1860000044107437, confidence:0.8768441677093506, loss:4.553779125213623
epoch14: step3000/4680
step 49000: accuracy:0.2070000022649765, confidence:0.880375325679779, loss:3.804492473602295
epoch14: step3500/4680
step 56000: accuracy:0.16300000250339508, confidence:0.8745215535163879, loss:4.454260349273682
epoch14: step4000/4680
step 63000: accuracy:0.335999995470047, confidence:0.7862876653671265, loss:2.717989206314087
epoch14: step4500/4680
step 0: accuracy:0.26899999380111694, confidence:0.8241919875144958, loss:3.7224888801574707
epoch15: step0/4680
step 7500: accuracy:0.3109999895095825, confidence:0.8138244152069092, loss:3.4857733249664307
epoch15: step500/4680
step 15000: accuracy:0.20499999821186066, confidence:0.9280362129211426, loss:5.728792667388916
epoch15: step1000/4680
step 22500: accuracy:0.14100000262260437, confidence:0.846156120300293, loss:10.758874893188477
epoch15: step1500/4680
step 30000: accuracy:0.2669999897480011, confidence:0.8168752193450928, loss:3.7010068893432617
epoch15: step2000/4680
step 37500: accuracy:0.10199999809265137, confidence:0.9820781946182251, loss:7.696356296539307
epoch15: step2500/4680
step 45000: accuracy:0.21799999475479126, confidence:0.8536142706871033, loss:4.4218831062316895
epoch15: step3000/4680
step 52500: accuracy:0.18700000643730164, confidence:0.8898646831512451, loss:4.7008819580078125
epoch15: step3500/4680
step 60000: accuracy:0.19099999964237213, confidence:0.8817124962806702, loss:4.353146076202393
epoch15: step4000/4680
step 67500: accuracy:0.3050000071525574, confidence:0.7959317564964294, loss:2.985305070877075
epoch15: step4500/4680
step 0: accuracy:0.2639999985694885, confidence:0.8382992148399353, loss:3.6874258518218994
epoch16: step0/4680
step 8000: accuracy:0.28999999165534973, confidence:0.8196178674697876, loss:3.9933054447174072
epoch16: step500/4680
step 16000: accuracy:0.1940000057220459, confidence:0.9140080213546753, loss:4.557399749755859
epoch16: step1000/4680
step 24000: accuracy:0.29600000381469727, confidence:0.7826139330863953, loss:3.3908467292785645
epoch16: step1500/4680
step 32000: accuracy:0.27399998903274536, confidence:0.7768619060516357, loss:3.185802936553955
epoch16: step2000/4680
step 40000: accuracy:0.1120000034570694, confidence:0.9659876823425293, loss:7.453812122344971
epoch16: step2500/4680
step 48000: accuracy:0.17599999904632568, confidence:0.9049559831619263, loss:5.567646026611328
epoch16: step3000/4680
step 56000: accuracy:0.23100000619888306, confidence:0.8561530113220215, loss:3.4168343544006348
epoch16: step3500/4680
step 64000: accuracy:0.14800000190734863, confidence:0.8739840984344482, loss:4.331788063049316
epoch16: step4000/4680
step 72000: accuracy:0.2919999957084656, confidence:0.7759553790092468, loss:2.808811664581299
epoch16: step4500/4680
step 0: accuracy:0.2879999876022339, confidence:0.8405073881149292, loss:3.650400161743164
epoch17: step0/4680
step 8500: accuracy:0.29899999499320984, confidence:0.857070803642273, loss:3.821202039718628
epoch17: step500/4680
step 17000: accuracy:0.20100000500679016, confidence:0.920090913772583, loss:5.9072394371032715
epoch17: step1000/4680
step 25500: accuracy:0.2240000069141388, confidence:0.8335648775100708, loss:5.335043907165527
epoch17: step1500/4680
step 34000: accuracy:0.35600000619888306, confidence:0.7690916657447815, loss:2.6487209796905518
epoch17: step2000/4680
step 42500: accuracy:0.1340000033378601, confidence:0.9422919750213623, loss:5.660849571228027
epoch17: step2500/4680
step 51000: accuracy:0.20600000023841858, confidence:0.8696484565734863, loss:4.500580787658691
epoch17: step3000/4680
step 59500: accuracy:0.19099999964237213, confidence:0.8858779072761536, loss:4.45631217956543
epoch17: step3500/4680
step 68000: accuracy:0.20999999344348907, confidence:0.8854560256004333, loss:4.209773540496826
epoch17: step4000/4680
step 76500: accuracy:0.3009999990463257, confidence:0.8044320940971375, loss:3.1512389183044434
epoch17: step4500/4680
step 0: accuracy:0.257999986410141, confidence:0.8549753427505493, loss:3.9279425144195557
epoch18: step0/4680
step 9000: accuracy:0.2840000092983246, confidence:0.8254808187484741, loss:3.9870975017547607
epoch18: step500/4680
step 18000: accuracy:0.18700000643730164, confidence:0.926296055316925, loss:5.251219749450684
epoch18: step1000/4680
step 27000: accuracy:0.3440000116825104, confidence:0.7563502192497253, loss:2.4571568965911865
epoch18: step1500/4680
step 36000: accuracy:0.30399999022483826, confidence:0.7601115703582764, loss:2.608797788619995
epoch18: step2000/4680
step 45000: accuracy:0.12600000202655792, confidence:0.955790102481842, loss:6.485745906829834
epoch18: step2500/4680
step 54000: accuracy:0.19099999964237213, confidence:0.864942729473114, loss:4.350491046905518
epoch18: step3000/4680
step 63000: accuracy:0.19099999964237213, confidence:0.8527992963790894, loss:3.867654323577881
epoch18: step3500/4680
step 72000: accuracy:0.16699999570846558, confidence:0.8743926286697388, loss:4.515583515167236
epoch18: step4000/4680
step 81000: accuracy:0.3179999887943268, confidence:0.8002833127975464, loss:2.859215497970581
epoch18: step4500/4680
step 0: accuracy:0.28600001335144043, confidence:0.8321014642715454, loss:3.816147565841675
epoch19: step0/4680
step 9500: accuracy:0.32100000977516174, confidence:0.8267573714256287, loss:3.2486605644226074
epoch19: step500/4680
step 19000: accuracy:0.22100000083446503, confidence:0.8984803557395935, loss:5.02004861831665
epoch19: step1000/4680
step 28500: accuracy:0.23499999940395355, confidence:0.8294755220413208, loss:4.863589286804199
epoch19: step1500/4680
step 38000: accuracy:0.39500001072883606, confidence:0.7739244699478149, loss:2.3807506561279297
epoch19: step2000/4680
step 47500: accuracy:0.17499999701976776, confidence:0.9294055104255676, loss:5.095301628112793
epoch19: step2500/4680
step 57000: accuracy:0.17299999296665192, confidence:0.8879031538963318, loss:4.9574456214904785
epoch19: step3000/4680
step 66500: accuracy:0.23000000417232513, confidence:0.8679530620574951, loss:3.8398218154907227
epoch19: step3500/4680
step 76000: accuracy:0.164000004529953, confidence:0.8840726613998413, loss:4.553890705108643
epoch19: step4000/4680
step 85500: accuracy:0.28299999237060547, confidence:0.8113760352134705, loss:3.324009656906128
epoch19: step4500/4680
step 0: accuracy:0.2840000092983246, confidence:0.8290616273880005, loss:3.736938714981079
epoch20: step0/4680
step 10000: accuracy:0.2849999964237213, confidence:0.8542672395706177, loss:3.875194549560547
epoch20: step500/4680
step 20000: accuracy:0.1889999955892563, confidence:0.9291983842849731, loss:5.838949203491211
epoch20: step1000/4680
step 30000: accuracy:0.3269999921321869, confidence:0.768095076084137, loss:2.6070168018341064
epoch20: step1500/4680
step 40000: accuracy:0.3149999976158142, confidence:0.757937490940094, loss:2.6349706649780273
epoch20: step2000/4680
step 50000: accuracy:0.13699999451637268, confidence:0.9201483130455017, loss:5.3360748291015625
epoch20: step2500/4680
step 60000: accuracy:0.23600000143051147, confidence:0.8687699437141418, loss:3.988455295562744
epoch20: step3000/4680
step 70000: accuracy:0.2540000081062317, confidence:0.8558148145675659, loss:3.599323034286499
epoch20: step3500/4680
step 80000: accuracy:0.1379999965429306, confidence:0.9001391530036926, loss:4.894920825958252
epoch20: step4000/4680
step 90000: accuracy:0.34700000286102295, confidence:0.7871434688568115, loss:2.7261555194854736
epoch20: step4500/4680
step 0: accuracy:0.26899999380111694, confidence:0.836775004863739, loss:4.085483074188232
epoch21: step0/4680
step 10500: accuracy:0.32600000500679016, confidence:0.7979476451873779, loss:2.9000704288482666
epoch21: step500/4680
step 21000: accuracy:0.21299999952316284, confidence:0.9120103716850281, loss:5.060457229614258
epoch21: step1000/4680
step 31500: accuracy:0.23800000548362732, confidence:0.8486309051513672, loss:5.468453884124756
epoch21: step1500/4680
step 42000: accuracy:0.37700000405311584, confidence:0.7620108127593994, loss:2.5757346153259277
epoch21: step2000/4680
step 52500: accuracy:0.1379999965429306, confidence:0.9567371606826782, loss:6.440582752227783
epoch21: step2500/4680
step 63000: accuracy:0.1850000023841858, confidence:0.8812147378921509, loss:4.868923664093018
epoch21: step3000/4680
step 73500: accuracy:0.2329999953508377, confidence:0.8590700030326843, loss:3.611391305923462
epoch21: step3500/4680
step 84000: accuracy:0.164000004529953, confidence:0.917551577091217, loss:5.149382591247559
epoch21: step4000/4680
step 94500: accuracy:0.2709999978542328, confidence:0.8345850706100464, loss:3.59603214263916
epoch21: step4500/4680
step 0: accuracy:0.2720000147819519, confidence:0.8402003645896912, loss:3.8316566944122314
epoch22: step0/4680
step 11000: accuracy:0.25999999046325684, confidence:0.8088513612747192, loss:3.9205853939056396
epoch22: step500/4680
step 22000: accuracy:0.20499999821186066, confidence:0.9101666808128357, loss:4.738283634185791
epoch22: step1000/4680
step 33000: accuracy:0.24400000274181366, confidence:0.794216513633728, loss:3.877727746963501
epoch22: step1500/4680
step 44000: accuracy:0.23999999463558197, confidence:0.7340087294578552, loss:2.8100812435150146
epoch22: step2000/4680
step 55000: accuracy:0.11699999868869781, confidence:0.9278902411460876, loss:5.449550628662109
epoch22: step2500/4680
step 66000: accuracy:0.21899999678134918, confidence:0.8389737606048584, loss:4.095170974731445
epoch22: step3000/4680
step 77000: accuracy:0.22599999606609344, confidence:0.8676767945289612, loss:3.7713980674743652
epoch22: step3500/4680
step 88000: accuracy:0.1459999978542328, confidence:0.8815209269523621, loss:4.673898220062256
epoch22: step4000/4680
step 99000: accuracy:0.3199999928474426, confidence:0.7752184271812439, loss:2.6773812770843506
epoch22: step4500/4680
step 0: accuracy:0.29499998688697815, confidence:0.8386606574058533, loss:3.7786507606506348
epoch23: step0/4680
step 11500: accuracy:0.2879999876022339, confidence:0.8434353470802307, loss:3.828315496444702
epoch23: step500/4680
step 23000: accuracy:0.17100000381469727, confidence:0.9343001842498779, loss:6.66212797164917
epoch23: step1000/4680
step 34500: accuracy:0.2160000056028366, confidence:0.845431923866272, loss:5.822061538696289
epoch23: step1500/4680
step 46000: accuracy:0.3440000116825104, confidence:0.7984878420829773, loss:3.0082433223724365
epoch23: step2000/4680
step 57500: accuracy:0.11699999868869781, confidence:0.9764944314956665, loss:8.390865325927734
epoch23: step2500/4680
step 69000: accuracy:0.1589999943971634, confidence:0.9131935834884644, loss:5.487486362457275
epoch23: step3000/4680
step 80500: accuracy:0.2460000067949295, confidence:0.8638676404953003, loss:3.6755025386810303
epoch23: step3500/4680
step 92000: accuracy:0.19599999487400055, confidence:0.9039990901947021, loss:4.6669697761535645
epoch23: step4000/4680
step 103500: accuracy:0.24199999868869781, confidence:0.8480454087257385, loss:4.188548564910889
epoch23: step4500/4680
step 0: accuracy:0.27900001406669617, confidence:0.8563305735588074, loss:4.030840873718262
epoch24: step0/4680
step 12000: accuracy:0.2750000059604645, confidence:0.8189594745635986, loss:3.6236391067504883
epoch24: step500/4680
step 24000: accuracy:0.2370000034570694, confidence:0.8845986723899841, loss:3.9755754470825195
epoch24: step1000/4680
step 36000: accuracy:0.3230000138282776, confidence:0.820058286190033, loss:3.7030625343322754
epoch24: step1500/4680
step 48000: accuracy:0.30000001192092896, confidence:0.7791255712509155, loss:2.6845085620880127
epoch24: step2000/4680
step 60000: accuracy:0.13500000536441803, confidence:0.9340787529945374, loss:5.75526762008667
epoch24: step2500/4680
step 72000: accuracy:0.2329999953508377, confidence:0.8585225939750671, loss:4.17929220199585
epoch24: step3000/4680
step 84000: accuracy:0.20800000429153442, confidence:0.8853204250335693, loss:4.090603351593018
epoch24: step3500/4680
step 96000: accuracy:0.14499999582767487, confidence:0.882376492023468, loss:4.879856586456299
epoch24: step4000/4680
step 108000: accuracy:0.3050000071525574, confidence:0.7891942858695984, loss:2.887150764465332
epoch24: step4500/4680
step 0: accuracy:0.27799999713897705, confidence:0.8376762866973877, loss:3.941575765609741
epoch25: step0/4680
step 12500: accuracy:0.3160000145435333, confidence:0.8611702919006348, loss:3.993967056274414
epoch25: step500/4680
step 25000: accuracy:0.1809999942779541, confidence:0.9244545102119446, loss:6.567878246307373
epoch25: step1000/4680
step 37500: accuracy:0.16599999368190765, confidence:0.8620839715003967, loss:5.579282283782959
epoch25: step1500/4680
step 50000: accuracy:0.34200000762939453, confidence:0.7994657158851624, loss:2.9895548820495605
epoch25: step2000/4680
step 62500: accuracy:0.1599999964237213, confidence:0.9168694019317627, loss:5.27493143081665
epoch25: step2500/4680
step 75000: accuracy:0.2150000035762787, confidence:0.8735790848731995, loss:4.45913553237915
epoch25: step3000/4680
step 87500: accuracy:0.2540000081062317, confidence:0.8671042919158936, loss:3.5451581478118896
epoch25: step3500/4680
step 100000: accuracy:0.20200000703334808, confidence:0.8922351002693176, loss:4.658977031707764
epoch25: step4000/4680
step 112500: accuracy:0.26600000262260437, confidence:0.8369212746620178, loss:3.5856332778930664
epoch25: step4500/4680
step 0: accuracy:0.25, confidence:0.8420044183731079, loss:3.90494966506958
epoch26: step0/4680
step 13000: accuracy:0.28700000047683716, confidence:0.8089416027069092, loss:3.617805004119873
epoch26: step500/4680
step 26000: accuracy:0.22300000488758087, confidence:0.9240121245384216, loss:4.746335983276367
epoch26: step1000/4680
step 39000: accuracy:0.21199999749660492, confidence:0.8295491337776184, loss:5.869282245635986
epoch26: step1500/4680
step 52000: accuracy:0.2800000011920929, confidence:0.7815329432487488, loss:2.999711275100708
epoch26: step2000/4680
step 65000: accuracy:0.12200000137090683, confidence:0.9588851928710938, loss:6.562225818634033
epoch26: step2500/4680
step 78000: accuracy:0.210999995470047, confidence:0.85359126329422, loss:4.194939136505127
epoch26: step3000/4680
step 91000: accuracy:0.1979999989271164, confidence:0.871078610420227, loss:4.305394172668457
epoch26: step3500/4680
step 104000: accuracy:0.13300000131130219, confidence:0.8860850930213928, loss:4.6560258865356445
epoch26: step4000/4680
step 117000: accuracy:0.3479999899864197, confidence:0.7736916542053223, loss:2.675773859024048
epoch26: step4500/4680
step 0: accuracy:0.25, confidence:0.8546679019927979, loss:4.193412780761719
epoch27: step0/4680
step 13500: accuracy:0.2939999997615814, confidence:0.8559325933456421, loss:4.178737163543701
epoch27: step500/4680
step 27000: accuracy:0.18799999356269836, confidence:0.936077892780304, loss:6.710846424102783
epoch27: step1000/4680
step 40500: accuracy:0.23199999332427979, confidence:0.8354688882827759, loss:4.655109882354736
epoch27: step1500/4680
step 54000: accuracy:0.30399999022483826, confidence:0.7987934350967407, loss:3.340167760848999
epoch27: step2000/4680
step 67500: accuracy:0.1379999965429306, confidence:0.9506335258483887, loss:6.335615158081055
epoch27: step2500/4680
step 81000: accuracy:0.20100000500679016, confidence:0.8820356130599976, loss:4.832890033721924
epoch27: step3000/4680
step 94500: accuracy:0.25200000405311584, confidence:0.857292652130127, loss:3.6322271823883057
epoch27: step3500/4680
step 108000: accuracy:0.1770000010728836, confidence:0.8760221004486084, loss:4.694682598114014
epoch27: step4000/4680
step 121500: accuracy:0.2630000114440918, confidence:0.839773952960968, loss:3.8257339000701904
epoch27: step4500/4680
step 0: accuracy:0.24500000476837158, confidence:0.8437743186950684, loss:3.855095386505127
epoch28: step0/4680
step 14000: accuracy:0.27399998903274536, confidence:0.8266221284866333, loss:3.855825901031494
epoch28: step500/4680
step 28000: accuracy:0.19699999690055847, confidence:0.9156741499900818, loss:5.123885631561279
epoch28: step1000/4680
step 42000: accuracy:0.23199999332427979, confidence:0.8462815880775452, loss:6.239124298095703
epoch28: step1500/4680
step 56000: accuracy:0.26899999380111694, confidence:0.7543277144432068, loss:3.059081554412842
epoch28: step2000/4680
step 70000: accuracy:0.11400000005960464, confidence:0.9481152296066284, loss:6.054115295410156
epoch28: step2500/4680
step 84000: accuracy:0.2370000034570694, confidence:0.8597514629364014, loss:4.132009983062744
epoch28: step3000/4680
step 98000: accuracy:0.21899999678134918, confidence:0.8680052757263184, loss:4.0548319816589355
epoch28: step3500/4680
step 112000: accuracy:0.1459999978542328, confidence:0.8940002918243408, loss:4.902271270751953
epoch28: step4000/4680
step 126000: accuracy:0.31700000166893005, confidence:0.789223849773407, loss:2.923156261444092
epoch28: step4500/4680
step 0: accuracy:0.26600000262260437, confidence:0.8411688208580017, loss:4.143052101135254
epoch29: step0/4680
step 14500: accuracy:0.28700000047683716, confidence:0.8617557883262634, loss:4.527776718139648
epoch29: step500/4680
step 29000: accuracy:0.19300000369548798, confidence:0.937279462814331, loss:6.530040264129639
epoch29: step1000/4680
step 43500: accuracy:0.30799999833106995, confidence:0.7936459183692932, loss:3.1637091636657715
epoch29: step1500/4680
step 58000: accuracy:0.34599998593330383, confidence:0.7840278744697571, loss:2.7839696407318115
epoch29: step2000/4680
step 72500: accuracy:0.15000000596046448, confidence:0.9497726559638977, loss:6.290832996368408
epoch29: step2500/4680
step 87000: accuracy:0.2280000001192093, confidence:0.8801699280738831, loss:4.582141876220703
epoch29: step3000/4680
step 101500: accuracy:0.22100000083446503, confidence:0.8496370911598206, loss:3.56132173538208
epoch29: step3500/4680
step 116000: accuracy:0.1809999942779541, confidence:0.882363498210907, loss:4.627656936645508
epoch29: step4000/4680
step 130500: accuracy:0.23399999737739563, confidence:0.8552632927894592, loss:4.293239116668701
epoch29: step4500/4680
2018-06-15 17:50:58.895492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 17:50:58.895723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
2018-06-15 17:51:01.113010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
CHEKPOINT DIR: uniform
model has been loaded from mnist_classifier.pkl
---------
Variables: name (type shape) [size]
---------
Variable:0 (float32_ref 5x5x1x32) [800, bytes: 3200]
Variable_1:0 (float32_ref 32) [32, bytes: 128]
Variable_2:0 (float32_ref 5x5x32x64) [51200, bytes: 204800]
Variable_3:0 (float32_ref 64) [64, bytes: 256]
Variable_4:0 (float32_ref 3136x1024) [3211264, bytes: 12845056]
Variable_5:0 (float32_ref 1024) [1024, bytes: 4096]
Variable_6:0 (float32_ref 1024x10) [10240, bytes: 40960]
Variable_7:0 (float32_ref 10) [10, bytes: 40]
discriminator/d_conv1/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]
discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]
discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]
discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]
generator/g_fc1/Matrix:0 (float32_ref 74x1024) [75776, bytes: 303104]
generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]
generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]
generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]
classifier/c_fc1/Matrix:0 (float32_ref 1024x64) [65536, bytes: 262144]
classifier/c_fc1/bias:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/beta:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/gamma:0 (float32_ref 64) [64, bytes: 256]
classifier/c_fc2/Matrix:0 (float32_ref 64x12) [768, bytes: 3072]
classifier/c_fc2/bias:0 (float32_ref 12) [12, bytes: 48]
Total size of variables: 16552792
Total bytes of variables: 66211168
 [*] Reading checkpoints...
 [*] Success to read MultiModalInfoGAN.model-43721
 [*] Load SUCCESS


SAMPLES SIZE=4680,LABELS=299520


SAVED TRAINING SET generated_training_set_mnist_UniformSample_mu_0_sigma_0.15_czcc_rzcc_czrc_rzrc
 [*] Training finished!
 [*] Testing finished!
2018-06-15 17:51:46.327703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 17:51:46.327944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
model has been loaded from mnist_classifier.pkl
argmax:[0 0 0 ..., 0 0 0]
step 0: accuracy:0.0, confidence:0.9998022317886353, loss:16.037174224853516
Assinging:1
[10000]
argmax:[5 5 5 ..., 5 5 5]
step 0: accuracy:0.0, confidence:0.9976364970207214, loss:15.872801780700684
Assinging:6
[   0    0    0    4    0 9959   36    0    1]
argmax:[1 1 1 ..., 1 1 1]
step 0: accuracy:0.0, confidence:0.99969083070755, loss:15.047454833984375
Assinging:2
[    0 10000]
argmax:[8 8 8 ..., 8 8 8]
step 0: accuracy:0.0, confidence:0.9997566342353821, loss:17.477663040161133
Assinging:9
[    0     0     0     0     0     0     0     0 10000]
argmax:[3 3 3 ..., 3 3 3]
step 0: accuracy:0.0, confidence:0.9999160170555115, loss:15.061266899108887
Assinging:4
[    0     0     0 10000]
argmax:[4 4 4 ..., 4 4 4]
step 0: accuracy:0.0008999999845400453, confidence:0.9990870952606201, loss:16.04001235961914
Assinging:5
[   1    0    0    0 9990    0    0    0    0    9]
argmax:[9 9 9 ..., 9 9 9]
step 0: accuracy:0.9998999834060669, confidence:0.9993384480476379, loss:0.0007068286649882793
Assinging:10
[   0    0    0    0    0    0    0    1    0 9999]
argmax:[6 6 6 ..., 6 6 6]
step 0: accuracy:0.0, confidence:0.9995357394218445, loss:21.198984146118164
Assinging:7
[   1    0    0    0    0    0 9999]
argmax:[2 2 2 ..., 2 2 2]
step 0: accuracy:0.0, confidence:0.9985897541046143, loss:17.366241455078125
Assinging:3
[   0    0 9997    0    0    0    0    0    3]
argmax:[7 7 7 ..., 7 7 7]
step 0: accuracy:0.0, confidence:0.9999059438705444, loss:14.124401092529297
Assinging:8
[    0     0     0     0     0     0     0 10000]
2018-06-15 17:52:04.547554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 17:52:04.547795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
step 0: accuracy:0.09700000286102295, confidence:0.9917106032371521, loss:13.641884803771973
epoch0: step0/4680
step 0: accuracy:0.10199999809265137, confidence:1.0, loss:46.72455978393555
epoch0: step500/4680
step 0: accuracy:0.09099999815225601, confidence:0.9999899864196777, loss:17.276268005371094
epoch0: step1000/4680
step 0: accuracy:0.1120000034570694, confidence:0.9999998807907104, loss:18.101675033569336
epoch0: step1500/4680
step 0: accuracy:0.08299999684095383, confidence:0.9998682141304016, loss:10.965690612792969
epoch0: step2000/4680
step 0: accuracy:0.07999999821186066, confidence:0.9997813701629639, loss:10.469667434692383
epoch0: step2500/4680
step 0: accuracy:0.07900000363588333, confidence:0.9990324974060059, loss:8.819620132446289
epoch0: step3000/4680
step 0: accuracy:0.11299999803304672, confidence:0.9999465346336365, loss:11.986656188964844
epoch0: step3500/4680
step 0: accuracy:0.0949999988079071, confidence:0.9995356202125549, loss:10.390328407287598
epoch0: step4000/4680
step 0: accuracy:0.09700000286102295, confidence:0.9991869926452637, loss:8.981743812561035
epoch0: step4500/4680
step 0: accuracy:0.0860000029206276, confidence:0.9996514320373535, loss:10.025629997253418
epoch1: step0/4680
step 500: accuracy:0.09000000357627869, confidence:0.983284592628479, loss:6.183300495147705
epoch1: step500/4680
step 1000: accuracy:0.0820000022649765, confidence:0.9976344704627991, loss:10.060052871704102
epoch1: step1000/4680
step 1500: accuracy:0.11999999731779099, confidence:0.987916886806488, loss:6.701872825622559
epoch1: step1500/4680
step 2000: accuracy:0.10899999737739563, confidence:0.9999060034751892, loss:11.584285736083984
epoch1: step2000/4680
step 2500: accuracy:0.09799999743700027, confidence:0.9996806383132935, loss:9.955377578735352
epoch1: step2500/4680
step 3000: accuracy:0.10400000214576721, confidence:0.9999943971633911, loss:13.774863243103027
epoch1: step3000/4680
step 3500: accuracy:0.11599999666213989, confidence:0.9991227984428406, loss:8.537510871887207
epoch1: step3500/4680
step 4000: accuracy:0.08799999952316284, confidence:0.9993424415588379, loss:9.157882690429688
epoch1: step4000/4680
step 4500: accuracy:0.09200000017881393, confidence:0.9986701607704163, loss:8.281678199768066
epoch1: step4500/4680
step 0: accuracy:0.10899999737739563, confidence:0.9995070099830627, loss:8.967235565185547
epoch2: step0/4680
step 1000: accuracy:0.11999999731779099, confidence:0.9804825186729431, loss:5.732586860656738
epoch2: step500/4680
step 2000: accuracy:0.07599999755620956, confidence:0.9949539303779602, loss:8.239424705505371
epoch2: step1000/4680
step 3000: accuracy:0.10499999672174454, confidence:0.989150881767273, loss:6.911355495452881
epoch2: step1500/4680
step 4000: accuracy:0.0949999988079071, confidence:0.9992581009864807, loss:8.926786422729492
epoch2: step2000/4680
step 5000: accuracy:0.09799999743700027, confidence:0.9991908073425293, loss:8.70307445526123
epoch2: step2500/4680
step 6000: accuracy:0.0949999988079071, confidence:0.99881511926651, loss:8.325423240661621
epoch2: step3000/4680
step 7000: accuracy:0.09000000357627869, confidence:0.9996181726455688, loss:9.79355525970459
epoch2: step3500/4680
step 8000: accuracy:0.1080000028014183, confidence:0.9990410804748535, loss:8.423209190368652
epoch2: step4000/4680
step 9000: accuracy:0.0949999988079071, confidence:0.9984055161476135, loss:8.031685829162598
epoch2: step4500/4680
step 0: accuracy:0.11599999666213989, confidence:0.9993638396263123, loss:8.654875755310059
epoch3: step0/4680
step 1500: accuracy:0.12700000405311584, confidence:0.8191748261451721, loss:3.643193244934082
epoch3: step500/4680
step 3000: accuracy:0.11900000274181366, confidence:0.9851365685462952, loss:6.947640419006348
epoch3: step1000/4680
step 4500: accuracy:0.09700000286102295, confidence:0.9472077488899231, loss:5.668559551239014
epoch3: step1500/4680
step 6000: accuracy:0.08799999952316284, confidence:0.9680293798446655, loss:5.811831474304199
epoch3: step2000/4680
step 7500: accuracy:0.09700000286102295, confidence:0.9939396381378174, loss:6.9517436027526855
epoch3: step2500/4680
step 9000: accuracy:0.0989999994635582, confidence:0.9983010292053223, loss:8.305258750915527
epoch3: step3000/4680
step 10500: accuracy:0.0989999994635582, confidence:0.9965145587921143, loss:7.291110515594482
epoch3: step3500/4680
step 12000: accuracy:0.10499999672174454, confidence:0.9981253147125244, loss:7.545781135559082
epoch3: step4000/4680
step 13500: accuracy:0.09799999743700027, confidence:0.9965709447860718, loss:7.402312755584717
epoch3: step4500/4680
step 0: accuracy:0.09300000220537186, confidence:0.9986450672149658, loss:8.296222686767578
epoch4: step0/4680
step 2000: accuracy:0.1459999978542328, confidence:0.8429288268089294, loss:4.00382661819458
epoch4: step500/4680
step 4000: accuracy:0.11699999868869781, confidence:0.929301381111145, loss:4.921916961669922
epoch4: step1000/4680
step 6000: accuracy:0.1469999998807907, confidence:0.8581992387771606, loss:4.740385055541992
epoch4: step1500/4680
step 8000: accuracy:0.1589999943971634, confidence:0.9354708194732666, loss:4.609830379486084
epoch4: step2000/4680
step 10000: accuracy:0.08699999749660492, confidence:0.9776426553726196, loss:5.819990158081055
epoch4: step2500/4680
step 12000: accuracy:0.09600000083446503, confidence:0.986809253692627, loss:6.427279472351074
epoch4: step3000/4680
step 14000: accuracy:0.09799999743700027, confidence:0.9773288369178772, loss:5.395786285400391
epoch4: step3500/4680
step 16000: accuracy:0.10599999874830246, confidence:0.9905083775520325, loss:6.283451080322266
epoch4: step4000/4680
step 18000: accuracy:0.1080000028014183, confidence:0.9906450510025024, loss:6.507950305938721
epoch4: step4500/4680
step 0: accuracy:0.08500000089406967, confidence:0.9969004988670349, loss:7.582364082336426
epoch5: step0/4680
step 2500: accuracy:0.20200000703334808, confidence:0.8587907552719116, loss:4.658815383911133
epoch5: step500/4680
step 5000: accuracy:0.2160000056028366, confidence:0.8253630995750427, loss:3.169039011001587
epoch5: step1000/4680
step 7500: accuracy:0.19300000369548798, confidence:0.8190224170684814, loss:3.6058268547058105
epoch5: step1500/4680
step 10000: accuracy:0.20999999344348907, confidence:0.8764305710792542, loss:3.6530771255493164
epoch5: step2000/4680
step 12500: accuracy:0.09799999743700027, confidence:0.9463368058204651, loss:5.01414680480957
epoch5: step2500/4680
step 15000: accuracy:0.09099999815225601, confidence:0.9646278619766235, loss:5.580995559692383
epoch5: step3000/4680
step 17500: accuracy:0.10899999737739563, confidence:0.9580869674682617, loss:4.863516807556152
epoch5: step3500/4680
step 20000: accuracy:0.09700000286102295, confidence:0.9607733488082886, loss:5.512773036956787
epoch5: step4000/4680
step 22500: accuracy:0.10499999672174454, confidence:0.9261983633041382, loss:5.2811384201049805
epoch5: step4500/4680
step 0: accuracy:0.11500000208616257, confidence:0.9807571768760681, loss:6.54526424407959
epoch6: step0/4680
step 3000: accuracy:0.2290000021457672, confidence:0.8349935412406921, loss:3.6428065299987793
epoch6: step500/4680
step 6000: accuracy:0.27300000190734863, confidence:0.7439312934875488, loss:2.55273699760437
epoch6: step1000/4680
step 9000: accuracy:0.20100000500679016, confidence:0.8156052231788635, loss:3.368534564971924
epoch6: step1500/4680
step 12000: accuracy:0.24899999797344208, confidence:0.8160256147384644, loss:3.172635316848755
epoch6: step2000/4680
step 15000: accuracy:0.21799999475479126, confidence:0.8513591885566711, loss:3.467674732208252
epoch6: step2500/4680
step 18000: accuracy:0.13699999451637268, confidence:0.912796676158905, loss:5.147616863250732
epoch6: step3000/4680
step 21000: accuracy:0.19200000166893005, confidence:0.8269840478897095, loss:3.307468891143799
epoch6: step3500/4680
step 24000: accuracy:0.1340000033378601, confidence:0.9260711669921875, loss:5.003295421600342
epoch6: step4000/4680
step 27000: accuracy:0.1459999978542328, confidence:0.8807154893875122, loss:4.742001056671143
epoch6: step4500/4680
step 0: accuracy:0.125, confidence:0.9608504176139832, loss:6.051033020019531
epoch7: step0/4680
step 3500: accuracy:0.24199999868869781, confidence:0.8584842681884766, loss:3.8016133308410645
epoch7: step500/4680
step 7000: accuracy:0.29100000858306885, confidence:0.7594742178916931, loss:2.3492612838745117
epoch7: step1000/4680
step 10500: accuracy:0.21400000154972076, confidence:0.8503687381744385, loss:3.418301820755005
epoch7: step1500/4680
step 14000: accuracy:0.21699999272823334, confidence:0.8272346258163452, loss:3.180821180343628
epoch7: step2000/4680
step 17500: accuracy:0.2639999985694885, confidence:0.8407894968986511, loss:3.3079614639282227
epoch7: step2500/4680
step 21000: accuracy:0.20800000429153442, confidence:0.8830991387367249, loss:4.293428897857666
epoch7: step3000/4680
step 24500: accuracy:0.11900000274181366, confidence:0.8951648473739624, loss:4.371980667114258
epoch7: step3500/4680
step 28000: accuracy:0.11800000071525574, confidence:0.9210407137870789, loss:4.8729166984558105
epoch7: step4000/4680
step 31500: accuracy:0.1860000044107437, confidence:0.8659622669219971, loss:4.491514205932617
epoch7: step4500/4680
step 0: accuracy:0.1080000028014183, confidence:0.9483062028884888, loss:5.989105701446533
epoch8: step0/4680
step 4000: accuracy:0.24300000071525574, confidence:0.8423978686332703, loss:3.2915425300598145
epoch8: step500/4680
step 8000: accuracy:0.3240000009536743, confidence:0.7102952003479004, loss:2.150303363800049
epoch8: step1000/4680
step 12000: accuracy:0.22599999606609344, confidence:0.8209282755851746, loss:3.163522243499756
epoch8: step1500/4680
step 16000: accuracy:0.2590000033378601, confidence:0.824504017829895, loss:2.92568302154541
epoch8: step2000/4680
step 20000: accuracy:0.28700000047683716, confidence:0.8142088055610657, loss:2.8742198944091797
epoch8: step2500/4680
step 24000: accuracy:0.22200000286102295, confidence:0.8526391386985779, loss:4.0231757164001465
epoch8: step3000/4680
step 28000: accuracy:0.20200000703334808, confidence:0.8495853543281555, loss:3.720813035964966
epoch8: step3500/4680
step 32000: accuracy:0.11999999731779099, confidence:0.9266462326049805, loss:4.924769401550293
epoch8: step4000/4680
step 36000: accuracy:0.23199999332427979, confidence:0.8515634536743164, loss:3.72898530960083
epoch8: step4500/4680
step 0: accuracy:0.13500000536441803, confidence:0.9300380945205688, loss:5.291749954223633
epoch9: step0/4680
step 4500: accuracy:0.25099998712539673, confidence:0.8436392545700073, loss:3.753075122833252
epoch9: step500/4680
step 9000: accuracy:0.3499999940395355, confidence:0.7373432517051697, loss:2.0855817794799805
epoch9: step1000/4680
step 13500: accuracy:0.2759999930858612, confidence:0.8210816383361816, loss:2.943495512008667
epoch9: step1500/4680
step 18000: accuracy:0.26899999380111694, confidence:0.8251075148582458, loss:2.8236072063446045
epoch9: step2000/4680
step 22500: accuracy:0.25200000405311584, confidence:0.8198689222335815, loss:3.145500421524048
epoch9: step2500/4680
step 27000: accuracy:0.22499999403953552, confidence:0.8646907806396484, loss:3.970078468322754
epoch9: step3000/4680
step 31500: accuracy:0.2240000069141388, confidence:0.8203586339950562, loss:3.492130756378174
epoch9: step3500/4680
step 36000: accuracy:0.14900000393390656, confidence:0.9179587960243225, loss:4.751407623291016
epoch9: step4000/4680
step 40500: accuracy:0.2370000034570694, confidence:0.8428952693939209, loss:3.868675947189331
epoch9: step4500/4680
step 0: accuracy:0.1469999998807907, confidence:0.9154726266860962, loss:5.152426719665527
epoch10: step0/4680
step 5000: accuracy:0.26100000739097595, confidence:0.8114038109779358, loss:2.6751041412353516
epoch10: step500/4680
step 10000: accuracy:0.38199999928474426, confidence:0.6846365332603455, loss:1.9169542789459229
epoch10: step1000/4680
step 15000: accuracy:0.257999986410141, confidence:0.8117854595184326, loss:2.8925139904022217
epoch10: step1500/4680
step 20000: accuracy:0.32499998807907104, confidence:0.7898033261299133, loss:2.470008134841919
epoch10: step2000/4680
step 25000: accuracy:0.3540000021457672, confidence:0.7798972129821777, loss:2.3484270572662354
epoch10: step2500/4680
step 30000: accuracy:0.2280000001192093, confidence:0.8829213976860046, loss:4.27089262008667
epoch10: step3000/4680
step 35000: accuracy:0.28700000047683716, confidence:0.8200980424880981, loss:2.9007949829101562
epoch10: step3500/4680
step 40000: accuracy:0.13099999725818634, confidence:0.9239078164100647, loss:5.043299674987793
epoch10: step4000/4680
step 45000: accuracy:0.2770000100135803, confidence:0.8564184308052063, loss:3.474708080291748
epoch10: step4500/4680
step 0: accuracy:0.14800000190734863, confidence:0.9253736734390259, loss:5.031378269195557
epoch11: step0/4680
step 5500: accuracy:0.2540000081062317, confidence:0.8722891211509705, loss:4.0301408767700195
epoch11: step500/4680
step 11000: accuracy:0.29499998688697815, confidence:0.8293269872665405, loss:2.792079448699951
epoch11: step1000/4680
step 16500: accuracy:0.2840000092983246, confidence:0.810312032699585, loss:2.8343520164489746
epoch11: step1500/4680
step 22000: accuracy:0.2540000081062317, confidence:0.8107532858848572, loss:2.9339048862457275
epoch11: step2000/4680
step 27500: accuracy:0.2939999997615814, confidence:0.8136956691741943, loss:2.9412641525268555
epoch11: step2500/4680
step 33000: accuracy:0.24199999868869781, confidence:0.8758505582809448, loss:3.744504690170288
epoch11: step3000/4680
step 38500: accuracy:0.2529999911785126, confidence:0.8118432760238647, loss:3.4239866733551025
epoch11: step3500/4680
step 44000: accuracy:0.16099999845027924, confidence:0.9073258638381958, loss:4.952974796295166
epoch11: step4000/4680
step 49500: accuracy:0.2460000067949295, confidence:0.8579260110855103, loss:3.9060466289520264
epoch11: step4500/4680
step 0: accuracy:0.14399999380111694, confidence:0.9254387021064758, loss:5.521134853363037
epoch12: step0/4680
step 6000: accuracy:0.2680000066757202, confidence:0.7741954326629639, loss:2.4551541805267334
epoch12: step500/4680
step 12000: accuracy:0.29499998688697815, confidence:0.8008636236190796, loss:2.627223253250122
epoch12: step1000/4680
step 18000: accuracy:0.29600000381469727, confidence:0.822570264339447, loss:2.74107027053833
epoch12: step1500/4680
step 24000: accuracy:0.36899998784065247, confidence:0.8185040354728699, loss:2.5355989933013916
epoch12: step2000/4680
step 30000: accuracy:0.3199999928474426, confidence:0.7960013747215271, loss:2.588043212890625
epoch12: step2500/4680
step 36000: accuracy:0.25, confidence:0.8824508190155029, loss:4.119998931884766
epoch12: step3000/4680
step 42000: accuracy:0.3009999990463257, confidence:0.8067985773086548, loss:2.758693218231201
epoch12: step3500/4680
step 48000: accuracy:0.16699999570846558, confidence:0.9075559377670288, loss:4.479870796203613
epoch12: step4000/4680
step 54000: accuracy:0.289000004529953, confidence:0.8335522413253784, loss:3.3586130142211914
epoch12: step4500/4680
step 0: accuracy:0.15700000524520874, confidence:0.9110923409461975, loss:5.006600379943848
epoch13: step0/4680
step 6500: accuracy:0.2240000069141388, confidence:0.8826786279678345, loss:4.389116287231445
epoch13: step500/4680
step 13000: accuracy:0.26499998569488525, confidence:0.8966837525367737, loss:3.9111127853393555
epoch13: step1000/4680
step 19500: accuracy:0.25699999928474426, confidence:0.8421455025672913, loss:3.3687875270843506
epoch13: step1500/4680
step 26000: accuracy:0.23999999463558197, confidence:0.8975746035575867, loss:3.4509289264678955
epoch13: step2000/4680
step 32500: accuracy:0.2930000126361847, confidence:0.8142696619033813, loss:2.9877138137817383
epoch13: step2500/4680
step 39000: accuracy:0.2409999966621399, confidence:0.8789359331130981, loss:3.670964241027832
epoch13: step3000/4680
step 45500: accuracy:0.2840000092983246, confidence:0.79644775390625, loss:3.2131524085998535
epoch13: step3500/4680
step 52000: accuracy:0.19900000095367432, confidence:0.904184877872467, loss:4.9795098304748535
epoch13: step4000/4680
step 58500: accuracy:0.2280000001192093, confidence:0.8691967129707336, loss:3.897289514541626
epoch13: step4500/4680
step 0: accuracy:0.1340000033378601, confidence:0.9440876245498657, loss:5.5949788093566895
epoch14: step0/4680
step 7000: accuracy:0.20000000298023224, confidence:0.846011757850647, loss:3.244957208633423
epoch14: step500/4680
step 14000: accuracy:0.3269999921321869, confidence:0.7688285708427429, loss:2.4329588413238525
epoch14: step1000/4680
step 21000: accuracy:0.32100000977516174, confidence:0.7917627096176147, loss:2.3338067531585693
epoch14: step1500/4680
step 28000: accuracy:0.3310000002384186, confidence:0.8082430362701416, loss:2.7516729831695557
epoch14: step2000/4680
step 35000: accuracy:0.26499998569488525, confidence:0.8686926960945129, loss:3.5962963104248047
epoch14: step2500/4680
step 42000: accuracy:0.2529999911785126, confidence:0.8888553977012634, loss:4.37326192855835
epoch14: step3000/4680
step 49000: accuracy:0.28999999165534973, confidence:0.8268183469772339, loss:3.0259299278259277
epoch14: step3500/4680
step 56000: accuracy:0.23100000619888306, confidence:0.8844437599182129, loss:4.214918613433838
epoch14: step4000/4680
step 63000: accuracy:0.2770000100135803, confidence:0.8476937413215637, loss:3.9701108932495117
epoch14: step4500/4680
step 0: accuracy:0.18799999356269836, confidence:0.9157882928848267, loss:5.278723239898682
epoch15: step0/4680
step 7500: accuracy:0.21299999952316284, confidence:0.9114693403244019, loss:5.62248420715332
epoch15: step500/4680
step 15000: accuracy:0.27399998903274536, confidence:0.9047860503196716, loss:4.083600997924805
epoch15: step1000/4680
step 22500: accuracy:0.27000001072883606, confidence:0.8714579939842224, loss:3.636348247528076
epoch15: step1500/4680
step 30000: accuracy:0.2199999988079071, confidence:0.9074242115020752, loss:3.8387222290039062
epoch15: step2000/4680
step 37500: accuracy:0.335999995470047, confidence:0.8411831855773926, loss:3.1025502681732178
epoch15: step2500/4680
step 45000: accuracy:0.20100000500679016, confidence:0.8865715265274048, loss:4.522210597991943
epoch15: step3000/4680
step 52500: accuracy:0.3179999887943268, confidence:0.8238814473152161, loss:2.9974632263183594
epoch15: step3500/4680
step 60000: accuracy:0.22100000083446503, confidence:0.8996968269348145, loss:4.366395473480225
epoch15: step4000/4680
step 67500: accuracy:0.22499999403953552, confidence:0.8635079264640808, loss:3.9746696949005127
epoch15: step4500/4680
step 0: accuracy:0.16500000655651093, confidence:0.9258052110671997, loss:5.456989765167236
epoch16: step0/4680
step 8000: accuracy:0.30300000309944153, confidence:0.8131347894668579, loss:2.7513015270233154
epoch16: step500/4680
step 16000: accuracy:0.37299999594688416, confidence:0.7348673343658447, loss:2.141303777694702
epoch16: step1000/4680
step 24000: accuracy:0.3479999899864197, confidence:0.7511356472969055, loss:2.0821526050567627
epoch16: step1500/4680
step 32000: accuracy:0.35199999809265137, confidence:0.8311542868614197, loss:2.7152411937713623
epoch16: step2000/4680
step 40000: accuracy:0.3779999911785126, confidence:0.796370267868042, loss:2.325193166732788
epoch16: step2500/4680
step 48000: accuracy:0.27300000190734863, confidence:0.8821490406990051, loss:3.816373586654663
epoch16: step3000/4680
step 56000: accuracy:0.36399999260902405, confidence:0.7881526350975037, loss:2.251420259475708
epoch16: step3500/4680
step 64000: accuracy:0.2150000035762787, confidence:0.9105193614959717, loss:5.072779655456543
epoch16: step4000/4680
step 72000: accuracy:0.31700000166893005, confidence:0.8482953906059265, loss:3.261636257171631
epoch16: step4500/4680
step 0: accuracy:0.25200000405311584, confidence:0.8989315629005432, loss:4.173079967498779
epoch17: step0/4680
step 8500: accuracy:0.22699999809265137, confidence:0.8773776292800903, loss:4.267733573913574
epoch17: step500/4680
step 17000: accuracy:0.2849999964237213, confidence:0.9181387424468994, loss:4.1314191818237305
epoch17: step1000/4680
step 25500: accuracy:0.28600001335144043, confidence:0.899472713470459, loss:4.03726053237915
epoch17: step1500/4680
step 34000: accuracy:0.29100000858306885, confidence:0.884957492351532, loss:3.0510456562042236
epoch17: step2000/4680
step 42500: accuracy:0.3580000102519989, confidence:0.8020634055137634, loss:2.573282241821289
epoch17: step2500/4680
step 51000: accuracy:0.23100000619888306, confidence:0.8622124195098877, loss:3.951763868331909
epoch17: step3000/4680
step 59500: accuracy:0.28999999165534973, confidence:0.807716429233551, loss:2.8871986865997314
epoch17: step3500/4680
step 68000: accuracy:0.23199999332427979, confidence:0.8943624496459961, loss:4.298403739929199
epoch17: step4000/4680
step 76500: accuracy:0.2329999953508377, confidence:0.857535183429718, loss:3.6888692378997803
epoch17: step4500/4680
step 0: accuracy:0.1679999977350235, confidence:0.9221833944320679, loss:5.3216552734375
epoch18: step0/4680
step 9000: accuracy:0.2540000081062317, confidence:0.8324987888336182, loss:3.009312391281128
epoch18: step500/4680
step 18000: accuracy:0.35600000619888306, confidence:0.7397258281707764, loss:2.300171375274658
epoch18: step1000/4680
step 27000: accuracy:0.4020000100135803, confidence:0.7568457126617432, loss:2.008892059326172
epoch18: step1500/4680
step 36000: accuracy:0.33799999952316284, confidence:0.8170382976531982, loss:2.8153388500213623
epoch18: step2000/4680
step 45000: accuracy:0.35499998927116394, confidence:0.8051257729530334, loss:2.5968408584594727
epoch18: step2500/4680
step 54000: accuracy:0.24500000476837158, confidence:0.8671675324440002, loss:3.9820899963378906
epoch18: step3000/4680
step 63000: accuracy:0.3970000147819519, confidence:0.810256838798523, loss:2.2502808570861816
epoch18: step3500/4680
step 72000: accuracy:0.1770000010728836, confidence:0.9211522340774536, loss:5.449563503265381
epoch18: step4000/4680
step 81000: accuracy:0.29499998688697815, confidence:0.8656964898109436, loss:3.606473922729492
epoch18: step4500/4680
step 0: accuracy:0.19099999964237213, confidence:0.9106410145759583, loss:4.742178440093994
epoch19: step0/4680
step 9500: accuracy:0.2160000056028366, confidence:0.8927707672119141, loss:4.740592956542969
epoch19: step500/4680
step 19000: accuracy:0.28600001335144043, confidence:0.9164294600486755, loss:4.018971920013428
epoch19: step1000/4680
step 28500: accuracy:0.2460000067949295, confidence:0.8979094624519348, loss:4.247214317321777
epoch19: step1500/4680
step 38000: accuracy:0.2639999985694885, confidence:0.9075378775596619, loss:3.4138643741607666
epoch19: step2000/4680
step 47500: accuracy:0.3610000014305115, confidence:0.7873619198799133, loss:2.3894712924957275
epoch19: step2500/4680
step 57000: accuracy:0.2669999897480011, confidence:0.860397219657898, loss:3.8285512924194336
epoch19: step3000/4680
step 66500: accuracy:0.24500000476837158, confidence:0.8445484042167664, loss:3.449819326400757
epoch19: step3500/4680
step 76000: accuracy:0.19599999487400055, confidence:0.9091789722442627, loss:4.600830078125
epoch19: step4000/4680
step 85500: accuracy:0.22100000083446503, confidence:0.8747799396514893, loss:3.8142693042755127
epoch19: step4500/4680
step 0: accuracy:0.13500000536441803, confidence:0.942398190498352, loss:5.488394737243652
epoch20: step0/4680
step 10000: accuracy:0.34599998593330383, confidence:0.7946589589118958, loss:2.5389585494995117
epoch20: step500/4680
step 20000: accuracy:0.33500000834465027, confidence:0.7793939709663391, loss:2.502467632293701
epoch20: step1000/4680
step 30000: accuracy:0.40700000524520874, confidence:0.7479957938194275, loss:1.9773343801498413
epoch20: step1500/4680
step 40000: accuracy:0.33500000834465027, confidence:0.8289428353309631, loss:2.927036762237549
epoch20: step2000/4680
step 50000: accuracy:0.41999998688697815, confidence:0.8103630542755127, loss:2.1997933387756348
epoch20: step2500/4680
step 60000: accuracy:0.28999999165534973, confidence:0.8718860149383545, loss:3.5583481788635254
epoch20: step3000/4680
step 70000: accuracy:0.38199999928474426, confidence:0.8285651803016663, loss:2.4467170238494873
epoch20: step3500/4680
step 80000: accuracy:0.23899999260902405, confidence:0.9076284766197205, loss:4.822539329528809
epoch20: step4000/4680
step 90000: accuracy:0.3199999928474426, confidence:0.8616359233856201, loss:3.4425313472747803
epoch20: step4500/4680
step 0: accuracy:0.22200000286102295, confidence:0.9017966985702515, loss:4.611987113952637
epoch21: step0/4680
step 10500: accuracy:0.29499998688697815, confidence:0.8503018021583557, loss:3.5095715522766113
epoch21: step500/4680
step 21000: accuracy:0.26600000262260437, confidence:0.9286115169525146, loss:4.567667007446289
epoch21: step1000/4680
step 31500: accuracy:0.28600001335144043, confidence:0.896300196647644, loss:4.118741989135742
epoch21: step1500/4680
step 42000: accuracy:0.23999999463558197, confidence:0.8796308636665344, loss:3.186073064804077
epoch21: step2000/4680
step 52500: accuracy:0.35100001096725464, confidence:0.7839379906654358, loss:2.3740906715393066
epoch21: step2500/4680
step 63000: accuracy:0.28299999237060547, confidence:0.8598122000694275, loss:3.5782880783081055
epoch21: step3000/4680
step 73500: accuracy:0.25600001215934753, confidence:0.8142570853233337, loss:3.127187967300415
epoch21: step3500/4680
step 84000: accuracy:0.2549999952316284, confidence:0.8878999352455139, loss:4.015740871429443
epoch21: step4000/4680
step 94500: accuracy:0.2720000147819519, confidence:0.8415366411209106, loss:3.422166347503662
epoch21: step4500/4680
step 0: accuracy:0.20800000429153442, confidence:0.9066677093505859, loss:4.649114608764648
epoch22: step0/4680
step 11000: accuracy:0.32199999690055847, confidence:0.7933903932571411, loss:2.424834728240967
epoch22: step500/4680
step 22000: accuracy:0.35600000619888306, confidence:0.7404832243919373, loss:2.187790870666504
epoch22: step1000/4680
step 33000: accuracy:0.41999998688697815, confidence:0.7646405100822449, loss:1.9377566576004028
epoch22: step1500/4680
step 44000: accuracy:0.3709999918937683, confidence:0.8073597550392151, loss:2.5832109451293945
epoch22: step2000/4680
step 55000: accuracy:0.43299999833106995, confidence:0.7967270612716675, loss:2.191800594329834
epoch22: step2500/4680
step 66000: accuracy:0.3059999942779541, confidence:0.855053186416626, loss:3.178952217102051
epoch22: step3000/4680
step 77000: accuracy:0.47099998593330383, confidence:0.791379988193512, loss:1.8602912425994873
epoch22: step3500/4680
step 88000: accuracy:0.2029999941587448, confidence:0.9195340871810913, loss:5.251957893371582
epoch22: step4000/4680
step 99000: accuracy:0.3140000104904175, confidence:0.8579058051109314, loss:3.2718536853790283
epoch22: step4500/4680
step 0: accuracy:0.2150000035762787, confidence:0.8969448208808899, loss:4.537694931030273
epoch23: step0/4680
step 11500: accuracy:0.28299999237060547, confidence:0.8719495534896851, loss:3.633150815963745
epoch23: step500/4680
step 23000: accuracy:0.2770000100135803, confidence:0.9106745719909668, loss:4.097714900970459
epoch23: step1000/4680
step 34500: accuracy:0.27799999713897705, confidence:0.8941112160682678, loss:3.870332956314087
epoch23: step1500/4680
step 46000: accuracy:0.3009999990463257, confidence:0.8679677248001099, loss:2.798576593399048
epoch23: step2000/4680
step 57500: accuracy:0.3930000066757202, confidence:0.7850515842437744, loss:2.214066982269287
epoch23: step2500/4680
step 69000: accuracy:0.29499998688697815, confidence:0.8531213402748108, loss:3.651014566421509
epoch23: step3000/4680
step 80500: accuracy:0.25600001215934753, confidence:0.8322014212608337, loss:3.147366523742676
epoch23: step3500/4680
step 92000: accuracy:0.20499999821186066, confidence:0.9103823900222778, loss:4.590320110321045
epoch23: step4000/4680
step 103500: accuracy:0.21299999952316284, confidence:0.8668185472488403, loss:3.7654786109924316
epoch23: step4500/4680
step 0: accuracy:0.1720000058412552, confidence:0.9306256771087646, loss:5.068798542022705
epoch24: step0/4680
step 12000: accuracy:0.33399999141693115, confidence:0.7723352313041687, loss:2.3007712364196777
epoch24: step500/4680
step 24000: accuracy:0.38499999046325684, confidence:0.747038722038269, loss:2.0312790870666504
epoch24: step1000/4680
step 36000: accuracy:0.47600001096725464, confidence:0.764868974685669, loss:1.7849059104919434
epoch24: step1500/4680
step 48000: accuracy:0.33899998664855957, confidence:0.8346176743507385, loss:2.797231674194336
epoch24: step2000/4680
step 60000: accuracy:0.45500001311302185, confidence:0.7960950136184692, loss:2.055347442626953
epoch24: step2500/4680
step 72000: accuracy:0.2930000126361847, confidence:0.8532443642616272, loss:3.1118578910827637
epoch24: step3000/4680
step 84000: accuracy:0.4490000009536743, confidence:0.8048281669616699, loss:1.9605962038040161
epoch24: step3500/4680
step 96000: accuracy:0.2720000147819519, confidence:0.8995505571365356, loss:4.429376125335693
epoch24: step4000/4680
step 108000: accuracy:0.3240000009536743, confidence:0.8596897125244141, loss:3.476024866104126
epoch24: step4500/4680
step 0: accuracy:0.20100000500679016, confidence:0.905559241771698, loss:4.764530658721924
epoch25: step0/4680
step 12500: accuracy:0.2709999978542328, confidence:0.8872470855712891, loss:4.09311056137085
epoch25: step500/4680
step 25000: accuracy:0.28600001335144043, confidence:0.8848637938499451, loss:3.3982181549072266
epoch25: step1000/4680
step 37500: accuracy:0.26899999380111694, confidence:0.8678687810897827, loss:3.5166006088256836
epoch25: step1500/4680
step 50000: accuracy:0.29600000381469727, confidence:0.8814910650253296, loss:2.9801108837127686
epoch25: step2000/4680
step 62500: accuracy:0.4009999930858612, confidence:0.7674147486686707, loss:2.066312551498413
epoch25: step2500/4680
step 75000: accuracy:0.28299999237060547, confidence:0.8485615849494934, loss:3.4645495414733887
epoch25: step3000/4680
step 87500: accuracy:0.27300000190734863, confidence:0.8195749521255493, loss:2.960484504699707
epoch25: step3500/4680
step 100000: accuracy:0.23100000619888306, confidence:0.8939037919044495, loss:4.1289215087890625
epoch25: step4000/4680
step 112500: accuracy:0.19900000095367432, confidence:0.8869560956954956, loss:4.07221794128418
epoch25: step4500/4680
step 0: accuracy:0.12800000607967377, confidence:0.9536034464836121, loss:5.664849281311035
epoch26: step0/4680
step 13000: accuracy:0.2639999985694885, confidence:0.8166762590408325, loss:2.831214427947998
epoch26: step500/4680
step 26000: accuracy:0.382999986410141, confidence:0.7804163098335266, loss:2.2063724994659424
epoch26: step1000/4680
step 39000: accuracy:0.3970000147819519, confidence:0.7738568782806396, loss:2.0628597736358643
epoch26: step1500/4680
step 52000: accuracy:0.34700000286102295, confidence:0.8349165320396423, loss:2.80777645111084
epoch26: step2000/4680
step 65000: accuracy:0.3880000114440918, confidence:0.8008893728256226, loss:2.2347412109375
epoch26: step2500/4680
step 78000: accuracy:0.2879999876022339, confidence:0.8612912893295288, loss:3.250544548034668
epoch26: step3000/4680
step 91000: accuracy:0.3659999966621399, confidence:0.8029775619506836, loss:2.450481653213501
epoch26: step3500/4680
step 104000: accuracy:0.24400000274181366, confidence:0.9069868922233582, loss:4.687394618988037
epoch26: step4000/4680
step 117000: accuracy:0.29100000858306885, confidence:0.8538522124290466, loss:3.7554359436035156
epoch26: step4500/4680
step 0: accuracy:0.1720000058412552, confidence:0.9155604243278503, loss:5.182341575622559
epoch27: step0/4680
step 13500: accuracy:0.21299999952316284, confidence:0.9134830236434937, loss:6.2660908699035645
epoch27: step500/4680
step 27000: accuracy:0.25699999928474426, confidence:0.9242077469825745, loss:4.598083019256592
epoch27: step1000/4680
step 40500: accuracy:0.2329999953508377, confidence:0.9163827300071716, loss:4.59277868270874
epoch27: step1500/4680
step 54000: accuracy:0.23899999260902405, confidence:0.8901846408843994, loss:3.3736090660095215
epoch27: step2000/4680
step 67500: accuracy:0.3720000088214874, confidence:0.7818501591682434, loss:2.1859378814697266
epoch27: step2500/4680
step 81000: accuracy:0.24199999868869781, confidence:0.8661039471626282, loss:3.606308698654175
epoch27: step3000/4680
step 94500: accuracy:0.24799999594688416, confidence:0.8400602340698242, loss:3.3155288696289062
epoch27: step3500/4680
step 108000: accuracy:0.23600000143051147, confidence:0.9114595055580139, loss:4.1975297927856445
epoch27: step4000/4680
step 121500: accuracy:0.22699999809265137, confidence:0.8869671821594238, loss:3.734968423843384
epoch27: step4500/4680
step 0: accuracy:0.1509999930858612, confidence:0.9418860077857971, loss:5.282088279724121
epoch28: step0/4680
step 14000: accuracy:0.35499998927116394, confidence:0.793238639831543, loss:2.3995449542999268
epoch28: step500/4680
step 28000: accuracy:0.31700000166893005, confidence:0.7868018746376038, loss:2.6870312690734863
epoch28: step1000/4680
step 42000: accuracy:0.4909999966621399, confidence:0.7514216303825378, loss:1.6302471160888672
epoch28: step1500/4680
step 56000: accuracy:0.30000001192092896, confidence:0.8352596759796143, loss:3.359814167022705
epoch28: step2000/4680
step 70000: accuracy:0.4009999930858612, confidence:0.7998661398887634, loss:2.2940564155578613
epoch28: step2500/4680
step 84000: accuracy:0.28999999165534973, confidence:0.8683975338935852, loss:3.6156651973724365
epoch28: step3000/4680
step 98000: accuracy:0.4099999964237213, confidence:0.8000222444534302, loss:2.1884820461273193
epoch28: step3500/4680
step 112000: accuracy:0.2849999964237213, confidence:0.9058996438980103, loss:4.667867183685303
epoch28: step4000/4680
step 126000: accuracy:0.31700000166893005, confidence:0.8631953597068787, loss:3.4812724590301514
epoch28: step4500/4680
step 0: accuracy:0.23100000619888306, confidence:0.9000230431556702, loss:4.597522735595703
epoch29: step0/4680
step 14500: accuracy:0.22499999403953552, confidence:0.8891236782073975, loss:4.936878681182861
epoch29: step500/4680
step 29000: accuracy:0.34700000286102295, confidence:0.8496087789535522, loss:2.7119810581207275
epoch29: step1000/4680
step 43500: accuracy:0.2759999930858612, confidence:0.8990562558174133, loss:3.8073947429656982
epoch29: step1500/4680
step 58000: accuracy:0.32100000977516174, confidence:0.857197642326355, loss:2.7344248294830322
epoch29: step2000/4680
step 72500: accuracy:0.43700000643730164, confidence:0.7412824630737305, loss:1.782285451889038
epoch29: step2500/4680
step 87000: accuracy:0.23000000417232513, confidence:0.8631089925765991, loss:3.6377477645874023
epoch29: step3000/4680
step 101500: accuracy:0.23399999737739563, confidence:0.8377685546875, loss:3.3477518558502197
epoch29: step3500/4680
step 116000: accuracy:0.24199999868869781, confidence:0.8882149457931519, loss:3.8167009353637695
epoch29: step4000/4680
step 130500: accuracy:0.23600000143051147, confidence:0.8788911700248718, loss:3.5877060890197754
epoch29: step4500/4680
2018-06-15 18:02:25.749605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 18:02:25.749854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
2018-06-15 18:02:27.916789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
CHEKPOINT DIR: uniform
model has been loaded from mnist_classifier.pkl
---------
Variables: name (type shape) [size]
---------
Variable:0 (float32_ref 5x5x1x32) [800, bytes: 3200]
Variable_1:0 (float32_ref 32) [32, bytes: 128]
Variable_2:0 (float32_ref 5x5x32x64) [51200, bytes: 204800]
Variable_3:0 (float32_ref 64) [64, bytes: 256]
Variable_4:0 (float32_ref 3136x1024) [3211264, bytes: 12845056]
Variable_5:0 (float32_ref 1024) [1024, bytes: 4096]
Variable_6:0 (float32_ref 1024x10) [10240, bytes: 40960]
Variable_7:0 (float32_ref 10) [10, bytes: 40]
discriminator/d_conv1/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]
discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]
discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]
discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]
generator/g_fc1/Matrix:0 (float32_ref 74x1024) [75776, bytes: 303104]
generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]
generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]
generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]
classifier/c_fc1/Matrix:0 (float32_ref 1024x64) [65536, bytes: 262144]
classifier/c_fc1/bias:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/beta:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/gamma:0 (float32_ref 64) [64, bytes: 256]
classifier/c_fc2/Matrix:0 (float32_ref 64x12) [768, bytes: 3072]
classifier/c_fc2/bias:0 (float32_ref 12) [12, bytes: 48]
Total size of variables: 16552792
Total bytes of variables: 66211168
 [*] Reading checkpoints...
 [*] Success to read MultiModalInfoGAN.model-43721
 [*] Load SUCCESS


SAMPLES SIZE=4680,LABELS=299520


SAVED TRAINING SET generated_training_set_mnist_UniformSample_mu_0_sigma_0.15_rzcc_rzrc_czcc_czrc
 [*] Training finished!
 [*] Testing finished!
2018-06-15 18:03:11.642863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 18:03:11.643112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
model has been loaded from mnist_classifier.pkl
argmax:[1 1 1 ..., 1 1 1]
step 0: accuracy:0.0, confidence:0.998964250087738, loss:14.234450340270996
Assinging:2
[    0 10000]
argmax:[8 8 8 ..., 8 8 8]
step 0: accuracy:9.999999747378752e-05, confidence:0.9962821006774902, loss:14.078629493713379
Assinging:9
[   0    7    1   20    0    0    1    0 9970    1]
argmax:[5 3 5 ..., 3 3 5]
step 0: accuracy:0.0015999999595806003, confidence:0.9588294625282288, loss:11.314593315124512
Assinging:4
[   0   21    6 5638    0 3895   30    1  393   16]
argmax:[6 6 6 ..., 6 6 6]
step 0: accuracy:0.0, confidence:0.9971836805343628, loss:19.124378204345703
Assinging:7
[   4    4    1    0    0    0 9991]
argmax:[7 7 7 ..., 7 7 7]
step 0: accuracy:9.999999747378752e-05, confidence:0.9975576996803284, loss:11.977566719055176
Assinging:8
[   1    6    0    1    0    0    0 9990    1    1]
argmax:[9 9 0 ..., 9 9 9]
step 0: accuracy:0.4927999973297119, confidence:0.9656084775924683, loss:4.456563472747803
Assinging:5
[  30    2    0    0 5008    0    1   15   16 4928]
argmax:[2 2 2 ..., 2 2 2]
step 0: accuracy:0.0, confidence:0.9954068064689636, loss:19.654895782470703
Assinging:3
[   0   11 9975    0    0    0    0    6    8]
argmax:[9 4 4 ..., 9 9 9]
step 0: accuracy:0.7405999898910522, confidence:0.9674437642097473, loss:2.053767442703247
Assinging:10
[   0   61    0    0 2511    0    0   10   12 7406]
argmax:[0 0 0 ..., 0 0 0]
step 0: accuracy:0.0, confidence:0.9997186660766602, loss:16.891040802001953
Assinging:1
[10000]
argmax:[5 5 3 ..., 3 3 5]
step 0: accuracy:0.0013000000035390258, confidence:0.9315531253814697, loss:8.664572715759277
Assinging:4
[   6    0    4 5294    1 4664    5    2   11   13]
2018-06-15 18:03:30.550105: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 18:03:30.550340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
step 0: accuracy:0.10100000351667404, confidence:0.935455858707428, loss:11.347689628601074
epoch0: step0/4680
step 0: accuracy:0.11800000071525574, confidence:1.0, loss:42.2789192199707
epoch0: step500/4680
step 0: accuracy:0.08900000154972076, confidence:1.0, loss:25.20575523376465
epoch0: step1000/4680
step 0: accuracy:0.09799999743700027, confidence:0.99998939037323, loss:12.781562805175781
epoch0: step1500/4680
step 0: accuracy:0.09600000083446503, confidence:0.999933123588562, loss:11.238414764404297
epoch0: step2000/4680
step 0: accuracy:0.09099999815225601, confidence:0.9995983839035034, loss:11.476069450378418
epoch0: step2500/4680
step 0: accuracy:0.07900000363588333, confidence:0.999577522277832, loss:9.946118354797363
epoch0: step3000/4680
step 0: accuracy:0.1080000028014183, confidence:0.9999750852584839, loss:14.075117111206055
epoch0: step3500/4680
step 0: accuracy:0.10199999809265137, confidence:0.9998936057090759, loss:12.053218841552734
epoch0: step4000/4680
step 0: accuracy:0.08500000089406967, confidence:0.9988698959350586, loss:8.557008743286133
epoch0: step4500/4680
step 0: accuracy:0.08299999684095383, confidence:0.9991791844367981, loss:8.767078399658203
epoch1: step0/4680
step 500: accuracy:0.09700000286102295, confidence:0.9999464750289917, loss:11.473953247070312
epoch1: step500/4680
step 1000: accuracy:0.09600000083446503, confidence:0.9996427893638611, loss:9.580565452575684
epoch1: step1000/4680
step 1500: accuracy:0.10100000351667404, confidence:0.9991414546966553, loss:9.535250663757324
epoch1: step1500/4680
step 2000: accuracy:0.08699999749660492, confidence:0.9997631907463074, loss:10.549723625183105
epoch1: step2000/4680
step 2500: accuracy:0.08299999684095383, confidence:0.999910295009613, loss:11.54354190826416
epoch1: step2500/4680
step 3000: accuracy:0.10400000214576721, confidence:0.9997455477714539, loss:9.82192325592041
epoch1: step3000/4680
step 3500: accuracy:0.10499999672174454, confidence:0.9997396469116211, loss:10.006153106689453
epoch1: step3500/4680
step 4000: accuracy:0.10100000351667404, confidence:0.9999718070030212, loss:12.170336723327637
epoch1: step4000/4680
step 4500: accuracy:0.09000000357627869, confidence:0.9991912841796875, loss:8.947073936462402
epoch1: step4500/4680
step 0: accuracy:0.10100000351667404, confidence:0.9994621872901917, loss:9.094695091247559
epoch2: step0/4680
step 1000: accuracy:0.08399999886751175, confidence:0.998395562171936, loss:8.300993919372559
epoch2: step500/4680
step 2000: accuracy:0.09799999743700027, confidence:0.9999607801437378, loss:11.496408462524414
epoch2: step1000/4680
step 3000: accuracy:0.10000000149011612, confidence:0.9999801516532898, loss:14.097380638122559
epoch2: step1500/4680
step 4000: accuracy:0.10999999940395355, confidence:0.9995697140693665, loss:10.064740180969238
epoch2: step2000/4680
step 5000: accuracy:0.0860000029206276, confidence:0.9999525547027588, loss:11.661907196044922
epoch2: step2500/4680
step 6000: accuracy:0.0949999988079071, confidence:0.9996041655540466, loss:9.3814058303833
epoch2: step3000/4680
step 7000: accuracy:0.09399999678134918, confidence:0.9991570115089417, loss:9.072432518005371
epoch2: step3500/4680
step 8000: accuracy:0.08399999886751175, confidence:0.9999819397926331, loss:12.563060760498047
epoch2: step4000/4680
step 9000: accuracy:0.09700000286102295, confidence:0.999272882938385, loss:8.890186309814453
epoch2: step4500/4680
step 0: accuracy:0.09799999743700027, confidence:0.9993876218795776, loss:9.003257751464844
epoch3: step0/4680
step 1500: accuracy:0.13699999451637268, confidence:0.9997721910476685, loss:9.528891563415527
epoch3: step500/4680
step 3000: accuracy:0.10000000149011612, confidence:0.9999709725379944, loss:11.72044563293457
epoch3: step1000/4680
step 4500: accuracy:0.09799999743700027, confidence:0.9998860359191895, loss:12.232280731201172
epoch3: step1500/4680
step 6000: accuracy:0.08699999749660492, confidence:0.9992477297782898, loss:9.672870635986328
epoch3: step2000/4680
step 7500: accuracy:0.1120000034570694, confidence:0.9997258186340332, loss:9.66664981842041
epoch3: step2500/4680
step 9000: accuracy:0.0989999994635582, confidence:0.9995256066322327, loss:9.51655101776123
epoch3: step3000/4680
step 10500: accuracy:0.08699999749660492, confidence:0.9981403350830078, loss:8.367147445678711
epoch3: step3500/4680
step 12000: accuracy:0.09000000357627869, confidence:0.9999315142631531, loss:11.120224952697754
epoch3: step4000/4680
step 13500: accuracy:0.10499999672174454, confidence:0.9969231486320496, loss:7.363864421844482
epoch3: step4500/4680
step 0: accuracy:0.10000000149011612, confidence:0.9979329705238342, loss:7.726211071014404
epoch4: step0/4680
step 2000: accuracy:0.12999999523162842, confidence:0.9935406446456909, loss:6.7093706130981445
epoch4: step500/4680
step 4000: accuracy:0.10300000011920929, confidence:0.9996169209480286, loss:9.177461624145508
epoch4: step1000/4680
step 6000: accuracy:0.09000000357627869, confidence:0.9999278783798218, loss:12.159116744995117
epoch4: step1500/4680
step 8000: accuracy:0.0989999994635582, confidence:0.9974522590637207, loss:8.379775047302246
epoch4: step2000/4680
step 10000: accuracy:0.11599999666213989, confidence:0.9994439482688904, loss:9.150888442993164
epoch4: step2500/4680
step 12000: accuracy:0.09600000083446503, confidence:0.9995332956314087, loss:9.518445014953613
epoch4: step3000/4680
step 14000: accuracy:0.08900000154972076, confidence:0.9974092841148376, loss:8.016437530517578
epoch4: step3500/4680
step 16000: accuracy:0.10199999809265137, confidence:0.9994910955429077, loss:9.195230484008789
epoch4: step4000/4680
step 18000: accuracy:0.11100000143051147, confidence:0.9972991943359375, loss:7.563064098358154
epoch4: step4500/4680
step 0: accuracy:0.09600000083446503, confidence:0.9977937936782837, loss:7.852323055267334
epoch5: step0/4680
step 2500: accuracy:0.0949999988079071, confidence:0.9925264716148376, loss:6.857313632965088
epoch5: step500/4680
step 5000: accuracy:0.10899999737739563, confidence:0.9996297955513, loss:9.380535125732422
epoch5: step1000/4680
step 7500: accuracy:0.12700000405311584, confidence:0.9988943934440613, loss:8.727924346923828
epoch5: step1500/4680
step 10000: accuracy:0.09099999815225601, confidence:0.9955199956893921, loss:7.661204814910889
epoch5: step2000/4680
step 12500: accuracy:0.09399999678134918, confidence:0.998727560043335, loss:9.017786979675293
epoch5: step2500/4680
step 15000: accuracy:0.0860000029206276, confidence:0.9987664222717285, loss:9.28604507446289
epoch5: step3000/4680
step 17500: accuracy:0.11999999731779099, confidence:0.9956846833229065, loss:7.2912445068359375
epoch5: step3500/4680
step 20000: accuracy:0.0860000029206276, confidence:0.9996545314788818, loss:9.825427055358887
epoch5: step4000/4680
step 22500: accuracy:0.10199999809265137, confidence:0.9947986006736755, loss:7.11783504486084
epoch5: step4500/4680
step 0: accuracy:0.10100000351667404, confidence:0.9962650537490845, loss:7.284191608428955
epoch6: step0/4680
step 3000: accuracy:0.10700000077486038, confidence:0.9790003299713135, loss:6.032111644744873
epoch6: step500/4680
step 6000: accuracy:0.0860000029206276, confidence:0.9996488094329834, loss:9.846631050109863
epoch6: step1000/4680
step 9000: accuracy:0.10400000214576721, confidence:0.9961424469947815, loss:8.25748062133789
epoch6: step1500/4680
step 12000: accuracy:0.09399999678134918, confidence:0.9777824878692627, loss:6.1919403076171875
epoch6: step2000/4680
step 15000: accuracy:0.10100000351667404, confidence:0.9987882971763611, loss:8.765453338623047
epoch6: step2500/4680
step 18000: accuracy:0.09399999678134918, confidence:0.9988563656806946, loss:9.55777645111084
epoch6: step3000/4680
step 21000: accuracy:0.09000000357627869, confidence:0.9896684288978577, loss:6.9390740394592285
epoch6: step3500/4680
step 24000: accuracy:0.08900000154972076, confidence:0.9995126724243164, loss:9.571272850036621
epoch6: step4000/4680
step 27000: accuracy:0.10999999940395355, confidence:0.9926623106002808, loss:6.712409496307373
epoch6: step4500/4680
step 0: accuracy:0.10700000077486038, confidence:0.9943642020225525, loss:6.865918159484863
epoch7: step0/4680
step 3500: accuracy:0.11900000274181366, confidence:0.9554842114448547, loss:5.48527717590332
epoch7: step500/4680
step 7000: accuracy:0.09799999743700027, confidence:0.9978206157684326, loss:7.892349720001221
epoch7: step1000/4680
step 10500: accuracy:0.10199999809265137, confidence:0.9943317770957947, loss:7.90542459487915
epoch7: step1500/4680
step 14000: accuracy:0.09700000286102295, confidence:0.9919015169143677, loss:7.345980644226074
epoch7: step2000/4680
step 17500: accuracy:0.1080000028014183, confidence:0.9945345520973206, loss:7.014092445373535
epoch7: step2500/4680
step 21000: accuracy:0.10899999737739563, confidence:0.9984822273254395, loss:8.663232803344727
epoch7: step3000/4680
step 24500: accuracy:0.10499999672174454, confidence:0.9678462743759155, loss:5.894440650939941
epoch7: step3500/4680
step 28000: accuracy:0.09099999815225601, confidence:0.9985501766204834, loss:8.743412971496582
epoch7: step4000/4680
step 31500: accuracy:0.10000000149011612, confidence:0.9876308441162109, loss:6.401880264282227
epoch7: step4500/4680
step 0: accuracy:0.09399999678134918, confidence:0.9895890355110168, loss:6.641870498657227
epoch8: step0/4680
step 4000: accuracy:0.11699999868869781, confidence:0.8788852095603943, loss:4.833585739135742
epoch8: step500/4680
step 8000: accuracy:0.10000000149011612, confidence:0.9956670999526978, loss:7.987301349639893
epoch8: step1000/4680
step 12000: accuracy:0.10700000077486038, confidence:0.9990569949150085, loss:9.090755462646484
epoch8: step1500/4680
step 16000: accuracy:0.1080000028014183, confidence:0.9685850143432617, loss:5.8611159324646
epoch8: step2000/4680
step 20000: accuracy:0.1340000033378601, confidence:0.9516389966011047, loss:6.332892417907715
epoch8: step2500/4680
step 24000: accuracy:0.0820000022649765, confidence:0.9970221519470215, loss:8.35474681854248
epoch8: step3000/4680
step 28000: accuracy:0.09099999815225601, confidence:0.9266440868377686, loss:4.984663009643555
epoch8: step3500/4680
step 32000: accuracy:0.10400000214576721, confidence:0.9744897484779358, loss:7.160221099853516
epoch8: step4000/4680
step 36000: accuracy:0.10999999940395355, confidence:0.9844087958335876, loss:6.536103248596191
epoch8: step4500/4680
step 0: accuracy:0.09200000017881393, confidence:0.983263373374939, loss:6.504330635070801
epoch9: step0/4680
step 4500: accuracy:0.12300000339746475, confidence:0.873643159866333, loss:4.583433151245117
epoch9: step500/4680
step 9000: accuracy:0.09799999743700027, confidence:0.994385302066803, loss:7.586658000946045
epoch9: step1000/4680
step 13500: accuracy:0.11400000005960464, confidence:0.9828401803970337, loss:7.030796051025391
epoch9: step1500/4680
step 18000: accuracy:0.11800000071525574, confidence:0.8895447850227356, loss:4.591371059417725
epoch9: step2000/4680
step 22500: accuracy:0.1469999998807907, confidence:0.9483519196510315, loss:7.498854160308838
epoch9: step2500/4680
step 27000: accuracy:0.09200000017881393, confidence:0.9968244433403015, loss:8.352126121520996
epoch9: step3000/4680
step 31500: accuracy:0.10599999874830246, confidence:0.9441567063331604, loss:5.090944290161133
epoch9: step3500/4680
step 36000: accuracy:0.0989999994635582, confidence:0.9567885994911194, loss:6.336848258972168
epoch9: step4000/4680
step 40500: accuracy:0.10400000214576721, confidence:0.9504731297492981, loss:6.111428260803223
epoch9: step4500/4680
step 0: accuracy:0.09600000083446503, confidence:0.9519234895706177, loss:6.104943752288818
epoch10: step0/4680
step 5000: accuracy:0.13500000536441803, confidence:0.9112196564674377, loss:5.017958641052246
epoch10: step500/4680
step 10000: accuracy:0.10100000351667404, confidence:0.9912768602371216, loss:7.639840126037598
epoch10: step1000/4680
step 15000: accuracy:0.08100000023841858, confidence:0.9555414915084839, loss:6.67122220993042
epoch10: step1500/4680
step 20000: accuracy:0.11800000071525574, confidence:0.9320652484893799, loss:5.128910064697266
epoch10: step2000/4680
step 25000: accuracy:0.14499999582767487, confidence:0.9555138349533081, loss:6.397939682006836
epoch10: step2500/4680
step 30000: accuracy:0.0989999994635582, confidence:0.9942963123321533, loss:7.323769569396973
epoch10: step3000/4680
step 35000: accuracy:0.09700000286102295, confidence:0.9157432317733765, loss:4.990970611572266
epoch10: step3500/4680
step 40000: accuracy:0.11699999868869781, confidence:0.9101513624191284, loss:5.892706394195557
epoch10: step4000/4680
step 45000: accuracy:0.1599999964237213, confidence:0.9137324690818787, loss:5.5855712890625
epoch10: step4500/4680
step 0: accuracy:0.164000004529953, confidence:0.9159647822380066, loss:5.43326997756958
epoch11: step0/4680
step 5500: accuracy:0.1679999977350235, confidence:0.8952154517173767, loss:5.095391750335693
epoch11: step500/4680
step 11000: accuracy:0.0989999994635582, confidence:0.9959710240364075, loss:8.765604019165039
epoch11: step1000/4680
step 16500: accuracy:0.10100000351667404, confidence:0.996077835559845, loss:8.083674430847168
epoch11: step1500/4680
step 22000: accuracy:0.1289999932050705, confidence:0.8253787159919739, loss:3.729304313659668
epoch11: step2000/4680
step 27500: accuracy:0.17100000381469727, confidence:0.9439806342124939, loss:7.592355728149414
epoch11: step2500/4680
step 33000: accuracy:0.10400000214576721, confidence:0.9977378249168396, loss:8.60033130645752
epoch11: step3000/4680
step 38500: accuracy:0.14499999582767487, confidence:0.8824180364608765, loss:4.5176801681518555
epoch11: step3500/4680
step 44000: accuracy:0.13899999856948853, confidence:0.927389919757843, loss:6.415481090545654
epoch11: step4000/4680
step 49500: accuracy:0.1599999964237213, confidence:0.9342299699783325, loss:6.038857460021973
epoch11: step4500/4680
step 0: accuracy:0.16699999570846558, confidence:0.9301705360412598, loss:5.77685022354126
epoch12: step0/4680
step 6000: accuracy:0.15199999511241913, confidence:0.9225402474403381, loss:5.18427038192749
epoch12: step500/4680
step 12000: accuracy:0.08699999749660492, confidence:0.9868496060371399, loss:8.58568286895752
epoch12: step1000/4680
step 18000: accuracy:0.1080000028014183, confidence:0.990098774433136, loss:7.787524700164795
epoch12: step1500/4680
step 24000: accuracy:0.14499999582767487, confidence:0.8491016626358032, loss:3.8568966388702393
epoch12: step2000/4680
step 30000: accuracy:0.15199999511241913, confidence:0.9636086225509644, loss:8.041350364685059
epoch12: step2500/4680
step 36000: accuracy:0.11299999803304672, confidence:0.9967962503433228, loss:8.547452926635742
epoch12: step3000/4680
step 42000: accuracy:0.11299999803304672, confidence:0.8925716280937195, loss:4.819363594055176
epoch12: step3500/4680
step 48000: accuracy:0.15299999713897705, confidence:0.8826436996459961, loss:5.4105939865112305
epoch12: step4000/4680
step 54000: accuracy:0.18299999833106995, confidence:0.9351368546485901, loss:5.9785614013671875
epoch12: step4500/4680
step 0: accuracy:0.15600000321865082, confidence:0.9227738976478577, loss:6.102684020996094
epoch13: step0/4680
step 6500: accuracy:0.19099999964237213, confidence:0.9032348394393921, loss:5.149012088775635
epoch13: step500/4680
step 13000: accuracy:0.09399999678134918, confidence:0.9920450448989868, loss:8.925691604614258
epoch13: step1000/4680
step 19500: accuracy:0.10300000011920929, confidence:0.9931866526603699, loss:7.9469428062438965
epoch13: step1500/4680
step 26000: accuracy:0.14499999582767487, confidence:0.7987186312675476, loss:3.521519660949707
epoch13: step2000/4680
step 32500: accuracy:0.17399999499320984, confidence:0.9429031014442444, loss:7.572819709777832
epoch13: step2500/4680
step 39000: accuracy:0.12600000202655792, confidence:0.9963308572769165, loss:7.98785400390625
epoch13: step3000/4680
step 45500: accuracy:0.14499999582767487, confidence:0.8979402780532837, loss:4.6343770027160645
epoch13: step3500/4680
step 52000: accuracy:0.17100000381469727, confidence:0.9049025774002075, loss:5.411559581756592
epoch13: step4000/4680
step 58500: accuracy:0.17000000178813934, confidence:0.9362837076187134, loss:6.355810642242432
epoch13: step4500/4680
step 0: accuracy:0.16899999976158142, confidence:0.9286045432090759, loss:6.061799049377441
epoch14: step0/4680
step 7000: accuracy:0.17900000512599945, confidence:0.8895607590675354, loss:4.568780899047852
epoch14: step500/4680
step 14000: accuracy:0.0949999988079071, confidence:0.9736860394477844, loss:8.366884231567383
epoch14: step1000/4680
step 21000: accuracy:0.10199999809265137, confidence:0.9775938391685486, loss:7.708739280700684
epoch14: step1500/4680
step 28000: accuracy:0.10000000149011612, confidence:0.8977733850479126, loss:4.698875427246094
epoch14: step2000/4680
step 35000: accuracy:0.14399999380111694, confidence:0.9595643281936646, loss:6.431056499481201
epoch14: step2500/4680
step 42000: accuracy:0.1120000034570694, confidence:0.9884464144706726, loss:6.899948596954346
epoch14: step3000/4680
step 49000: accuracy:0.0989999994635582, confidence:0.924342930316925, loss:5.358295917510986
epoch14: step3500/4680
step 56000: accuracy:0.164000004529953, confidence:0.8801866173744202, loss:5.242754936218262
epoch14: step4000/4680
step 63000: accuracy:0.18299999833106995, confidence:0.922649621963501, loss:5.605364799499512
epoch14: step4500/4680
step 0: accuracy:0.18199999630451202, confidence:0.9155640602111816, loss:5.427326202392578
epoch15: step0/4680
step 7500: accuracy:0.1599999964237213, confidence:0.8931146264076233, loss:4.80969762802124
epoch15: step500/4680
step 15000: accuracy:0.0820000022649765, confidence:0.9805903434753418, loss:8.625487327575684
epoch15: step1000/4680
step 22500: accuracy:0.10999999940395355, confidence:0.9784499406814575, loss:7.459607124328613
epoch15: step1500/4680
step 30000: accuracy:0.15399999916553497, confidence:0.86517333984375, loss:4.046205043792725
epoch15: step2000/4680
step 37500: accuracy:0.14000000059604645, confidence:0.9714027047157288, loss:8.135149002075195
epoch15: step2500/4680
step 45000: accuracy:0.09000000357627869, confidence:0.9953361749649048, loss:7.901068210601807
epoch15: step3000/4680
step 52500: accuracy:0.1080000028014183, confidence:0.9368628263473511, loss:5.416667938232422
epoch15: step3500/4680
step 60000: accuracy:0.18299999833106995, confidence:0.8879580497741699, loss:5.112895488739014
epoch15: step4000/4680
step 67500: accuracy:0.1679999977350235, confidence:0.9130504727363586, loss:5.447185039520264
epoch15: step4500/4680
step 0: accuracy:0.17800000309944153, confidence:0.9103289246559143, loss:5.1431169509887695
epoch16: step0/4680
step 8000: accuracy:0.1809999942779541, confidence:0.9032096266746521, loss:4.523918628692627
epoch16: step500/4680
step 16000: accuracy:0.12099999934434891, confidence:0.9679736495018005, loss:8.439388275146484
epoch16: step1000/4680
step 24000: accuracy:0.11500000208616257, confidence:0.9914328455924988, loss:7.714622497558594
epoch16: step1500/4680
step 32000: accuracy:0.16899999976158142, confidence:0.8520055413246155, loss:4.014450550079346
epoch16: step2000/4680
step 40000: accuracy:0.15600000321865082, confidence:0.9609665870666504, loss:8.353318214416504
epoch16: step2500/4680
step 48000: accuracy:0.1120000034570694, confidence:0.9941428303718567, loss:7.661047458648682
epoch16: step3000/4680
step 56000: accuracy:0.15600000321865082, confidence:0.8660891056060791, loss:4.364553928375244
epoch16: step3500/4680
step 64000: accuracy:0.1509999930858612, confidence:0.9158234000205994, loss:5.902076244354248
epoch16: step4000/4680
step 72000: accuracy:0.12200000137090683, confidence:0.9319239854812622, loss:6.318914890289307
epoch16: step4500/4680
step 0: accuracy:0.17100000381469727, confidence:0.9255158305168152, loss:5.834827423095703
epoch17: step0/4680
step 8500: accuracy:0.1809999942779541, confidence:0.8964288234710693, loss:4.659218788146973
epoch17: step500/4680
step 17000: accuracy:0.1509999930858612, confidence:0.9565229415893555, loss:8.037985801696777
epoch17: step1000/4680
step 25500: accuracy:0.09700000286102295, confidence:0.9806779026985168, loss:7.506099700927734
epoch17: step1500/4680
step 34000: accuracy:0.15800000727176666, confidence:0.8773548007011414, loss:4.140384197235107
epoch17: step2000/4680
step 42500: accuracy:0.17499999701976776, confidence:0.9408665299415588, loss:6.438300132751465
epoch17: step2500/4680
step 51000: accuracy:0.09700000286102295, confidence:0.9949215054512024, loss:8.14166259765625
epoch17: step3000/4680
step 59500: accuracy:0.12999999523162842, confidence:0.8826574087142944, loss:4.491613864898682
epoch17: step3500/4680
step 68000: accuracy:0.20600000023841858, confidence:0.8790076971054077, loss:4.737364292144775
epoch17: step4000/4680
step 76500: accuracy:0.16699999570846558, confidence:0.9162046909332275, loss:5.46038293838501
epoch17: step4500/4680
step 0: accuracy:0.16099999845027924, confidence:0.9113396406173706, loss:5.45305871963501
epoch18: step0/4680
step 9000: accuracy:0.21899999678134918, confidence:0.8578435182571411, loss:4.388875961303711
epoch18: step500/4680
step 18000: accuracy:0.12200000137090683, confidence:0.9685565233230591, loss:8.948124885559082
epoch18: step1000/4680
step 27000: accuracy:0.09600000083446503, confidence:0.9488979578018188, loss:7.347741603851318
epoch18: step1500/4680
step 36000: accuracy:0.14300000667572021, confidence:0.8575890064239502, loss:4.014075756072998
epoch18: step2000/4680
step 45000: accuracy:0.12399999797344208, confidence:0.9461914300918579, loss:6.579015254974365
epoch18: step2500/4680
step 54000: accuracy:0.07900000363588333, confidence:0.979771614074707, loss:6.985105991363525
epoch18: step3000/4680
step 63000: accuracy:0.15000000596046448, confidence:0.8558531403541565, loss:4.28596830368042
epoch18: step3500/4680
step 72000: accuracy:0.1809999942779541, confidence:0.8945465087890625, loss:5.2629170417785645
epoch18: step4000/4680
step 81000: accuracy:0.17499999701976776, confidence:0.92586749792099, loss:6.128077030181885
epoch18: step4500/4680
step 0: accuracy:0.1899999976158142, confidence:0.9248582124710083, loss:5.500067234039307
epoch19: step0/4680
step 9500: accuracy:0.24500000476837158, confidence:0.8505634665489197, loss:3.795717239379883
epoch19: step500/4680
step 19000: accuracy:0.17399999499320984, confidence:0.9532314538955688, loss:6.481482982635498
epoch19: step1000/4680
step 28500: accuracy:0.09399999678134918, confidence:0.9546364545822144, loss:8.40676498413086
epoch19: step1500/4680
step 38000: accuracy:0.16699999570846558, confidence:0.8075079917907715, loss:3.61542010307312
epoch19: step2000/4680
step 47500: accuracy:0.16899999976158142, confidence:0.9595218896865845, loss:7.855485916137695
epoch19: step2500/4680
step 57000: accuracy:0.12099999934434891, confidence:0.9767928719520569, loss:6.4737420082092285
epoch19: step3000/4680
step 66500: accuracy:0.11299999803304672, confidence:0.9298795461654663, loss:5.418374538421631
epoch19: step3500/4680
step 76000: accuracy:0.18000000715255737, confidence:0.8945273756980896, loss:5.171451568603516
epoch19: step4000/4680
step 85500: accuracy:0.1599999964237213, confidence:0.9288968443870544, loss:5.743607997894287
epoch19: step4500/4680
step 0: accuracy:0.15399999916553497, confidence:0.9269876480102539, loss:5.623793125152588
epoch20: step0/4680
step 10000: accuracy:0.1979999989271164, confidence:0.8830915689468384, loss:4.3384690284729
epoch20: step500/4680
step 20000: accuracy:0.10300000011920929, confidence:0.971768856048584, loss:8.642897605895996
epoch20: step1000/4680
step 30000: accuracy:0.12700000405311584, confidence:0.9388325810432434, loss:8.32859992980957
epoch20: step1500/4680
step 40000: accuracy:0.1979999989271164, confidence:0.8308916091918945, loss:3.8490405082702637
epoch20: step2000/4680
step 50000: accuracy:0.164000004529953, confidence:0.9404674172401428, loss:8.185938835144043
epoch20: step2500/4680
step 60000: accuracy:0.1080000028014183, confidence:0.9832552075386047, loss:7.03352165222168
epoch20: step3000/4680
step 70000: accuracy:0.19200000166893005, confidence:0.8407172560691833, loss:3.827258348464966
epoch20: step3500/4680
step 80000: accuracy:0.20600000023841858, confidence:0.8991617560386658, loss:5.049263000488281
epoch20: step4000/4680
step 90000: accuracy:0.18199999630451202, confidence:0.9350353479385376, loss:6.6229248046875
epoch20: step4500/4680
step 0: accuracy:0.16500000655651093, confidence:0.9264667630195618, loss:6.1953864097595215
epoch21: step0/4680
step 10500: accuracy:0.21299999952316284, confidence:0.8637314438819885, loss:4.411648273468018
epoch21: step500/4680
step 21000: accuracy:0.13699999451637268, confidence:0.9639620780944824, loss:7.340238571166992
epoch21: step1000/4680
step 31500: accuracy:0.11599999666213989, confidence:0.9243866205215454, loss:8.214896202087402
epoch21: step1500/4680
step 42000: accuracy:0.1979999989271164, confidence:0.8186770081520081, loss:3.7542080879211426
epoch21: step2000/4680
step 52500: accuracy:0.1720000058412552, confidence:0.9532428979873657, loss:7.668435096740723
epoch21: step2500/4680
step 63000: accuracy:0.10899999737739563, confidence:0.9925366044044495, loss:7.9918131828308105
epoch21: step3000/4680
step 73500: accuracy:0.17100000381469727, confidence:0.8410544991493225, loss:3.644113779067993
epoch21: step3500/4680
step 84000: accuracy:0.2070000022649765, confidence:0.8706120252609253, loss:4.635406017303467
epoch21: step4000/4680
step 94500: accuracy:0.164000004529953, confidence:0.922172486782074, loss:5.5594329833984375
epoch21: step4500/4680
step 0: accuracy:0.16099999845027924, confidence:0.9129683971405029, loss:5.186924457550049
epoch22: step0/4680
step 11000: accuracy:0.27399998903274536, confidence:0.8574026823043823, loss:4.190752029418945
epoch22: step500/4680
step 22000: accuracy:0.12399999797344208, confidence:0.9751278758049011, loss:8.27480697631836
epoch22: step1000/4680
step 33000: accuracy:0.18000000715255737, confidence:0.8884397149085999, loss:6.282394886016846
epoch22: step1500/4680
step 44000: accuracy:0.17299999296665192, confidence:0.8644724488258362, loss:4.563392162322998
epoch22: step2000/4680
step 55000: accuracy:0.10499999672174454, confidence:0.9841623306274414, loss:9.356654167175293
epoch22: step2500/4680
step 66000: accuracy:0.10199999809265137, confidence:0.9939699769020081, loss:8.102715492248535
epoch22: step3000/4680
step 77000: accuracy:0.19599999487400055, confidence:0.8453982472419739, loss:3.687376022338867
epoch22: step3500/4680
step 88000: accuracy:0.2409999966621399, confidence:0.8556663393974304, loss:4.20244836807251
epoch22: step4000/4680
step 99000: accuracy:0.18000000715255737, confidence:0.9147018194198608, loss:5.175194263458252
epoch22: step4500/4680
step 0: accuracy:0.19099999964237213, confidence:0.9027493000030518, loss:5.035702228546143
epoch23: step0/4680
step 11500: accuracy:0.2549999952316284, confidence:0.8755871653556824, loss:4.84490442276001
epoch23: step500/4680
step 23000: accuracy:0.13699999451637268, confidence:0.9646487236022949, loss:8.054716110229492
epoch23: step1000/4680
step 34500: accuracy:0.13300000131130219, confidence:0.9402442574501038, loss:8.265125274658203
epoch23: step1500/4680
step 46000: accuracy:0.20999999344348907, confidence:0.8194299340248108, loss:3.713698148727417
epoch23: step2000/4680
step 57500: accuracy:0.16599999368190765, confidence:0.9613754153251648, loss:8.33045482635498
epoch23: step2500/4680
step 69000: accuracy:0.10100000351667404, confidence:0.9910514950752258, loss:8.095746994018555
epoch23: step3000/4680
step 80500: accuracy:0.16899999976158142, confidence:0.850936233997345, loss:3.943413019180298
epoch23: step3500/4680
step 92000: accuracy:0.210999995470047, confidence:0.8560018539428711, loss:4.38093900680542
epoch23: step4000/4680
step 103500: accuracy:0.19499999284744263, confidence:0.9128988981246948, loss:5.0378899574279785
epoch23: step4500/4680
step 0: accuracy:0.1860000044107437, confidence:0.9047311544418335, loss:4.934574127197266
epoch24: step0/4680
step 12000: accuracy:0.2720000147819519, confidence:0.8764752149581909, loss:4.632328987121582
epoch24: step500/4680
step 24000: accuracy:0.10599999874830246, confidence:0.9759849905967712, loss:9.184370040893555
epoch24: step1000/4680
step 36000: accuracy:0.13300000131130219, confidence:0.9412185549736023, loss:8.99145793914795
epoch24: step1500/4680
step 48000: accuracy:0.210999995470047, confidence:0.8715537786483765, loss:4.2282938957214355
epoch24: step2000/4680
step 60000: accuracy:0.17499999701976776, confidence:0.9592064023017883, loss:8.370133399963379
epoch24: step2500/4680
step 72000: accuracy:0.10400000214576721, confidence:0.9860463738441467, loss:8.614986419677734
epoch24: step3000/4680
step 84000: accuracy:0.1509999930858612, confidence:0.8584393262863159, loss:4.09104585647583
epoch24: step3500/4680
step 96000: accuracy:0.2280000001192093, confidence:0.85782790184021, loss:4.444423675537109
epoch24: step4000/4680
step 108000: accuracy:0.164000004529953, confidence:0.8988637328147888, loss:5.15354061126709
epoch24: step4500/4680
step 0: accuracy:0.19900000095367432, confidence:0.8938227891921997, loss:4.835424423217773
epoch25: step0/4680
step 12500: accuracy:0.26600000262260437, confidence:0.8910826444625854, loss:4.919960975646973
epoch25: step500/4680
step 25000: accuracy:0.09799999743700027, confidence:0.9775374531745911, loss:10.315690040588379
epoch25: step1000/4680
step 37500: accuracy:0.12300000339746475, confidence:0.9721193909645081, loss:9.954895973205566
epoch25: step1500/4680
step 50000: accuracy:0.18700000643730164, confidence:0.8648597002029419, loss:4.252030372619629
epoch25: step2000/4680
step 62500: accuracy:0.10599999874830246, confidence:0.9792466163635254, loss:9.622648239135742
epoch25: step2500/4680
step 75000: accuracy:0.09399999678134918, confidence:0.9801287651062012, loss:7.7370781898498535
epoch25: step3000/4680
step 87500: accuracy:0.19099999964237213, confidence:0.866948127746582, loss:4.100917339324951
epoch25: step3500/4680
step 100000: accuracy:0.23000000417232513, confidence:0.8510313034057617, loss:4.220025062561035
epoch25: step4000/4680
step 112500: accuracy:0.17000000178813934, confidence:0.8748757839202881, loss:4.739738464355469
epoch25: step4500/4680
step 0: accuracy:0.20000000298023224, confidence:0.8850884437561035, loss:4.545366287231445
epoch26: step0/4680
step 13000: accuracy:0.25600001215934753, confidence:0.8859370946884155, loss:5.0531392097473145
epoch26: step500/4680
step 26000: accuracy:0.09700000286102295, confidence:0.9840424060821533, loss:10.132607460021973
epoch26: step1000/4680
step 39000: accuracy:0.20000000298023224, confidence:0.9014847278594971, loss:7.525514125823975
epoch26: step1500/4680
step 52000: accuracy:0.23399999737739563, confidence:0.8362709879875183, loss:4.31785249710083
epoch26: step2000/4680
step 65000: accuracy:0.18700000643730164, confidence:0.927580714225769, loss:7.604219436645508
epoch26: step2500/4680
step 78000: accuracy:0.09099999815225601, confidence:0.9815147519111633, loss:8.042381286621094
epoch26: step3000/4680
step 91000: accuracy:0.14300000667572021, confidence:0.8533920645713806, loss:4.226431846618652
epoch26: step3500/4680
step 104000: accuracy:0.20000000298023224, confidence:0.862619936466217, loss:4.504541397094727
epoch26: step4000/4680
step 117000: accuracy:0.16899999976158142, confidence:0.8949477672576904, loss:5.142062664031982
epoch26: step4500/4680
step 0: accuracy:0.17599999904632568, confidence:0.8959723114967346, loss:4.787416934967041
epoch27: step0/4680
step 13500: accuracy:0.25999999046325684, confidence:0.879716694355011, loss:4.98431396484375
epoch27: step500/4680
step 27000: accuracy:0.08500000089406967, confidence:0.9852732419967651, loss:11.60977554321289
epoch27: step1000/4680
step 40500: accuracy:0.15800000727176666, confidence:0.9133001565933228, loss:6.8717546463012695
epoch27: step1500/4680
step 54000: accuracy:0.21799999475479126, confidence:0.8246951699256897, loss:4.106083393096924
epoch27: step2000/4680
step 67500: accuracy:0.20999999344348907, confidence:0.907204806804657, loss:6.1344313621521
epoch27: step2500/4680
step 81000: accuracy:0.09799999743700027, confidence:0.993582546710968, loss:9.843758583068848
epoch27: step3000/4680
step 94500: accuracy:0.11999999731779099, confidence:0.9233689904212952, loss:5.331260681152344
epoch27: step3500/4680
step 108000: accuracy:0.23600000143051147, confidence:0.8279103636741638, loss:4.060800552368164
epoch27: step4000/4680
step 121500: accuracy:0.20499999821186066, confidence:0.881200909614563, loss:4.459682941436768
epoch27: step4500/4680
step 0: accuracy:0.1809999942779541, confidence:0.875372052192688, loss:4.525714874267578
epoch28: step0/4680
step 14000: accuracy:0.2680000066757202, confidence:0.8862352967262268, loss:5.105949878692627
epoch28: step500/4680
step 28000: accuracy:0.10199999809265137, confidence:0.9838212728500366, loss:10.33023452758789
epoch28: step1000/4680
step 42000: accuracy:0.10199999809265137, confidence:0.9365425705909729, loss:7.18695068359375
epoch28: step1500/4680
step 56000: accuracy:0.21400000154972076, confidence:0.8412242531776428, loss:4.02689790725708
epoch28: step2000/4680
step 70000: accuracy:0.1889999955892563, confidence:0.9217599630355835, loss:6.869080066680908
epoch28: step2500/4680
step 84000: accuracy:0.09700000286102295, confidence:0.9456709027290344, loss:6.848326683044434
epoch28: step3000/4680
step 98000: accuracy:0.1940000057220459, confidence:0.8519861698150635, loss:4.152143478393555
epoch28: step3500/4680
step 112000: accuracy:0.21899999678134918, confidence:0.8510943055152893, loss:4.511152267456055
epoch28: step4000/4680
step 126000: accuracy:0.18299999833106995, confidence:0.9048090577125549, loss:5.205639362335205
epoch28: step4500/4680
step 0: accuracy:0.20200000703334808, confidence:0.8931551575660706, loss:4.923971652984619
epoch29: step0/4680
step 14500: accuracy:0.24500000476837158, confidence:0.8758921027183533, loss:5.029179573059082
epoch29: step500/4680
step 29000: accuracy:0.10499999672174454, confidence:0.9736111164093018, loss:10.449419021606445
epoch29: step1000/4680
step 43500: accuracy:0.1420000046491623, confidence:0.9352142810821533, loss:7.596541881561279
epoch29: step1500/4680
step 58000: accuracy:0.21199999749660492, confidence:0.8474600911140442, loss:4.275662422180176
epoch29: step2000/4680
step 72500: accuracy:0.17299999296665192, confidence:0.9537323117256165, loss:7.461637496948242
epoch29: step2500/4680
step 87000: accuracy:0.10499999672174454, confidence:0.9623969793319702, loss:7.337090969085693
epoch29: step3000/4680
step 101500: accuracy:0.19499999284744263, confidence:0.8667256236076355, loss:4.224264144897461
epoch29: step3500/4680
step 116000: accuracy:0.26100000739097595, confidence:0.8465586304664612, loss:4.208755016326904
epoch29: step4000/4680
step 130500: accuracy:0.1899999976158142, confidence:0.8852031826972961, loss:4.619998931884766
epoch29: step4500/4680
2018-06-15 18:13:50.642389: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 18:13:50.642699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
2018-06-15 18:13:52.804975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
CHEKPOINT DIR: uniform
model has been loaded from mnist_classifier.pkl
---------
Variables: name (type shape) [size]
---------
Variable:0 (float32_ref 5x5x1x32) [800, bytes: 3200]
Variable_1:0 (float32_ref 32) [32, bytes: 128]
Variable_2:0 (float32_ref 5x5x32x64) [51200, bytes: 204800]
Variable_3:0 (float32_ref 64) [64, bytes: 256]
Variable_4:0 (float32_ref 3136x1024) [3211264, bytes: 12845056]
Variable_5:0 (float32_ref 1024) [1024, bytes: 4096]
Variable_6:0 (float32_ref 1024x10) [10240, bytes: 40960]
Variable_7:0 (float32_ref 10) [10, bytes: 40]
discriminator/d_conv1/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]
discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]
discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]
discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]
generator/g_fc1/Matrix:0 (float32_ref 74x1024) [75776, bytes: 303104]
generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]
generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]
generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]
classifier/c_fc1/Matrix:0 (float32_ref 1024x64) [65536, bytes: 262144]
classifier/c_fc1/bias:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/beta:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/gamma:0 (float32_ref 64) [64, bytes: 256]
classifier/c_fc2/Matrix:0 (float32_ref 64x12) [768, bytes: 3072]
classifier/c_fc2/bias:0 (float32_ref 12) [12, bytes: 48]
Total size of variables: 16552792
Total bytes of variables: 66211168
 [*] Reading checkpoints...
 [*] Success to read MultiModalInfoGAN.model-43721
 [*] Load SUCCESS


SAMPLES SIZE=4680,LABELS=299520


SAVED TRAINING SET generated_training_set_mnist_UniformSample_mu_0_sigma_0.15_czrc_czcc_rzcc_rzrc
 [*] Training finished!
 [*] Testing finished!
2018-06-15 18:14:37.231795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 18:14:37.232046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
model has been loaded from mnist_classifier.pkl
argmax:[4 4 4 ..., 4 4 4]
step 0: accuracy:0.0, confidence:0.9868118166923523, loss:13.050642013549805
Assinging:5
[   0  117    0    0 9883]
argmax:[5 5 5 ..., 5 5 5]
step 0: accuracy:0.0, confidence:0.979282796382904, loss:10.079038619995117
Assinging:6
[   0  234    0    0    0 9766]
argmax:[8 8 8 ..., 8 8 8]
step 0: accuracy:0.0, confidence:0.9999372959136963, loss:16.00054931640625
Assinging:9
[    0     0     0     0     0     0     0     0 10000]
argmax:[2 2 2 ..., 2 2 2]
step 0: accuracy:0.0, confidence:0.9992236495018005, loss:19.44969367980957
Assinging:3
[    0     0 10000]
argmax:[0 0 0 ..., 0 0 0]
step 0: accuracy:0.0, confidence:0.9998763799667358, loss:17.36652183532715
Assinging:1
[10000]
argmax:[1 1 1 ..., 1 1 1]
step 0: accuracy:0.0, confidence:0.9985288977622986, loss:14.410422325134277
Assinging:2
[    0 10000]
argmax:[9 9 9 ..., 9 9 9]
step 0: accuracy:0.9883000254631042, confidence:0.990810751914978, loss:0.016689734533429146
Assinging:10
[   0    0    0    0    0    0    0    0  117 9883]
argmax:[6 6 6 ..., 6 6 6]
step 0: accuracy:0.0, confidence:0.993722140789032, loss:21.713523864746094
Assinging:7
[   0  117    0    0    0    0 9883]
argmax:[7 7 7 ..., 7 7 7]
step 0: accuracy:0.0, confidence:0.9960007667541504, loss:12.1192626953125
Assinging:8
[    0     0     0     0     0     0     0 10000]
argmax:[3 3 3 ..., 3 3 3]
step 0: accuracy:0.0, confidence:0.9996283054351807, loss:16.491819381713867
Assinging:4
[    0     0     0 10000]
2018-06-15 18:14:55.300247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 0002:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 18:14:55.300527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
step 0: accuracy:0.08299999684095383, confidence:0.90973299741745, loss:7.085927963256836
epoch0: step0/4680
step 0: accuracy:0.08799999952316284, confidence:1.0, loss:69.31914520263672
epoch0: step500/4680
step 0: accuracy:0.09099999815225601, confidence:1.0, loss:39.7664909362793
epoch0: step1000/4680
step 0: accuracy:0.10199999809265137, confidence:0.9998658895492554, loss:10.941695213317871
epoch0: step1500/4680
step 0: accuracy:0.10000000149011612, confidence:0.9988294243812561, loss:8.274734497070312
epoch0: step2000/4680
step 0: accuracy:0.0729999989271164, confidence:0.9996041059494019, loss:9.77148723602295
epoch0: step2500/4680
step 0: accuracy:0.11599999666213989, confidence:0.9999756217002869, loss:13.022879600524902
epoch0: step3000/4680
step 0: accuracy:0.11299999803304672, confidence:0.9996446967124939, loss:10.589448928833008
epoch0: step3500/4680
step 0: accuracy:0.0949999988079071, confidence:0.9997867345809937, loss:12.246623039245605
epoch0: step4000/4680
step 0: accuracy:0.10499999672174454, confidence:0.999945878982544, loss:11.372072219848633
epoch0: step4500/4680
step 0: accuracy:0.1080000028014183, confidence:0.9999433755874634, loss:11.272683143615723
epoch1: step0/4680
step 500: accuracy:0.10400000214576721, confidence:0.999561071395874, loss:10.5350980758667
epoch1: step500/4680
step 1000: accuracy:0.0820000022649765, confidence:0.9997514486312866, loss:10.98617172241211
epoch1: step1000/4680
step 1500: accuracy:0.10499999672174454, confidence:0.999981164932251, loss:13.73659610748291
epoch1: step1500/4680
step 2000: accuracy:0.10400000214576721, confidence:0.9992440342903137, loss:8.93051528930664
epoch1: step2000/4680
step 2500: accuracy:0.08500000089406967, confidence:0.998835563659668, loss:8.953817367553711
epoch1: step2500/4680
step 3000: accuracy:0.11100000143051147, confidence:0.9998299479484558, loss:10.454834938049316
epoch1: step3000/4680
step 3500: accuracy:0.11599999666213989, confidence:0.999958872795105, loss:11.923470497131348
epoch1: step3500/4680
step 4000: accuracy:0.08799999952316284, confidence:0.9999493360519409, loss:12.283685684204102
epoch1: step4000/4680
step 4500: accuracy:0.08699999749660492, confidence:0.9998961091041565, loss:11.225658416748047
epoch1: step4500/4680
step 0: accuracy:0.09600000083446503, confidence:0.9999032020568848, loss:11.126561164855957
epoch2: step0/4680
step 1000: accuracy:0.10499999672174454, confidence:0.9994408488273621, loss:10.004593849182129
epoch2: step500/4680
step 2000: accuracy:0.07599999755620956, confidence:0.9999962449073792, loss:15.97236156463623
epoch2: step1000/4680
step 3000: accuracy:0.09399999678134918, confidence:0.9997645616531372, loss:11.376324653625488
epoch2: step1500/4680
step 4000: accuracy:0.09200000017881393, confidence:0.9997583031654358, loss:10.200201034545898
epoch2: step2000/4680
step 5000: accuracy:0.09600000083446503, confidence:0.9985938668251038, loss:8.531235694885254
epoch2: step2500/4680
step 6000: accuracy:0.12800000607967377, confidence:0.9999391436576843, loss:11.307536125183105
epoch2: step3000/4680
step 7000: accuracy:0.09000000357627869, confidence:0.999894380569458, loss:11.012272834777832
epoch2: step3500/4680
step 8000: accuracy:0.1080000028014183, confidence:0.999728798866272, loss:9.395858764648438
epoch2: step4000/4680
step 9000: accuracy:0.10599999874830246, confidence:0.9999176263809204, loss:11.247162818908691
epoch2: step4500/4680
step 0: accuracy:0.09600000083446503, confidence:0.9999045133590698, loss:11.190037727355957
epoch3: step0/4680
step 1500: accuracy:0.10000000149011612, confidence:0.998805582523346, loss:8.727879524230957
epoch3: step500/4680
step 3000: accuracy:0.11900000274181366, confidence:0.999991774559021, loss:14.234685897827148
epoch3: step1000/4680
step 4500: accuracy:0.11699999868869781, confidence:0.9997860193252563, loss:11.723190307617188
epoch3: step1500/4680
step 6000: accuracy:0.11100000143051147, confidence:0.9989792108535767, loss:9.03554630279541
epoch3: step2000/4680
step 7500: accuracy:0.07800000160932541, confidence:0.9995138049125671, loss:9.690631866455078
epoch3: step2500/4680
step 9000: accuracy:0.10300000011920929, confidence:0.9999685287475586, loss:12.066940307617188
epoch3: step3000/4680
step 10500: accuracy:0.0989999994635582, confidence:0.99973064661026, loss:10.510000228881836
epoch3: step3500/4680
step 12000: accuracy:0.10499999672174454, confidence:0.9997149705886841, loss:9.434746742248535
epoch3: step4000/4680
step 13500: accuracy:0.1080000028014183, confidence:0.9997339844703674, loss:9.793794631958008
epoch3: step4500/4680
step 0: accuracy:0.10100000351667404, confidence:0.9997614026069641, loss:9.852184295654297
epoch4: step0/4680
step 2000: accuracy:0.10999999940395355, confidence:0.9979833364486694, loss:8.551704406738281
epoch4: step500/4680
step 4000: accuracy:0.0820000022649765, confidence:0.9994511008262634, loss:9.717052459716797
epoch4: step1000/4680
step 6000: accuracy:0.0989999994635582, confidence:0.9998834729194641, loss:11.45179271697998
epoch4: step1500/4680
step 8000: accuracy:0.1080000028014183, confidence:0.9990569949150085, loss:9.039079666137695
epoch4: step2000/4680
step 10000: accuracy:0.09700000286102295, confidence:0.9993504881858826, loss:9.265861511230469
epoch4: step2500/4680
step 12000: accuracy:0.11699999868869781, confidence:0.9999654293060303, loss:12.331853866577148
epoch4: step3000/4680
step 14000: accuracy:0.09799999743700027, confidence:0.9996788501739502, loss:11.035179138183594
epoch4: step3500/4680
step 16000: accuracy:0.10599999874830246, confidence:0.9993891716003418, loss:8.83291244506836
epoch4: step4000/4680
step 18000: accuracy:0.10599999874830246, confidence:0.999575674533844, loss:9.158016204833984
epoch4: step4500/4680
step 0: accuracy:0.10499999672174454, confidence:0.9996553659439087, loss:9.3260498046875
epoch5: step0/4680
step 2500: accuracy:0.09799999743700027, confidence:0.9986198544502258, loss:9.630340576171875
epoch5: step500/4680
step 5000: accuracy:0.07900000363588333, confidence:0.9997457265853882, loss:10.798861503601074
epoch5: step1000/4680
step 7500: accuracy:0.09300000220537186, confidence:0.9996983408927917, loss:10.580916404724121
epoch5: step1500/4680
step 10000: accuracy:0.1120000034570694, confidence:0.9991982579231262, loss:9.093722343444824
epoch5: step2000/4680
step 12500: accuracy:0.0949999988079071, confidence:0.9983788132667542, loss:8.817219734191895
epoch5: step2500/4680
step 15000: accuracy:0.10199999809265137, confidence:0.9997195601463318, loss:10.110586166381836
epoch5: step3000/4680
step 17500: accuracy:0.10899999737739563, confidence:0.9996288418769836, loss:11.152379989624023
epoch5: step3500/4680
step 20000: accuracy:0.09700000286102295, confidence:0.999591588973999, loss:9.404138565063477
epoch5: step4000/4680
step 22500: accuracy:0.09399999678134918, confidence:0.9996110796928406, loss:9.299903869628906
epoch5: step4500/4680
step 0: accuracy:0.10599999874830246, confidence:0.999676525592804, loss:9.28708267211914
epoch6: step0/4680
step 3000: accuracy:0.09399999678134918, confidence:0.9964551329612732, loss:8.527462005615234
epoch6: step500/4680
step 6000: accuracy:0.0820000022649765, confidence:0.9998295307159424, loss:11.625575065612793
epoch6: step1000/4680
step 9000: accuracy:0.08299999684095383, confidence:0.9988964796066284, loss:9.34001636505127
epoch6: step1500/4680
step 12000: accuracy:0.10199999809265137, confidence:0.9983471632003784, loss:8.688432693481445
epoch6: step2000/4680
step 15000: accuracy:0.10899999737739563, confidence:0.9966319799423218, loss:8.060302734375
epoch6: step2500/4680
step 18000: accuracy:0.125, confidence:0.9996901750564575, loss:9.927457809448242
epoch6: step3000/4680
step 21000: accuracy:0.10199999809265137, confidence:0.9993422627449036, loss:10.24848461151123
epoch6: step3500/4680
step 24000: accuracy:0.10899999737739563, confidence:0.9991098642349243, loss:8.578804969787598
epoch6: step4000/4680
step 27000: accuracy:0.09700000286102295, confidence:0.9995902180671692, loss:9.264513969421387
epoch6: step4500/4680
step 0: accuracy:0.11299999803304672, confidence:0.9996840953826904, loss:9.201239585876465
epoch7: step0/4680
step 3500: accuracy:0.10999999940395355, confidence:0.9968600869178772, loss:8.20380973815918
epoch7: step500/4680
step 7000: accuracy:0.09600000083446503, confidence:0.999384880065918, loss:10.06800651550293
epoch7: step1000/4680
step 10500: accuracy:0.09799999743700027, confidence:0.9983503222465515, loss:8.94922924041748
epoch7: step1500/4680
step 14000: accuracy:0.125, confidence:0.9987418055534363, loss:8.72307014465332
epoch7: step2000/4680
step 17500: accuracy:0.10599999874830246, confidence:0.9959275722503662, loss:7.827749252319336
epoch7: step2500/4680
step 21000: accuracy:0.09399999678134918, confidence:0.9997774362564087, loss:10.712830543518066
epoch7: step3000/4680
step 24500: accuracy:0.0860000029206276, confidence:0.9992657899856567, loss:9.601001739501953
epoch7: step3500/4680
step 28000: accuracy:0.0989999994635582, confidence:0.999281108379364, loss:8.919270515441895
epoch7: step4000/4680
step 31500: accuracy:0.10100000351667404, confidence:0.9994992613792419, loss:9.282125473022461
epoch7: step4500/4680
step 0: accuracy:0.09200000017881393, confidence:0.9995877146720886, loss:9.449058532714844
epoch8: step0/4680
step 4000: accuracy:0.09399999678134918, confidence:0.9936600923538208, loss:7.508024215698242
epoch8: step500/4680
step 8000: accuracy:0.08900000154972076, confidence:0.9995906949043274, loss:10.83137035369873
epoch8: step1000/4680
step 12000: accuracy:0.10300000011920929, confidence:0.9986594915390015, loss:8.980018615722656
epoch8: step1500/4680
step 16000: accuracy:0.09000000357627869, confidence:0.9982731938362122, loss:8.689295768737793
epoch8: step2000/4680
step 20000: accuracy:0.09000000357627869, confidence:0.996091902256012, loss:8.161949157714844
epoch8: step2500/4680
step 24000: accuracy:0.1080000028014183, confidence:0.9997910261154175, loss:10.851795196533203
epoch8: step3000/4680
step 28000: accuracy:0.09099999815225601, confidence:0.9990243911743164, loss:9.700763702392578
epoch8: step3500/4680
step 32000: accuracy:0.09700000286102295, confidence:0.9969298243522644, loss:7.958958148956299
epoch8: step4000/4680
step 36000: accuracy:0.0949999988079071, confidence:0.999626636505127, loss:9.747187614440918
epoch8: step4500/4680
step 0: accuracy:0.09799999743700027, confidence:0.9995874166488647, loss:9.475029945373535
epoch9: step0/4680
step 4500: accuracy:0.09000000357627869, confidence:0.994585394859314, loss:7.5498270988464355
epoch9: step500/4680
step 9000: accuracy:0.0860000029206276, confidence:0.9998940825462341, loss:12.459906578063965
epoch9: step1000/4680
step 13500: accuracy:0.10999999940395355, confidence:0.9991540312767029, loss:9.27222728729248
epoch9: step1500/4680
step 18000: accuracy:0.09000000357627869, confidence:0.9991998076438904, loss:9.448734283447266
epoch9: step2000/4680
step 22500: accuracy:0.09300000220537186, confidence:0.9953998327255249, loss:8.062232971191406
epoch9: step2500/4680
step 27000: accuracy:0.09799999743700027, confidence:0.999804675579071, loss:11.118583679199219
epoch9: step3000/4680
step 31500: accuracy:0.09700000286102295, confidence:0.9995843172073364, loss:11.042441368103027
epoch9: step3500/4680
step 36000: accuracy:0.11100000143051147, confidence:0.9988464713096619, loss:8.857170104980469
epoch9: step4000/4680
step 40500: accuracy:0.11299999803304672, confidence:0.9994757771492004, loss:9.340302467346191
epoch9: step4500/4680
step 0: accuracy:0.10100000351667404, confidence:0.9996011257171631, loss:9.618775367736816
epoch10: step0/4680
step 5000: accuracy:0.07699999958276749, confidence:0.9955233931541443, loss:8.159032821655273
epoch10: step500/4680
step 10000: accuracy:0.08500000089406967, confidence:0.9995505213737488, loss:10.757306098937988
epoch10: step1000/4680
step 15000: accuracy:0.10400000214576721, confidence:0.99861079454422, loss:8.861021041870117
epoch10: step1500/4680
step 20000: accuracy:0.09300000220537186, confidence:0.9986521601676941, loss:8.615538597106934
epoch10: step2000/4680
step 25000: accuracy:0.10400000214576721, confidence:0.9988057613372803, loss:8.795393943786621
epoch10: step2500/4680
step 30000: accuracy:0.1120000034570694, confidence:0.9995009899139404, loss:9.71656322479248
epoch10: step3000/4680
step 35000: accuracy:0.10700000077486038, confidence:0.999146580696106, loss:10.968118667602539
epoch10: step3500/4680
step 40000: accuracy:0.10000000149011612, confidence:0.9967369437217712, loss:8.342550277709961
epoch10: step4000/4680
step 45000: accuracy:0.09200000017881393, confidence:0.9995090365409851, loss:9.798543930053711
epoch10: step4500/4680
step 0: accuracy:0.09700000286102295, confidence:0.9996691942214966, loss:9.884478569030762
epoch11: step0/4680
step 5500: accuracy:0.0949999988079071, confidence:0.9966315627098083, loss:8.06019115447998
epoch11: step500/4680
step 11000: accuracy:0.07599999755620956, confidence:0.9997202157974243, loss:10.841885566711426
epoch11: step1000/4680
step 16500: accuracy:0.10400000214576721, confidence:0.9992225170135498, loss:9.381287574768066
epoch11: step1500/4680
step 22000: accuracy:0.09700000286102295, confidence:0.9983108043670654, loss:8.726919174194336
epoch11: step2000/4680
step 27500: accuracy:0.09399999678134918, confidence:0.9974309206008911, loss:8.405768394470215
epoch11: step2500/4680
step 33000: accuracy:0.12300000339746475, confidence:0.9996674656867981, loss:10.370711326599121
epoch11: step3000/4680
step 38500: accuracy:0.10000000149011612, confidence:0.9994930624961853, loss:9.401293754577637
epoch11: step3500/4680
step 44000: accuracy:0.0949999988079071, confidence:0.9987484216690063, loss:8.497013092041016
epoch11: step4000/4680
step 49500: accuracy:0.10199999809265137, confidence:0.9992361664772034, loss:9.188645362854004
epoch11: step4500/4680
step 0: accuracy:0.1120000034570694, confidence:0.9994687438011169, loss:9.285808563232422
epoch12: step0/4680
step 6000: accuracy:0.09799999743700027, confidence:0.9974846243858337, loss:8.36678695678711
epoch12: step500/4680
step 12000: accuracy:0.10199999809265137, confidence:0.997833788394928, loss:8.52878475189209
epoch12: step1000/4680
step 18000: accuracy:0.09300000220537186, confidence:0.9992501735687256, loss:9.563155174255371
epoch12: step1500/4680
step 24000: accuracy:0.08799999952316284, confidence:0.9992009997367859, loss:9.419689178466797
epoch12: step2000/4680
step 30000: accuracy:0.09700000286102295, confidence:0.9978538751602173, loss:8.237214088439941
epoch12: step2500/4680
step 36000: accuracy:0.11100000143051147, confidence:0.9993067979812622, loss:9.19731616973877
epoch12: step3000/4680
step 42000: accuracy:0.0989999994635582, confidence:0.9992891550064087, loss:9.039491653442383
epoch12: step3500/4680
step 48000: accuracy:0.09600000083446503, confidence:0.9987358450889587, loss:8.385660171508789
epoch12: step4000/4680
step 54000: accuracy:0.11299999803304672, confidence:0.9989466667175293, loss:8.629467964172363
epoch12: step4500/4680
step 0: accuracy:0.09300000220537186, confidence:0.9992911219596863, loss:9.079413414001465
epoch13: step0/4680
step 6500: accuracy:0.09700000286102295, confidence:0.9945728182792664, loss:7.453828811645508
epoch13: step500/4680
step 13000: accuracy:0.08799999952316284, confidence:0.9987324476242065, loss:10.194721221923828
epoch13: step1000/4680
step 19500: accuracy:0.0989999994635582, confidence:0.9976383447647095, loss:8.615415573120117
epoch13: step1500/4680
step 26000: accuracy:0.10300000011920929, confidence:0.9988475441932678, loss:8.975564002990723
epoch13: step2000/4680
step 32500: accuracy:0.09799999743700027, confidence:0.9980512857437134, loss:8.606597900390625
epoch13: step2500/4680
step 39000: accuracy:0.1120000034570694, confidence:0.9997484683990479, loss:10.748132705688477
epoch13: step3000/4680
step 45500: accuracy:0.07699999958276749, confidence:0.9994570016860962, loss:9.587552070617676
epoch13: step3500/4680
step 52000: accuracy:0.09000000357627869, confidence:0.9990058541297913, loss:9.342695236206055
epoch13: step4000/4680
step 58500: accuracy:0.10400000214576721, confidence:0.9991572499275208, loss:8.853178977966309
epoch13: step4500/4680
step 0: accuracy:0.09799999743700027, confidence:0.9995153546333313, loss:9.365808486938477
epoch14: step0/4680
step 7000: accuracy:0.09099999815225601, confidence:0.9917349815368652, loss:7.080141067504883
epoch14: step500/4680
step 14000: accuracy:0.09300000220537186, confidence:0.9996501803398132, loss:10.750454902648926
epoch14: step1000/4680
step 21000: accuracy:0.08299999684095383, confidence:0.9992367029190063, loss:9.821364402770996
epoch14: step1500/4680
step 28000: accuracy:0.10999999940395355, confidence:0.9988628029823303, loss:9.181648254394531
epoch14: step2000/4680
step 35000: accuracy:0.10499999672174454, confidence:0.9962909817695618, loss:7.669334888458252
epoch14: step2500/4680
step 42000: accuracy:0.11500000208616257, confidence:0.999497652053833, loss:9.569967269897461
epoch14: step3000/4680
step 49000: accuracy:0.08799999952316284, confidence:0.998982310295105, loss:9.13110637664795
epoch14: step3500/4680
step 56000: accuracy:0.0989999994635582, confidence:0.9982190728187561, loss:8.191193580627441
epoch14: step4000/4680
step 63000: accuracy:0.09799999743700027, confidence:0.9990724921226501, loss:9.100115776062012
epoch14: step4500/4680
step 0: accuracy:0.10700000077486038, confidence:0.999380886554718, loss:9.148420333862305
epoch15: step0/4680
step 7500: accuracy:0.10100000351667404, confidence:0.9935956001281738, loss:7.215278148651123
epoch15: step500/4680
step 15000: accuracy:0.09600000083446503, confidence:0.9989222884178162, loss:10.003820419311523
epoch15: step1000/4680
step 22500: accuracy:0.10000000149011612, confidence:0.9982791543006897, loss:8.847198486328125
epoch15: step1500/4680
step 30000: accuracy:0.09399999678134918, confidence:0.9983705282211304, loss:8.986350059509277
epoch15: step2000/4680
step 37500: accuracy:0.10499999672174454, confidence:0.9984357357025146, loss:9.838215827941895
epoch15: step2500/4680
step 45000: accuracy:0.10999999940395355, confidence:0.9998664259910583, loss:12.006617546081543
epoch15: step3000/4680
step 52500: accuracy:0.10700000077486038, confidence:0.9990538954734802, loss:8.68842887878418
epoch15: step3500/4680
step 60000: accuracy:0.10599999874830246, confidence:0.9983645081520081, loss:8.33395004272461
epoch15: step4000/4680
step 67500: accuracy:0.12200000137090683, confidence:0.9987801313400269, loss:8.392871856689453
epoch15: step4500/4680
step 0: accuracy:0.10199999809265137, confidence:0.9991875290870667, loss:8.906051635742188
epoch16: step0/4680
step 8000: accuracy:0.07800000160932541, confidence:0.9916896820068359, loss:7.340419292449951
epoch16: step500/4680
step 16000: accuracy:0.09300000220537186, confidence:0.9995769262313843, loss:10.684115409851074
epoch16: step1000/4680
step 24000: accuracy:0.10599999874830246, confidence:0.9994173049926758, loss:9.624828338623047
epoch16: step1500/4680
step 32000: accuracy:0.10400000214576721, confidence:0.9985201358795166, loss:8.474026679992676
epoch16: step2000/4680
step 40000: accuracy:0.10499999672174454, confidence:0.997010350227356, loss:8.02853012084961
epoch16: step2500/4680
step 48000: accuracy:0.12399999797344208, confidence:0.9996877312660217, loss:10.51267147064209
epoch16: step3000/4680
step 56000: accuracy:0.09799999743700027, confidence:0.999018669128418, loss:9.031689643859863
epoch16: step3500/4680
step 64000: accuracy:0.08799999952316284, confidence:0.9971393942832947, loss:8.006877899169922
epoch16: step4000/4680
step 72000: accuracy:0.10499999672174454, confidence:0.9979527592658997, loss:8.26693058013916
epoch16: step4500/4680
step 0: accuracy:0.09700000286102295, confidence:0.9987899661064148, loss:8.880887985229492
epoch17: step0/4680
step 8500: accuracy:0.1080000028014183, confidence:0.9944265484809875, loss:7.271945476531982
epoch17: step500/4680
step 17000: accuracy:0.0989999994635582, confidence:0.9991670846939087, loss:9.739911079406738
epoch17: step1000/4680
step 25500: accuracy:0.10599999874830246, confidence:0.9990594983100891, loss:9.373324394226074
epoch17: step1500/4680
step 34000: accuracy:0.1080000028014183, confidence:0.9985561370849609, loss:8.4932279586792
epoch17: step2000/4680
step 42500: accuracy:0.09000000357627869, confidence:0.996884822845459, loss:7.987095355987549
epoch17: step2500/4680
step 51000: accuracy:0.10700000077486038, confidence:0.999640941619873, loss:10.242491722106934
epoch17: step3000/4680
step 59500: accuracy:0.09399999678134918, confidence:0.9991032481193542, loss:9.407785415649414
epoch17: step3500/4680
step 68000: accuracy:0.0989999994635582, confidence:0.9964836239814758, loss:7.845958709716797
epoch17: step4000/4680
step 76500: accuracy:0.10499999672174454, confidence:0.9967907667160034, loss:7.982776641845703
epoch17: step4500/4680
step 0: accuracy:0.10899999737739563, confidence:0.9984213709831238, loss:8.569197654724121
epoch18: step0/4680
step 9000: accuracy:0.09200000017881393, confidence:0.9805402159690857, loss:6.309831619262695
epoch18: step500/4680
step 18000: accuracy:0.07999999821186066, confidence:0.9972123503684998, loss:8.685521125793457
epoch18: step1000/4680
step 27000: accuracy:0.08900000154972076, confidence:0.9977213740348816, loss:8.860849380493164
epoch18: step1500/4680
step 36000: accuracy:0.10499999672174454, confidence:0.9973732829093933, loss:8.172592163085938
epoch18: step2000/4680
step 45000: accuracy:0.10199999809265137, confidence:0.9885131120681763, loss:6.797398567199707
epoch18: step2500/4680
step 54000: accuracy:0.11299999803304672, confidence:0.9997429847717285, loss:10.584421157836914
epoch18: step3000/4680
step 63000: accuracy:0.09700000286102295, confidence:0.9976750612258911, loss:8.166516304016113
epoch18: step3500/4680
step 72000: accuracy:0.08900000154972076, confidence:0.9958585500717163, loss:7.865569591522217
epoch18: step4000/4680
step 81000: accuracy:0.08299999684095383, confidence:0.9968436360359192, loss:8.436118125915527
epoch18: step4500/4680
step 0: accuracy:0.10000000149011612, confidence:0.9985284209251404, loss:9.029869079589844
epoch19: step0/4680
step 9500: accuracy:0.11100000143051147, confidence:0.9834044575691223, loss:6.384143829345703
epoch19: step500/4680
step 19000: accuracy:0.09200000017881393, confidence:0.993421733379364, loss:7.7427215576171875
epoch19: step1000/4680
step 28500: accuracy:0.10700000077486038, confidence:0.9990476965904236, loss:9.352821350097656
epoch19: step1500/4680
step 38000: accuracy:0.09700000286102295, confidence:0.9967410564422607, loss:8.042314529418945
epoch19: step2000/4680
step 47500: accuracy:0.08900000154972076, confidence:0.9836848974227905, loss:6.344545841217041
epoch19: step2500/4680
step 57000: accuracy:0.11500000208616257, confidence:0.9993298053741455, loss:9.212879180908203
epoch19: step3000/4680
step 66500: accuracy:0.09300000220537186, confidence:0.998092532157898, loss:8.455512046813965
epoch19: step3500/4680
step 76000: accuracy:0.12300000339746475, confidence:0.989326000213623, loss:6.44740629196167
epoch19: step4000/4680
step 85500: accuracy:0.09700000286102295, confidence:0.9895585775375366, loss:6.9228010177612305
epoch19: step4500/4680
step 0: accuracy:0.0860000029206276, confidence:0.9956482648849487, loss:7.916540145874023
epoch20: step0/4680
step 10000: accuracy:0.09200000017881393, confidence:0.9728667736053467, loss:5.717523097991943
epoch20: step500/4680
step 20000: accuracy:0.0949999988079071, confidence:0.9894009828567505, loss:7.554027557373047
epoch20: step1000/4680
step 30000: accuracy:0.08100000023841858, confidence:0.998699963092804, loss:9.29875659942627
epoch20: step1500/4680
step 40000: accuracy:0.11999999731779099, confidence:0.9958003163337708, loss:7.4103217124938965
epoch20: step2000/4680
step 50000: accuracy:0.0989999994635582, confidence:0.9705924391746521, loss:5.6077070236206055
epoch20: step2500/4680
step 60000: accuracy:0.09399999678134918, confidence:0.99825519323349, loss:8.268328666687012
epoch20: step3000/4680
step 70000: accuracy:0.10300000011920929, confidence:0.9930258989334106, loss:6.766526222229004
epoch20: step3500/4680
step 80000: accuracy:0.09600000083446503, confidence:0.9741567969322205, loss:6.105183124542236
epoch20: step4000/4680
step 90000: accuracy:0.10499999672174454, confidence:0.9678628444671631, loss:6.375214099884033
epoch20: step4500/4680
step 0: accuracy:0.10700000077486038, confidence:0.9879397749900818, loss:7.522614002227783
epoch21: step0/4680
step 10500: accuracy:0.11900000274181366, confidence:0.948699951171875, loss:5.124757766723633
epoch21: step500/4680
step 21000: accuracy:0.09000000357627869, confidence:0.9798271656036377, loss:6.424937725067139
epoch21: step1000/4680
step 31500: accuracy:0.09700000286102295, confidence:0.9987848401069641, loss:9.942143440246582
epoch21: step1500/4680
step 42000: accuracy:0.10300000011920929, confidence:0.9949543476104736, loss:7.125021934509277
epoch21: step2000/4680
step 52500: accuracy:0.10599999874830246, confidence:0.9582851529121399, loss:5.457714080810547
epoch21: step2500/4680
step 63000: accuracy:0.12600000202655792, confidence:0.9986901879310608, loss:8.310396194458008
epoch21: step3000/4680
step 73500: accuracy:0.08299999684095383, confidence:0.98063063621521, loss:5.881349563598633
epoch21: step3500/4680
step 84000: accuracy:0.11500000208616257, confidence:0.945464015007019, loss:5.260582447052002
epoch21: step4000/4680
step 94500: accuracy:0.11400000005960464, confidence:0.924660325050354, loss:5.561890602111816
epoch21: step4500/4680
step 0: accuracy:0.10499999672174454, confidence:0.9701278805732727, loss:6.6546454429626465
epoch22: step0/4680
step 11000: accuracy:0.09799999743700027, confidence:0.8631373643875122, loss:4.730940341949463
epoch22: step500/4680
step 22000: accuracy:0.0860000029206276, confidence:0.9687705039978027, loss:6.2360005378723145
epoch22: step1000/4680
step 33000: accuracy:0.10199999809265137, confidence:0.9991483092308044, loss:10.196338653564453
epoch22: step1500/4680
step 44000: accuracy:0.10100000351667404, confidence:0.9917226433753967, loss:6.6872053146362305
epoch22: step2000/4680
step 55000: accuracy:0.10599999874830246, confidence:0.935876727104187, loss:5.034928798675537
epoch22: step2500/4680
step 66000: accuracy:0.11299999803304672, confidence:0.9979397058486938, loss:8.226099967956543
epoch22: step3000/4680
step 77000: accuracy:0.1080000028014183, confidence:0.9407654404640198, loss:4.930703163146973
epoch22: step3500/4680
step 88000: accuracy:0.125, confidence:0.8889256119728088, loss:4.65237283706665
epoch22: step4000/4680
step 99000: accuracy:0.1809999942779541, confidence:0.9290383458137512, loss:5.524867534637451
epoch22: step4500/4680
step 0: accuracy:0.14499999582767487, confidence:0.9600250124931335, loss:6.688556671142578
epoch23: step0/4680
step 11500: accuracy:0.13199999928474426, confidence:0.7899526357650757, loss:4.251185417175293
epoch23: step500/4680
step 23000: accuracy:0.0729999989271164, confidence:0.9209006428718567, loss:5.632854461669922
epoch23: step1000/4680
step 34500: accuracy:0.11100000143051147, confidence:0.9995940327644348, loss:10.827914237976074
epoch23: step1500/4680
step 46000: accuracy:0.10499999672174454, confidence:0.9948686361312866, loss:7.525671482086182
epoch23: step2000/4680
step 57500: accuracy:0.08399999886751175, confidence:0.9275644421577454, loss:4.746549606323242
epoch23: step2500/4680
step 69000: accuracy:0.10999999940395355, confidence:0.9923456907272339, loss:7.341088771820068
epoch23: step3000/4680
step 80500: accuracy:0.11599999666213989, confidence:0.9294466376304626, loss:4.80974817276001
epoch23: step3500/4680
step 92000: accuracy:0.1459999978542328, confidence:0.9061163663864136, loss:4.717482566833496
epoch23: step4000/4680
step 103500: accuracy:0.18799999356269836, confidence:0.8973718881607056, loss:4.589517593383789
epoch23: step4500/4680
step 0: accuracy:0.1850000023841858, confidence:0.9398208260536194, loss:5.621046543121338
epoch24: step0/4680
step 12000: accuracy:0.2409999966621399, confidence:0.7933815717697144, loss:3.6309876441955566
epoch24: step500/4680
step 24000: accuracy:0.0949999988079071, confidence:0.9191948175430298, loss:5.463352203369141
epoch24: step1000/4680
step 36000: accuracy:0.1120000034570694, confidence:0.9983087182044983, loss:8.96666145324707
epoch24: step1500/4680
step 48000: accuracy:0.1080000028014183, confidence:0.9912242293357849, loss:6.674169063568115
epoch24: step2000/4680
step 60000: accuracy:0.10300000011920929, confidence:0.9337729215621948, loss:4.781951427459717
epoch24: step2500/4680
step 72000: accuracy:0.10199999809265137, confidence:0.9782887697219849, loss:6.7295403480529785
epoch24: step3000/4680
step 84000: accuracy:0.11699999868869781, confidence:0.9042186141014099, loss:4.517353534698486
epoch24: step3500/4680
step 96000: accuracy:0.1599999964237213, confidence:0.9088264107704163, loss:4.662081241607666
epoch24: step4000/4680
step 108000: accuracy:0.20100000500679016, confidence:0.881373405456543, loss:4.363811016082764
epoch24: step4500/4680
step 0: accuracy:0.164000004529953, confidence:0.9363602995872498, loss:5.589511394500732
epoch25: step0/4680
step 12500: accuracy:0.26499998569488525, confidence:0.7885610461235046, loss:3.3185346126556396
epoch25: step500/4680
step 25000: accuracy:0.11599999666213989, confidence:0.8490641117095947, loss:5.00381326675415
epoch25: step1000/4680
step 37500: accuracy:0.09099999815225601, confidence:0.9974347949028015, loss:9.076598167419434
epoch25: step1500/4680
step 50000: accuracy:0.09700000286102295, confidence:0.9850701689720154, loss:6.283613204956055
epoch25: step2000/4680
step 62500: accuracy:0.08399999886751175, confidence:0.9032706022262573, loss:4.371621608734131
epoch25: step2500/4680
step 75000: accuracy:0.15399999916553497, confidence:0.9428650140762329, loss:5.495677947998047
epoch25: step3000/4680
step 87500: accuracy:0.1459999978542328, confidence:0.8783259987831116, loss:4.245489120483398
epoch25: step3500/4680
step 100000: accuracy:0.19300000369548798, confidence:0.8826175332069397, loss:4.136892795562744
epoch25: step4000/4680
step 112500: accuracy:0.20000000298023224, confidence:0.8848335146903992, loss:4.609391689300537
epoch25: step4500/4680
step 0: accuracy:0.19599999487400055, confidence:0.926948070526123, loss:5.3814263343811035
epoch26: step0/4680
step 13000: accuracy:0.25200000405311584, confidence:0.8063245415687561, loss:3.3833274841308594
epoch26: step500/4680
step 26000: accuracy:0.12300000339746475, confidence:0.8703210949897766, loss:5.063582420349121
epoch26: step1000/4680
step 39000: accuracy:0.09799999743700027, confidence:0.9966846704483032, loss:8.949615478515625
epoch26: step1500/4680
step 52000: accuracy:0.11100000143051147, confidence:0.9694501757621765, loss:5.518540859222412
epoch26: step2000/4680
step 65000: accuracy:0.1080000028014183, confidence:0.8944264650344849, loss:4.379207611083984
epoch26: step2500/4680
step 78000: accuracy:0.13500000536441803, confidence:0.9641962647438049, loss:6.29140043258667
epoch26: step3000/4680
step 91000: accuracy:0.19200000166893005, confidence:0.8559234738349915, loss:3.718142509460449
epoch26: step3500/4680
step 104000: accuracy:0.1940000057220459, confidence:0.8763905167579651, loss:4.014955520629883
epoch26: step4000/4680
step 117000: accuracy:0.20600000023841858, confidence:0.8958777189254761, loss:5.115487575531006
epoch26: step4500/4680
step 0: accuracy:0.18799999356269836, confidence:0.9262025356292725, loss:5.6785478591918945
epoch27: step0/4680
step 13500: accuracy:0.25099998712539673, confidence:0.8111929893493652, loss:3.3138554096221924
epoch27: step500/4680
step 27000: accuracy:0.10000000149011612, confidence:0.9355457425117493, loss:6.431205749511719
epoch27: step1000/4680
step 40500: accuracy:0.1080000028014183, confidence:0.9948658347129822, loss:8.708120346069336
epoch27: step1500/4680
step 54000: accuracy:0.09099999815225601, confidence:0.9722540378570557, loss:6.139744281768799
epoch27: step2000/4680
step 67500: accuracy:0.10700000077486038, confidence:0.8995180726051331, loss:4.713346481323242
epoch27: step2500/4680
step 81000: accuracy:0.13600000739097595, confidence:0.9616577625274658, loss:6.412777423858643
epoch27: step3000/4680
step 94500: accuracy:0.1599999964237213, confidence:0.8401898145675659, loss:3.8605055809020996
epoch27: step3500/4680
step 108000: accuracy:0.2370000034570694, confidence:0.8519418835639954, loss:3.454786539077759
epoch27: step4000/4680
step 121500: accuracy:0.23000000417232513, confidence:0.8667097687721252, loss:4.4103007316589355
epoch27: step4500/4680
step 0: accuracy:0.21199999749660492, confidence:0.9270160794258118, loss:5.275364398956299
epoch28: step0/4680
step 14000: accuracy:0.2619999945163727, confidence:0.8266140222549438, loss:3.3747031688690186
epoch28: step500/4680
step 28000: accuracy:0.11500000208616257, confidence:0.9117258787155151, loss:5.59334659576416
epoch28: step1000/4680
step 42000: accuracy:0.10100000351667404, confidence:0.9927109479904175, loss:8.032883644104004
epoch28: step1500/4680
step 56000: accuracy:0.1080000028014183, confidence:0.9683759808540344, loss:5.887148380279541
epoch28: step2000/4680
step 70000: accuracy:0.13199999928474426, confidence:0.8585418462753296, loss:4.428591728210449
epoch28: step2500/4680
step 84000: accuracy:0.15299999713897705, confidence:0.9612544775009155, loss:6.6114888191223145
epoch28: step3000/4680
step 98000: accuracy:0.1889999955892563, confidence:0.8533305525779724, loss:3.8650996685028076
epoch28: step3500/4680
step 112000: accuracy:0.23899999260902405, confidence:0.8200427293777466, loss:3.0595507621765137
epoch28: step4000/4680
step 126000: accuracy:0.22499999403953552, confidence:0.8532110452651978, loss:4.27797269821167
epoch28: step4500/4680
step 0: accuracy:0.210999995470047, confidence:0.9094303250312805, loss:4.999882221221924
epoch29: step0/4680
step 14500: accuracy:0.25200000405311584, confidence:0.8383919596672058, loss:3.3911514282226562
epoch29: step500/4680
step 29000: accuracy:0.15800000727176666, confidence:0.8628310561180115, loss:4.7112836837768555
epoch29: step1000/4680
step 43500: accuracy:0.08799999952316284, confidence:0.990409791469574, loss:8.835886001586914
epoch29: step1500/4680
step 58000: accuracy:0.10599999874830246, confidence:0.9165014624595642, loss:4.606932640075684
epoch29: step2000/4680
step 72500: accuracy:0.15299999713897705, confidence:0.8247514963150024, loss:4.101030349731445
epoch29: step2500/4680
step 87000: accuracy:0.16200000047683716, confidence:0.9599935412406921, loss:6.623126029968262
epoch29: step3000/4680
step 101500: accuracy:0.20999999344348907, confidence:0.853614091873169, loss:3.804497718811035
epoch29: step3500/4680
step 116000: accuracy:0.23499999940395355, confidence:0.8082699179649353, loss:3.0057625770568848
epoch29: step4000/4680
step 130500: accuracy:0.21799999475479126, confidence:0.8510077595710754, loss:4.264754772186279
epoch29: step4500/4680
