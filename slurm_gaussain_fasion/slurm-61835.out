2018-06-15 19:40:08.560045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000b:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:40:08.560105: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000b:01:00.0, compute capability: 6.0)
2018-06-15 19:40:09.965341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000b:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
CHEKPOINT DIR: uniform
model has been loaded from fashion-mnist_classifier.pkl
---------
Variables: name (type shape) [size]
---------
Variable:0 (float32_ref 5x5x1x32) [800, bytes: 3200]
Variable_1:0 (float32_ref 32) [32, bytes: 128]
Variable_2:0 (float32_ref 5x5x32x64) [51200, bytes: 204800]
Variable_3:0 (float32_ref 64) [64, bytes: 256]
Variable_4:0 (float32_ref 3136x1024) [3211264, bytes: 12845056]
Variable_5:0 (float32_ref 1024) [1024, bytes: 4096]
Variable_6:0 (float32_ref 1024x10) [10240, bytes: 40960]
Variable_7:0 (float32_ref 10) [10, bytes: 40]
discriminator/d_conv1/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]
discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]
discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]
discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]
generator/g_fc1/Matrix:0 (float32_ref 74x1024) [75776, bytes: 303104]
generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]
generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]
generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]
classifier/c_fc1/Matrix:0 (float32_ref 1024x64) [65536, bytes: 262144]
classifier/c_fc1/bias:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/beta:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/gamma:0 (float32_ref 64) [64, bytes: 256]
classifier/c_fc2/Matrix:0 (float32_ref 64x12) [768, bytes: 3072]
classifier/c_fc2/bias:0 (float32_ref 12) [12, bytes: 48]
Total size of variables: 16552792
Total bytes of variables: 66211168
 [*] Reading checkpoints...
 [*] Success to read MultiModalInfoGAN.model-43721
 [*] Load SUCCESS


SAMPLES SIZE=4680,LABELS=299520


SAVED TRAINING SET generated_training_set_fashion-mnist_UniformSample_mu_0_sigma_0.15_czcc_czrc_rzcc_rzrc
 [*] Training finished!
 [*] Testing finished!
2018-06-15 19:41:00.660788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000b:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:41:00.660957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000b:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
model has been loaded from fashion-mnist_classifier.pkl
argmax:[3 3 3 ..., 6 6 6]
step 0: accuracy:0.0, confidence:0.9714910387992859, loss:14.669201850891113
Assinging:4
[ 625    0    0 8862    0    0  513]
argmax:[1 1 1 ..., 1 1 1]
step 0: accuracy:0.0, confidence:0.9989244341850281, loss:16.06884002685547
Assinging:2
[    0 10000]
argmax:[7 7 7 ..., 7 7 7]
step 0: accuracy:0.14090000092983246, confidence:0.734927773475647, loss:1.4632506370544434
Assinging:8
[   0    0    0    0    0  200    0 8391    0 1409]
argmax:[0 0 0 ..., 0 0 0]
step 0: accuracy:0.0, confidence:0.9953765869140625, loss:15.222267150878906
Assinging:1
[9920    0    0    0    0    0   80]
argmax:[4 4 4 ..., 2 2 2]
step 0: accuracy:0.0, confidence:0.9291709065437317, loss:11.873727798461914
Assinging:5
[   0    0 1066    0 8463    0  471]
argmax:[9 9 9 ..., 5 5 5]
step 0: accuracy:0.8774999976158142, confidence:0.9917004108428955, loss:1.5468921661376953
Assinging:10
[   0    0    0    0    0 1225    0    0    0 8775]
argmax:[2 2 2 ..., 2 2 2]
step 0: accuracy:0.0, confidence:0.6685035228729248, loss:13.229225158691406
Assinging:3
[   0    0 9220    0  468    0  312]
argmax:[7 7 7 ..., 5 5 5]
step 0: accuracy:0.0, confidence:0.9782641530036926, loss:9.14956283569336
Assinging:8
[   0    0    0    0    0 1576    0 8424]
argmax:[2 2 2 ..., 6 6 2]
step 0: accuracy:0.0, confidence:0.6487152576446533, loss:6.259286403656006
Assinging:3
[   0    0 7606  199 1644    0  551]
argmax:[8 8 8 ..., 8 8 8]
step 0: accuracy:0.0, confidence:0.9997271299362183, loss:16.723506927490234
Assinging:9
[    0     0     0     0     0     0     0     0 10000]
2018-06-15 19:41:15.590217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000b:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:41:15.590447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000b:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
step 0: accuracy:0.09700000286102295, confidence:0.9653154015541077, loss:8.655978202819824
epoch0: step0/4680
step 0: accuracy:0.11500000208616257, confidence:1.0, loss:27.600902557373047
epoch0: step500/4680
step 0: accuracy:0.08799999952316284, confidence:1.0, loss:35.33317565917969
epoch0: step1000/4680
step 0: accuracy:0.10400000214576721, confidence:0.9999517798423767, loss:12.346650123596191
epoch0: step1500/4680
step 0: accuracy:0.1120000034570694, confidence:0.9994349479675293, loss:9.809842109680176
epoch0: step2000/4680
step 0: accuracy:0.10400000214576721, confidence:0.9983705878257751, loss:8.599135398864746
epoch0: step2500/4680
step 0: accuracy:0.08900000154972076, confidence:0.9981547594070435, loss:8.832263946533203
epoch0: step3000/4680
step 0: accuracy:0.1080000028014183, confidence:0.9989151358604431, loss:8.606307983398438
epoch0: step3500/4680
step 0: accuracy:0.09099999815225601, confidence:0.9989537596702576, loss:9.124727249145508
epoch0: step4000/4680
step 0: accuracy:0.09700000286102295, confidence:0.9792414903640747, loss:6.232550144195557
epoch0: step4500/4680
step 0: accuracy:0.0860000029206276, confidence:0.9880239367485046, loss:6.962220668792725
epoch1: step0/4680
step 500: accuracy:0.09000000357627869, confidence:0.9967495799064636, loss:8.38357162475586
epoch1: step500/4680
step 1000: accuracy:0.1120000034570694, confidence:0.9910329580307007, loss:7.481375694274902
epoch1: step1000/4680
step 1500: accuracy:0.0860000029206276, confidence:0.9965312480926514, loss:8.034209251403809
epoch1: step1500/4680
step 2000: accuracy:0.10100000351667404, confidence:0.9849594831466675, loss:6.505215644836426
epoch1: step2000/4680
step 2500: accuracy:0.10999999940395355, confidence:0.9961680173873901, loss:8.335613250732422
epoch1: step2500/4680
step 3000: accuracy:0.10700000077486038, confidence:0.9960030913352966, loss:7.75754451751709
epoch1: step3000/4680
step 3500: accuracy:0.10499999672174454, confidence:0.9979118704795837, loss:8.052590370178223
epoch1: step3500/4680
step 4000: accuracy:0.09200000017881393, confidence:0.9968976974487305, loss:7.883708477020264
epoch1: step4000/4680
step 4500: accuracy:0.09200000017881393, confidence:0.9630979895591736, loss:5.684189796447754
epoch1: step4500/4680
step 0: accuracy:0.10899999737739563, confidence:0.9793331027030945, loss:6.166694641113281
epoch2: step0/4680
step 1000: accuracy:0.10700000077486038, confidence:0.9937732815742493, loss:7.556487083435059
epoch2: step500/4680
step 2000: accuracy:0.11500000208616257, confidence:0.9966580867767334, loss:9.180171966552734
epoch2: step1000/4680
step 3000: accuracy:0.09799999743700027, confidence:0.9846637845039368, loss:6.500950813293457
epoch2: step1500/4680
step 4000: accuracy:0.09000000357627869, confidence:0.9937663674354553, loss:8.381271362304688
epoch2: step2000/4680
step 5000: accuracy:0.09200000017881393, confidence:0.9954098463058472, loss:8.330999374389648
epoch2: step2500/4680
step 6000: accuracy:0.10000000149011612, confidence:0.9449585676193237, loss:5.302084445953369
epoch2: step3000/4680
step 7000: accuracy:0.09399999678134918, confidence:0.9925990104675293, loss:7.414314270019531
epoch2: step3500/4680
step 8000: accuracy:0.09399999678134918, confidence:0.9974246025085449, loss:8.501635551452637
epoch2: step4000/4680
step 9000: accuracy:0.08500000089406967, confidence:0.7925574779510498, loss:5.417360305786133
epoch2: step4500/4680
step 0: accuracy:0.0949999988079071, confidence:0.7748827338218689, loss:5.458133697509766
epoch3: step0/4680
step 1500: accuracy:0.08699999749660492, confidence:0.9843519926071167, loss:6.92672872543335
epoch3: step500/4680
step 3000: accuracy:0.09200000017881393, confidence:0.9991381168365479, loss:10.775514602661133
epoch3: step1000/4680
step 4500: accuracy:0.11500000208616257, confidence:0.9952486753463745, loss:7.8667192459106445
epoch3: step1500/4680
step 6000: accuracy:0.10100000351667404, confidence:0.9236649870872498, loss:8.70971965789795
epoch3: step2000/4680
step 7500: accuracy:0.10199999809265137, confidence:0.9960949420928955, loss:8.547118186950684
epoch3: step2500/4680
step 9000: accuracy:0.1120000034570694, confidence:0.853895902633667, loss:4.2124433517456055
epoch3: step3000/4680
step 10500: accuracy:0.08699999749660492, confidence:0.9976274967193604, loss:8.684969902038574
epoch3: step3500/4680
step 12000: accuracy:0.10599999874830246, confidence:0.9944976568222046, loss:8.80603313446045
epoch3: step4000/4680
step 13500: accuracy:0.11699999868869781, confidence:0.8768747448921204, loss:6.89439582824707
epoch3: step4500/4680
step 0: accuracy:0.09399999678134918, confidence:0.8513503074645996, loss:6.824486255645752
epoch4: step0/4680
step 2000: accuracy:0.10400000214576721, confidence:0.9720824360847473, loss:6.499670028686523
epoch4: step500/4680
step 4000: accuracy:0.12099999934434891, confidence:0.975977897644043, loss:7.019006252288818
epoch4: step1000/4680
step 6000: accuracy:0.10100000351667404, confidence:0.9684112071990967, loss:8.331567764282227
epoch4: step1500/4680
step 8000: accuracy:0.10499999672174454, confidence:0.987835168838501, loss:10.234349250793457
epoch4: step2000/4680
step 10000: accuracy:0.10199999809265137, confidence:0.9952241778373718, loss:7.995532512664795
epoch4: step2500/4680
step 12000: accuracy:0.0949999988079071, confidence:0.9469020962715149, loss:5.257033348083496
epoch4: step3000/4680
step 14000: accuracy:0.08900000154972076, confidence:0.9638257026672363, loss:6.9267659187316895
epoch4: step3500/4680
step 16000: accuracy:0.1120000034570694, confidence:0.9646314978599548, loss:7.329225063323975
epoch4: step4000/4680
step 18000: accuracy:0.10000000149011612, confidence:0.8727425336837769, loss:7.200228691101074
epoch4: step4500/4680
step 0: accuracy:0.0820000022649765, confidence:0.95252525806427, loss:8.320661544799805
epoch5: step0/4680
step 2500: accuracy:0.09700000286102295, confidence:0.9969324469566345, loss:8.45766830444336
epoch5: step500/4680
step 5000: accuracy:0.12600000202655792, confidence:0.9996629953384399, loss:10.732953071594238
epoch5: step1000/4680
step 7500: accuracy:0.0949999988079071, confidence:0.9954692125320435, loss:9.556650161743164
epoch5: step1500/4680
step 10000: accuracy:0.09799999743700027, confidence:0.837478518486023, loss:10.065716743469238
epoch5: step2000/4680
step 12500: accuracy:0.11100000143051147, confidence:0.9991380572319031, loss:9.94084358215332
epoch5: step2500/4680
step 15000: accuracy:0.09700000286102295, confidence:0.9928798675537109, loss:7.2521820068359375
epoch5: step3000/4680
step 17500: accuracy:0.11999999731779099, confidence:0.9741501212120056, loss:7.163689136505127
epoch5: step3500/4680
step 20000: accuracy:0.1289999932050705, confidence:0.9370425939559937, loss:7.3530683517456055
epoch5: step4000/4680
step 22500: accuracy:0.10100000351667404, confidence:0.8564820289611816, loss:7.065061569213867
epoch5: step4500/4680
step 0: accuracy:0.1080000028014183, confidence:0.8237171769142151, loss:6.761975288391113
epoch6: step0/4680
step 3000: accuracy:0.10499999672174454, confidence:0.9836093783378601, loss:8.871783256530762
epoch6: step500/4680
step 6000: accuracy:0.11500000208616257, confidence:0.915738046169281, loss:6.062807559967041
epoch6: step1000/4680
step 9000: accuracy:0.052000001072883606, confidence:0.8110107183456421, loss:7.900461196899414
epoch6: step1500/4680
step 12000: accuracy:0.10300000011920929, confidence:0.9738397002220154, loss:13.357853889465332
epoch6: step2000/4680
step 15000: accuracy:0.11299999803304672, confidence:0.9089838862419128, loss:5.563854217529297
epoch6: step2500/4680
step 18000: accuracy:0.10000000149011612, confidence:0.953113853931427, loss:5.847138404846191
epoch6: step3000/4680
step 21000: accuracy:0.09000000357627869, confidence:0.9806548953056335, loss:8.961730003356934
epoch6: step3500/4680
step 24000: accuracy:0.08900000154972076, confidence:0.9954082369804382, loss:9.722091674804688
epoch6: step4000/4680
step 27000: accuracy:0.0989999994635582, confidence:0.9160014986991882, loss:6.80234432220459
epoch6: step4500/4680
step 0: accuracy:0.1289999932050705, confidence:0.9014229774475098, loss:6.401968955993652
epoch7: step0/4680
step 3500: accuracy:0.09799999743700027, confidence:0.9344907999038696, loss:7.13669490814209
epoch7: step500/4680
step 7000: accuracy:0.10499999672174454, confidence:0.8659906983375549, loss:5.722999572753906
epoch7: step1000/4680
step 10500: accuracy:0.07699999958276749, confidence:0.8155217170715332, loss:7.45862340927124
epoch7: step1500/4680
step 14000: accuracy:0.1080000028014183, confidence:0.8261356353759766, loss:12.65749740600586
epoch7: step2000/4680
step 17500: accuracy:0.09600000083446503, confidence:0.9389437437057495, loss:6.538189888000488
epoch7: step2500/4680
step 21000: accuracy:0.09399999678134918, confidence:0.9147984981536865, loss:5.705245494842529
epoch7: step3000/4680
step 24500: accuracy:0.10499999672174454, confidence:0.8775636553764343, loss:7.405525207519531
epoch7: step3500/4680
step 28000: accuracy:0.1080000028014183, confidence:0.9781179428100586, loss:9.218057632446289
epoch7: step4000/4680
step 31500: accuracy:0.10300000011920929, confidence:0.8696842193603516, loss:7.3583984375
epoch7: step4500/4680
step 0: accuracy:0.09200000017881393, confidence:0.8558429479598999, loss:7.1941819190979
epoch8: step0/4680
step 4000: accuracy:0.10000000149011612, confidence:0.9628353714942932, loss:7.429196834564209
epoch8: step500/4680
step 8000: accuracy:0.10000000149011612, confidence:0.7981681227684021, loss:5.247359275817871
epoch8: step1000/4680
step 12000: accuracy:0.0689999982714653, confidence:0.8154557943344116, loss:7.133959770202637
epoch8: step1500/4680
step 16000: accuracy:0.10499999672174454, confidence:0.7688584327697754, loss:10.431662559509277
epoch8: step2000/4680
step 20000: accuracy:0.09799999743700027, confidence:0.9070508480072021, loss:6.6913042068481445
epoch8: step2500/4680
step 24000: accuracy:0.07000000029802322, confidence:0.8368735909461975, loss:5.698477268218994
epoch8: step3000/4680
step 28000: accuracy:0.0949999988079071, confidence:0.8189297318458557, loss:7.346002578735352
epoch8: step3500/4680
step 32000: accuracy:0.0989999994635582, confidence:0.9754981398582458, loss:9.122834205627441
epoch8: step4000/4680
step 36000: accuracy:0.10199999809265137, confidence:0.8767854571342468, loss:7.470276355743408
epoch8: step4500/4680
step 0: accuracy:0.09600000083446503, confidence:0.8423661589622498, loss:7.108388900756836
epoch9: step0/4680
step 4500: accuracy:0.09200000017881393, confidence:0.9542984366416931, loss:7.6789727210998535
epoch9: step500/4680
step 9000: accuracy:0.11599999666213989, confidence:0.8393010497093201, loss:5.700979709625244
epoch9: step1000/4680
step 13500: accuracy:0.06599999964237213, confidence:0.7987781763076782, loss:7.1526875495910645
epoch9: step1500/4680
step 18000: accuracy:0.1379999965429306, confidence:0.7824349403381348, loss:9.960782051086426
epoch9: step2000/4680
step 22500: accuracy:0.10700000077486038, confidence:0.9087179899215698, loss:7.149001121520996
epoch9: step2500/4680
step 27000: accuracy:0.07599999755620956, confidence:0.8964059948921204, loss:6.780206680297852
epoch9: step3000/4680
step 31500: accuracy:0.10599999874830246, confidence:0.8681418299674988, loss:7.888433933258057
epoch9: step3500/4680
step 36000: accuracy:0.0860000029206276, confidence:0.9928281307220459, loss:11.145994186401367
epoch9: step4000/4680
step 40500: accuracy:0.11100000143051147, confidence:0.926937997341156, loss:7.913479804992676
epoch9: step4500/4680
step 0: accuracy:0.1080000028014183, confidence:0.906330406665802, loss:7.883456230163574
epoch10: step0/4680
step 5000: accuracy:0.0949999988079071, confidence:0.9452671408653259, loss:7.841658592224121
epoch10: step500/4680
step 10000: accuracy:0.11800000071525574, confidence:0.802538275718689, loss:5.798087120056152
epoch10: step1000/4680
step 15000: accuracy:0.05900000035762787, confidence:0.8129230737686157, loss:7.452826499938965
epoch10: step1500/4680
step 20000: accuracy:0.13600000739097595, confidence:0.7762673497200012, loss:10.638623237609863
epoch10: step2000/4680
step 25000: accuracy:0.17900000512599945, confidence:0.7530884742736816, loss:5.650158405303955
epoch10: step2500/4680
step 30000: accuracy:0.10300000011920929, confidence:0.8429614305496216, loss:6.172004699707031
epoch10: step3000/4680
step 35000: accuracy:0.09200000017881393, confidence:0.7552751302719116, loss:6.968796253204346
epoch10: step3500/4680
step 40000: accuracy:0.11500000208616257, confidence:0.9724369049072266, loss:10.571746826171875
epoch10: step4000/4680
step 45000: accuracy:0.09600000083446503, confidence:0.8908348679542542, loss:8.44072437286377
epoch10: step4500/4680
step 0: accuracy:0.09300000220537186, confidence:0.8781744241714478, loss:8.432462692260742
epoch11: step0/4680
step 5500: accuracy:0.10499999672174454, confidence:0.8957221508026123, loss:6.868556976318359
epoch11: step500/4680
step 11000: accuracy:0.11699999868869781, confidence:0.7751134634017944, loss:5.782684803009033
epoch11: step1000/4680
step 16500: accuracy:0.09600000083446503, confidence:0.8528919219970703, loss:8.641866683959961
epoch11: step1500/4680
step 22000: accuracy:0.11400000005960464, confidence:0.8658573627471924, loss:10.988459587097168
epoch11: step2000/4680
step 27500: accuracy:0.0989999994635582, confidence:0.8596803545951843, loss:7.582620620727539
epoch11: step2500/4680
step 33000: accuracy:0.07800000160932541, confidence:0.9244862794876099, loss:7.992095470428467
epoch11: step3000/4680
step 38500: accuracy:0.12999999523162842, confidence:0.7881162166595459, loss:7.944930076599121
epoch11: step3500/4680
step 44000: accuracy:0.09600000083446503, confidence:0.9880309104919434, loss:12.176108360290527
epoch11: step4000/4680
step 49500: accuracy:0.10599999874830246, confidence:0.9086180329322815, loss:9.384725570678711
epoch11: step4500/4680
step 0: accuracy:0.1120000034570694, confidence:0.8990739583969116, loss:9.229208946228027
epoch12: step0/4680
step 6000: accuracy:0.10100000351667404, confidence:0.9775099158287048, loss:11.283008575439453
epoch12: step500/4680
step 12000: accuracy:0.11999999731779099, confidence:0.9144223928451538, loss:8.507800102233887
epoch12: step1000/4680
step 18000: accuracy:0.06199999898672104, confidence:0.8318799734115601, loss:8.537075996398926
epoch12: step1500/4680
step 24000: accuracy:0.13600000739097595, confidence:0.8001338839530945, loss:11.436558723449707
epoch12: step2000/4680
step 30000: accuracy:0.14300000667572021, confidence:0.7985880374908447, loss:6.272012710571289
epoch12: step2500/4680
step 36000: accuracy:0.08799999952316284, confidence:0.996052086353302, loss:10.523781776428223
epoch12: step3000/4680
step 42000: accuracy:0.09799999743700027, confidence:0.8088197112083435, loss:7.712743759155273
epoch12: step3500/4680
step 48000: accuracy:0.10199999809265137, confidence:0.9910565614700317, loss:12.61829948425293
epoch12: step4000/4680
step 54000: accuracy:0.10999999940395355, confidence:0.9394741058349609, loss:9.309663772583008
epoch12: step4500/4680
step 0: accuracy:0.0949999988079071, confidence:0.9387958645820618, loss:9.622489929199219
epoch13: step0/4680
step 6500: accuracy:0.10599999874830246, confidence:0.9581347107887268, loss:9.404027938842773
epoch13: step500/4680
step 13000: accuracy:0.11800000071525574, confidence:0.8805534839630127, loss:8.570252418518066
epoch13: step1000/4680
step 19500: accuracy:0.052000001072883606, confidence:0.8113077282905579, loss:8.911280632019043
epoch13: step1500/4680
step 26000: accuracy:0.15299999713897705, confidence:0.7683756351470947, loss:10.665070533752441
epoch13: step2000/4680
step 32500: accuracy:0.11400000005960464, confidence:0.7954219579696655, loss:6.802427291870117
epoch13: step2500/4680
step 39000: accuracy:0.10400000214576721, confidence:0.941121518611908, loss:9.840803146362305
epoch13: step3000/4680
step 45500: accuracy:0.12800000607967377, confidence:0.8308320045471191, loss:8.931081771850586
epoch13: step3500/4680
step 52000: accuracy:0.10999999940395355, confidence:0.9811515212059021, loss:11.678544998168945
epoch13: step4000/4680
step 58500: accuracy:0.10999999940395355, confidence:0.9440150856971741, loss:9.907513618469238
epoch13: step4500/4680
step 0: accuracy:0.09799999743700027, confidence:0.9295225143432617, loss:9.967113494873047
epoch14: step0/4680
step 7000: accuracy:0.09799999743700027, confidence:0.9391604065895081, loss:9.623604774475098
epoch14: step500/4680
step 14000: accuracy:0.11500000208616257, confidence:0.8449908494949341, loss:7.580943584442139
epoch14: step1000/4680
step 21000: accuracy:0.054999999701976776, confidence:0.8462952971458435, loss:8.73482894897461
epoch14: step1500/4680
step 28000: accuracy:0.13899999856948853, confidence:0.7837681174278259, loss:11.234245300292969
epoch14: step2000/4680
step 35000: accuracy:0.13899999856948853, confidence:0.7928285598754883, loss:6.4326171875
epoch14: step2500/4680
step 42000: accuracy:0.08299999684095383, confidence:0.8927571177482605, loss:7.515992164611816
epoch14: step3000/4680
step 49000: accuracy:0.05299999937415123, confidence:0.8095537424087524, loss:7.44874906539917
epoch14: step3500/4680
step 56000: accuracy:0.10999999940395355, confidence:0.9687761068344116, loss:10.876495361328125
epoch14: step4000/4680
step 63000: accuracy:0.09600000083446503, confidence:0.9269517660140991, loss:9.25255012512207
epoch14: step4500/4680
step 0: accuracy:0.10199999809265137, confidence:0.9096121788024902, loss:8.621237754821777
epoch15: step0/4680
step 7500: accuracy:0.09600000083446503, confidence:0.9281892776489258, loss:8.856927871704102
epoch15: step500/4680
step 15000: accuracy:0.125, confidence:0.8484742641448975, loss:7.039070129394531
epoch15: step1000/4680
step 22500: accuracy:0.05999999865889549, confidence:0.8268070816993713, loss:8.603742599487305
epoch15: step1500/4680
step 30000: accuracy:0.12800000607967377, confidence:0.8010900616645813, loss:11.5150785446167
epoch15: step2000/4680
step 37500: accuracy:0.14000000059604645, confidence:0.7771598100662231, loss:6.628052711486816
epoch15: step2500/4680
step 45000: accuracy:0.06800000369548798, confidence:0.9136995673179626, loss:8.90096664428711
epoch15: step3000/4680
step 52500: accuracy:0.09700000286102295, confidence:0.802297055721283, loss:7.935989856719971
epoch15: step3500/4680
step 60000: accuracy:0.0860000029206276, confidence:0.9860283136367798, loss:13.957106590270996
epoch15: step4000/4680
step 67500: accuracy:0.12399999797344208, confidence:0.9152231216430664, loss:9.613534927368164
epoch15: step4500/4680
step 0: accuracy:0.11800000071525574, confidence:0.9222313761711121, loss:9.708772659301758
epoch16: step0/4680
step 8000: accuracy:0.09600000083446503, confidence:0.9481708407402039, loss:10.741714477539062
epoch16: step500/4680
step 16000: accuracy:0.10100000351667404, confidence:0.8898707032203674, loss:8.613944053649902
epoch16: step1000/4680
step 24000: accuracy:0.05700000002980232, confidence:0.8445605635643005, loss:9.26888656616211
epoch16: step1500/4680
step 32000: accuracy:0.12200000137090683, confidence:0.8103581070899963, loss:12.252755165100098
epoch16: step2000/4680
step 40000: accuracy:0.12800000607967377, confidence:0.8059397339820862, loss:6.616533279418945
epoch16: step2500/4680
step 48000: accuracy:0.07500000298023224, confidence:0.8652967214584351, loss:7.444711208343506
epoch16: step3000/4680
step 56000: accuracy:0.05999999865889549, confidence:0.7957362532615662, loss:7.65419340133667
epoch16: step3500/4680
step 64000: accuracy:0.10700000077486038, confidence:0.9757571220397949, loss:11.958841323852539
epoch16: step4000/4680
step 72000: accuracy:0.10599999874830246, confidence:0.9165624380111694, loss:9.44947338104248
epoch16: step4500/4680
step 0: accuracy:0.11100000143051147, confidence:0.9145171642303467, loss:9.020509719848633
epoch17: step0/4680
step 8500: accuracy:0.1080000028014183, confidence:0.9273890852928162, loss:9.190631866455078
epoch17: step500/4680
step 17000: accuracy:0.12600000202655792, confidence:0.8237022757530212, loss:7.364274501800537
epoch17: step1000/4680
step 25500: accuracy:0.04899999871850014, confidence:0.8377440571784973, loss:8.496240615844727
epoch17: step1500/4680
step 34000: accuracy:0.1340000033378601, confidence:0.806083619594574, loss:11.478042602539062
epoch17: step2000/4680
step 42500: accuracy:0.10700000077486038, confidence:0.7618069648742676, loss:6.719274044036865
epoch17: step2500/4680
step 51000: accuracy:0.08699999749660492, confidence:0.924237072467804, loss:9.061588287353516
epoch17: step3000/4680
step 59500: accuracy:0.09200000017881393, confidence:0.8132831454277039, loss:9.009381294250488
epoch17: step3500/4680
step 68000: accuracy:0.09399999678134918, confidence:0.9874810576438904, loss:13.267049789428711
epoch17: step4000/4680
step 76500: accuracy:0.11699999868869781, confidence:0.9362902641296387, loss:10.273691177368164
epoch17: step4500/4680
step 0: accuracy:0.10999999940395355, confidence:0.9283778071403503, loss:10.169965744018555
epoch18: step0/4680
step 9000: accuracy:0.09799999743700027, confidence:0.9275070428848267, loss:9.934881210327148
epoch18: step500/4680
step 18000: accuracy:0.10199999809265137, confidence:0.8628327250480652, loss:8.260281562805176
epoch18: step1000/4680
step 27000: accuracy:0.06800000369548798, confidence:0.8555394411087036, loss:9.382991790771484
epoch18: step1500/4680
step 36000: accuracy:0.15199999511241913, confidence:0.8435505628585815, loss:12.03056526184082
epoch18: step2000/4680
step 45000: accuracy:0.09399999678134918, confidence:0.7943004965782166, loss:6.706161975860596
epoch18: step2500/4680
step 54000: accuracy:0.061000000685453415, confidence:0.8463904857635498, loss:7.798611640930176
epoch18: step3000/4680
step 63000: accuracy:0.07500000298023224, confidence:0.8055594563484192, loss:7.9184250831604
epoch18: step3500/4680
step 72000: accuracy:0.0860000029206276, confidence:0.977489709854126, loss:13.058073997497559
epoch18: step4000/4680
step 81000: accuracy:0.0989999994635582, confidence:0.933357298374176, loss:10.396713256835938
epoch18: step4500/4680
step 0: accuracy:0.10199999809265137, confidence:0.9223370552062988, loss:9.753795623779297
epoch19: step0/4680
step 9500: accuracy:0.12099999934434891, confidence:0.9223460555076599, loss:9.32247543334961
epoch19: step500/4680
step 19000: accuracy:0.12700000405311584, confidence:0.8124791383743286, loss:7.394728660583496
epoch19: step1000/4680
step 28500: accuracy:0.06599999964237213, confidence:0.8476412296295166, loss:9.224373817443848
epoch19: step1500/4680
step 38000: accuracy:0.15000000596046448, confidence:0.8431858420372009, loss:11.033658027648926
epoch19: step2000/4680
step 47500: accuracy:0.10899999737739563, confidence:0.7799987196922302, loss:6.8209919929504395
epoch19: step2500/4680
step 57000: accuracy:0.07199999690055847, confidence:0.9194914698600769, loss:8.79695987701416
epoch19: step3000/4680
step 66500: accuracy:0.0949999988079071, confidence:0.8185522556304932, loss:9.115093231201172
epoch19: step3500/4680
step 76000: accuracy:0.10700000077486038, confidence:0.9850849509239197, loss:13.769312858581543
epoch19: step4000/4680
step 85500: accuracy:0.11100000143051147, confidence:0.9351291656494141, loss:10.780828475952148
epoch19: step4500/4680
step 0: accuracy:0.09799999743700027, confidence:0.9241217374801636, loss:10.568140029907227
epoch20: step0/4680
step 10000: accuracy:0.09099999815225601, confidence:0.9439901113510132, loss:11.662914276123047
epoch20: step500/4680
step 20000: accuracy:0.1080000028014183, confidence:0.8786531686782837, loss:8.354961395263672
epoch20: step1000/4680
step 30000: accuracy:0.08100000023841858, confidence:0.859623908996582, loss:9.510869979858398
epoch20: step1500/4680
step 40000: accuracy:0.11100000143051147, confidence:0.8674960136413574, loss:12.213715553283691
epoch20: step2000/4680
step 50000: accuracy:0.10499999672174454, confidence:0.8012678027153015, loss:6.141880035400391
epoch20: step2500/4680
step 60000: accuracy:0.061000000685453415, confidence:0.9005875587463379, loss:8.817150115966797
epoch20: step3000/4680
step 70000: accuracy:0.07800000160932541, confidence:0.8166758418083191, loss:8.693526268005371
epoch20: step3500/4680
step 80000: accuracy:0.10999999940395355, confidence:0.9880586266517639, loss:13.80380916595459
epoch20: step4000/4680
step 90000: accuracy:0.125, confidence:0.924197793006897, loss:9.765022277832031
epoch20: step4500/4680
step 0: accuracy:0.11299999803304672, confidence:0.9240666627883911, loss:9.569770812988281
epoch21: step0/4680
step 10500: accuracy:0.09799999743700027, confidence:0.9224186539649963, loss:9.592430114746094
epoch21: step500/4680
step 21000: accuracy:0.13500000536441803, confidence:0.8316734433174133, loss:7.747868061065674
epoch21: step1000/4680
step 31500: accuracy:0.05700000002980232, confidence:0.8336028456687927, loss:9.222490310668945
epoch21: step1500/4680
step 42000: accuracy:0.1599999964237213, confidence:0.8548129796981812, loss:13.877764701843262
epoch21: step2000/4680
step 52500: accuracy:0.09300000220537186, confidence:0.7811726331710815, loss:8.05634880065918
epoch21: step2500/4680
step 63000: accuracy:0.09700000286102295, confidence:0.9612748026847839, loss:9.914690971374512
epoch21: step3000/4680
step 73500: accuracy:0.09700000286102295, confidence:0.8126530051231384, loss:9.074007987976074
epoch21: step3500/4680
step 84000: accuracy:0.125, confidence:0.9604943990707397, loss:11.590738296508789
epoch21: step4000/4680
step 94500: accuracy:0.09099999815225601, confidence:0.9273986220359802, loss:11.100050926208496
epoch21: step4500/4680
step 0: accuracy:0.11599999666213989, confidence:0.928126871585846, loss:10.176236152648926
epoch22: step0/4680
step 11000: accuracy:0.09600000083446503, confidence:0.9102509617805481, loss:9.894183158874512
epoch22: step500/4680
step 22000: accuracy:0.10300000011920929, confidence:0.8701475858688354, loss:8.323870658874512
epoch22: step1000/4680
step 33000: accuracy:0.0729999989271164, confidence:0.8618447780609131, loss:9.227631568908691
epoch22: step1500/4680
step 44000: accuracy:0.15399999916553497, confidence:0.8649699091911316, loss:10.972293853759766
epoch22: step2000/4680
step 55000: accuracy:0.1379999965429306, confidence:0.8452580571174622, loss:6.592794418334961
epoch22: step2500/4680
step 66000: accuracy:0.0729999989271164, confidence:0.9049499034881592, loss:8.667247772216797
epoch22: step3000/4680
step 77000: accuracy:0.10000000149011612, confidence:0.8018639087677002, loss:7.727964401245117
epoch22: step3500/4680
step 88000: accuracy:0.10199999809265137, confidence:0.9684016108512878, loss:11.824982643127441
epoch22: step4000/4680
step 99000: accuracy:0.11100000143051147, confidence:0.9124612808227539, loss:9.295291900634766
epoch22: step4500/4680
step 0: accuracy:0.10999999940395355, confidence:0.9038075804710388, loss:9.039543151855469
epoch23: step0/4680
step 11500: accuracy:0.09099999815225601, confidence:0.9273776412010193, loss:9.887019157409668
epoch23: step500/4680
step 23000: accuracy:0.125, confidence:0.8282009363174438, loss:7.4760823249816895
epoch23: step1000/4680
step 34500: accuracy:0.061000000685453415, confidence:0.8497924208641052, loss:9.423263549804688
epoch23: step1500/4680
step 46000: accuracy:0.1459999978542328, confidence:0.8772701621055603, loss:11.905625343322754
epoch23: step2000/4680
step 57500: accuracy:0.07500000298023224, confidence:0.7929799556732178, loss:8.081178665161133
epoch23: step2500/4680
step 69000: accuracy:0.06599999964237213, confidence:0.9073281288146973, loss:9.234023094177246
epoch23: step3000/4680
step 80500: accuracy:0.12200000137090683, confidence:0.8072333931922913, loss:8.726224899291992
epoch23: step3500/4680
step 92000: accuracy:0.11500000208616257, confidence:0.9625371098518372, loss:12.196087837219238
epoch23: step4000/4680
step 103500: accuracy:0.1120000034570694, confidence:0.9104245901107788, loss:10.187521934509277
epoch23: step4500/4680
step 0: accuracy:0.12399999797344208, confidence:0.9126401543617249, loss:9.672893524169922
epoch24: step0/4680
step 12000: accuracy:0.10400000214576721, confidence:0.9210622310638428, loss:10.679107666015625
epoch24: step500/4680
step 24000: accuracy:0.11299999803304672, confidence:0.8574537634849548, loss:8.054776191711426
epoch24: step1000/4680
step 36000: accuracy:0.09099999815225601, confidence:0.8545975089073181, loss:9.527572631835938
epoch24: step1500/4680
step 48000: accuracy:0.17100000381469727, confidence:0.8552592396736145, loss:11.919573783874512
epoch24: step2000/4680
step 60000: accuracy:0.11100000143051147, confidence:0.7969598174095154, loss:6.298773288726807
epoch24: step2500/4680
step 72000: accuracy:0.0860000029206276, confidence:0.9319484233856201, loss:9.154290199279785
epoch24: step3000/4680
step 84000: accuracy:0.07100000232458115, confidence:0.8043815493583679, loss:7.927337169647217
epoch24: step3500/4680
step 96000: accuracy:0.09399999678134918, confidence:0.9797240495681763, loss:13.0573148727417
epoch24: step4000/4680
step 108000: accuracy:0.10499999672174454, confidence:0.91096031665802, loss:9.767212867736816
epoch24: step4500/4680
step 0: accuracy:0.1080000028014183, confidence:0.9042776823043823, loss:9.890205383300781
epoch25: step0/4680
step 12500: accuracy:0.1120000034570694, confidence:0.9203878045082092, loss:9.335214614868164
epoch25: step500/4680
step 25000: accuracy:0.0989999994635582, confidence:0.8331069350242615, loss:7.820468425750732
epoch25: step1000/4680
step 37500: accuracy:0.07699999958276749, confidence:0.8521137833595276, loss:9.377930641174316
epoch25: step1500/4680
step 50000: accuracy:0.14000000059604645, confidence:0.8533494472503662, loss:11.522753715515137
epoch25: step2000/4680
step 62500: accuracy:0.08299999684095383, confidence:0.7888639569282532, loss:7.489083290100098
epoch25: step2500/4680
step 75000: accuracy:0.057999998331069946, confidence:0.869875967502594, loss:8.347131729125977
epoch25: step3000/4680
step 87500: accuracy:0.13300000131130219, confidence:0.8033179044723511, loss:8.108887672424316
epoch25: step3500/4680
step 100000: accuracy:0.09700000286102295, confidence:0.9728034138679504, loss:13.47807502746582
epoch25: step4000/4680
step 112500: accuracy:0.12800000607967377, confidence:0.926979660987854, loss:10.392910957336426
epoch25: step4500/4680
step 0: accuracy:0.09700000286102295, confidence:0.9130904674530029, loss:10.494658470153809
epoch26: step0/4680
step 13000: accuracy:0.09000000357627869, confidence:0.9028635025024414, loss:11.109712600708008
epoch26: step500/4680
step 26000: accuracy:0.11100000143051147, confidence:0.8575328588485718, loss:8.321979522705078
epoch26: step1000/4680
step 39000: accuracy:0.08100000023841858, confidence:0.8673511743545532, loss:10.29482650756836
epoch26: step1500/4680
step 52000: accuracy:0.13600000739097595, confidence:0.8429368734359741, loss:10.685386657714844
epoch26: step2000/4680
step 65000: accuracy:0.0729999989271164, confidence:0.7938475012779236, loss:7.238358020782471
epoch26: step2500/4680
step 78000: accuracy:0.07699999958276749, confidence:0.9200565218925476, loss:8.938980102539062
epoch26: step3000/4680
step 91000: accuracy:0.08299999684095383, confidence:0.8135578632354736, loss:8.591140747070312
epoch26: step3500/4680
step 104000: accuracy:0.11999999731779099, confidence:0.9811714887619019, loss:12.659786224365234
epoch26: step4000/4680
step 117000: accuracy:0.1080000028014183, confidence:0.9361965656280518, loss:10.101831436157227
epoch26: step4500/4680
step 0: accuracy:0.0989999994635582, confidence:0.9178646206855774, loss:9.717267036437988
epoch27: step0/4680
step 13500: accuracy:0.10700000077486038, confidence:0.9345058798789978, loss:10.125236511230469
epoch27: step500/4680
step 27000: accuracy:0.1120000034570694, confidence:0.8770818114280701, loss:8.461761474609375
epoch27: step1000/4680
step 40500: accuracy:0.07900000363588333, confidence:0.8649951219558716, loss:10.035750389099121
epoch27: step1500/4680
step 54000: accuracy:0.1469999998807907, confidence:0.8816717863082886, loss:12.163281440734863
epoch27: step2000/4680
step 67500: accuracy:0.06400000303983688, confidence:0.800774335861206, loss:7.315746784210205
epoch27: step2500/4680
step 81000: accuracy:0.05999999865889549, confidence:0.8746907711029053, loss:8.845675468444824
epoch27: step3000/4680
step 94500: accuracy:0.09399999678134918, confidence:0.8050447702407837, loss:9.016221046447754
epoch27: step3500/4680
step 108000: accuracy:0.11599999666213989, confidence:0.9818810820579529, loss:13.390522003173828
epoch27: step4000/4680
step 121500: accuracy:0.12700000405311584, confidence:0.9377961158752441, loss:10.324103355407715
epoch27: step4500/4680
step 0: accuracy:0.12800000607967377, confidence:0.9220295548439026, loss:10.163866996765137
epoch28: step0/4680
step 14000: accuracy:0.12099999934434891, confidence:0.8983747959136963, loss:8.877456665039062
epoch28: step500/4680
step 28000: accuracy:0.09399999678134918, confidence:0.8049008846282959, loss:7.966524124145508
epoch28: step1000/4680
step 42000: accuracy:0.07199999690055847, confidence:0.8586312532424927, loss:9.593791007995605
epoch28: step1500/4680
step 56000: accuracy:0.14300000667572021, confidence:0.8815050721168518, loss:12.558271408081055
epoch28: step2000/4680
step 70000: accuracy:0.07500000298023224, confidence:0.8174451589584351, loss:7.512988567352295
epoch28: step2500/4680
step 84000: accuracy:0.06700000166893005, confidence:0.8998115658760071, loss:8.73628044128418
epoch28: step3000/4680
step 98000: accuracy:0.1120000034570694, confidence:0.8171676993370056, loss:8.753416061401367
epoch28: step3500/4680
step 112000: accuracy:0.10400000214576721, confidence:0.9806929230690002, loss:13.932568550109863
epoch28: step4000/4680
step 126000: accuracy:0.11900000274181366, confidence:0.9382860660552979, loss:10.521232604980469
epoch28: step4500/4680
step 0: accuracy:0.12099999934434891, confidence:0.9231312870979309, loss:10.38038444519043
epoch29: step0/4680
step 14500: accuracy:0.09000000357627869, confidence:0.9500218629837036, loss:10.756097793579102
epoch29: step500/4680
step 29000: accuracy:0.12399999797344208, confidence:0.8783956170082092, loss:8.538403511047363
epoch29: step1000/4680
step 43500: accuracy:0.09700000286102295, confidence:0.8902565836906433, loss:9.801668167114258
epoch29: step1500/4680
step 58000: accuracy:0.16200000047683716, confidence:0.8996565341949463, loss:11.54570484161377
epoch29: step2000/4680
step 72500: accuracy:0.05000000074505806, confidence:0.8027850389480591, loss:6.923870086669922
epoch29: step2500/4680
step 87000: accuracy:0.07000000029802322, confidence:0.8828973174095154, loss:8.567432403564453
epoch29: step3000/4680
step 101500: accuracy:0.09799999743700027, confidence:0.8133879899978638, loss:8.834623336791992
epoch29: step3500/4680
step 116000: accuracy:0.11400000005960464, confidence:0.9801233410835266, loss:13.68342113494873
epoch29: step4000/4680
step 130500: accuracy:0.0989999994635582, confidence:0.949333131313324, loss:11.134810447692871
epoch29: step4500/4680
2018-06-15 19:49:45.766934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000b:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:49:45.766996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000b:01:00.0, compute capability: 6.0)
2018-06-15 19:49:47.213823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000b:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
CHEKPOINT DIR: uniform
model has been loaded from fashion-mnist_classifier.pkl
---------
Variables: name (type shape) [size]
---------
Variable:0 (float32_ref 5x5x1x32) [800, bytes: 3200]
Variable_1:0 (float32_ref 32) [32, bytes: 128]
Variable_2:0 (float32_ref 5x5x32x64) [51200, bytes: 204800]
Variable_3:0 (float32_ref 64) [64, bytes: 256]
Variable_4:0 (float32_ref 3136x1024) [3211264, bytes: 12845056]
Variable_5:0 (float32_ref 1024) [1024, bytes: 4096]
Variable_6:0 (float32_ref 1024x10) [10240, bytes: 40960]
Variable_7:0 (float32_ref 10) [10, bytes: 40]
discriminator/d_conv1/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]
discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]
discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]
discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]
generator/g_fc1/Matrix:0 (float32_ref 74x1024) [75776, bytes: 303104]
generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]
generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]
generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]
classifier/c_fc1/Matrix:0 (float32_ref 1024x64) [65536, bytes: 262144]
classifier/c_fc1/bias:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/beta:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/gamma:0 (float32_ref 64) [64, bytes: 256]
classifier/c_fc2/Matrix:0 (float32_ref 64x12) [768, bytes: 3072]
classifier/c_fc2/bias:0 (float32_ref 12) [12, bytes: 48]
Total size of variables: 16552792
Total bytes of variables: 66211168
 [*] Reading checkpoints...
 [*] Success to read MultiModalInfoGAN.model-43721
 [*] Load SUCCESS


SAMPLES SIZE=4680,LABELS=299520


SAVED TRAINING SET generated_training_set_fashion-mnist_UniformSample_mu_0_sigma_0.15_czcc_rzcc_czrc_rzrc
 [*] Training finished!
 [*] Testing finished!
2018-06-15 19:50:24.092661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000b:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:50:24.092888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000b:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
model has been loaded from fashion-mnist_classifier.pkl
argmax:[7 7 7 ..., 7 7 7]
step 0: accuracy:0.00279999990016222, confidence:0.9966154098510742, loss:8.930298805236816
Assinging:8
[   0    0    0    0    0    0    0 9972    0   28]
argmax:[3 3 3 ..., 3 3 3]
step 0: accuracy:0.0, confidence:0.99849933385849, loss:12.398773193359375
Assinging:4
[    0     0     0 10000]
argmax:[4 4 4 ..., 4 4 4]
step 0: accuracy:0.0, confidence:0.7333277463912964, loss:12.49646282196045
Assinging:5
[   0    0   69    0 9436    0  495]
argmax:[7 7 7 ..., 7 7 7]
step 0: accuracy:0.0, confidence:0.9991472363471985, loss:12.881529808044434
Assinging:8
[    0     0     0     0     0     0     0 10000]
argmax:[8 8 8 ..., 8 8 8]
step 0: accuracy:0.0, confidence:0.9999984502792358, loss:18.03530502319336
Assinging:9
[    0     0     0     0     0     0     0     0 10000]
argmax:[1 1 1 ..., 1 1 1]
step 0: accuracy:0.0, confidence:0.9999997019767761, loss:22.120338439941406
Assinging:2
[    0 10000]
argmax:[0 0 0 ..., 0 0 0]
step 0: accuracy:0.0, confidence:0.9982784986495972, loss:19.026287078857422
Assinging:1
[9973    0    0    0    0    0   27]
argmax:[4 4 4 ..., 4 4 4]
step 0: accuracy:0.0, confidence:0.9923480749130249, loss:13.898530960083008
Assinging:5
[   0    0   17    8 9946    0   29]
argmax:[2 2 2 ..., 4 4 2]
step 0: accuracy:0.0, confidence:0.7929224967956543, loss:12.912821769714355
Assinging:3
[   0    0 8741    0 1259]
argmax:[9 9 9 ..., 9 9 9]
step 0: accuracy:1.0, confidence:0.9999990463256836, loss:8.733385925552284e-07
Assinging:10
[    0     0     0     0     0     0     0     0     0 10000]
2018-06-15 19:50:39.375419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000b:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:50:39.375610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000b:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
step 0: accuracy:0.09099999815225601, confidence:0.6227267980575562, loss:6.402685642242432
epoch0: step0/4680
step 0: accuracy:0.09200000017881393, confidence:1.0, loss:48.71805953979492
epoch0: step500/4680
step 0: accuracy:0.09399999678134918, confidence:1.0, loss:25.87636375427246
epoch0: step1000/4680
step 0: accuracy:0.09000000357627869, confidence:0.9997005462646484, loss:10.980119705200195
epoch0: step1500/4680
step 0: accuracy:0.0860000029206276, confidence:0.9998469948768616, loss:10.632733345031738
epoch0: step2000/4680
step 0: accuracy:0.10199999809265137, confidence:0.9999837875366211, loss:13.004593849182129
epoch0: step2500/4680
step 0: accuracy:0.11599999666213989, confidence:0.9999974370002747, loss:15.275214195251465
epoch0: step3000/4680
step 0: accuracy:0.0949999988079071, confidence:0.9999891519546509, loss:15.051511764526367
epoch0: step3500/4680
step 0: accuracy:0.09799999743700027, confidence:0.9999892711639404, loss:17.075252532958984
epoch0: step4000/4680
step 0: accuracy:0.09700000286102295, confidence:0.997733473777771, loss:8.086028099060059
epoch0: step4500/4680
step 0: accuracy:0.0860000029206276, confidence:0.9988287091255188, loss:8.757939338684082
epoch1: step0/4680
step 500: accuracy:0.10899999737739563, confidence:0.9999373555183411, loss:11.764619827270508
epoch1: step500/4680
step 1000: accuracy:0.11400000005960464, confidence:0.9994804859161377, loss:9.935799598693848
epoch1: step1000/4680
step 1500: accuracy:0.11400000005960464, confidence:0.9885945916175842, loss:6.352077960968018
epoch1: step1500/4680
step 2000: accuracy:0.11500000208616257, confidence:0.9465203881263733, loss:5.004449844360352
epoch1: step2000/4680
step 2500: accuracy:0.10599999874830246, confidence:0.9434922337532043, loss:5.035912036895752
epoch1: step2500/4680
step 3000: accuracy:0.11100000143051147, confidence:0.9976827502250671, loss:7.996194362640381
epoch1: step3000/4680
step 3500: accuracy:0.07000000029802322, confidence:0.9574341177940369, loss:5.333415508270264
epoch1: step3500/4680
step 4000: accuracy:0.09000000357627869, confidence:0.9590825438499451, loss:5.711873531341553
epoch1: step4000/4680
step 4500: accuracy:0.09200000017881393, confidence:0.9925246834754944, loss:7.033708572387695
epoch1: step4500/4680
step 0: accuracy:0.10899999737739563, confidence:0.9948087334632874, loss:7.2197160720825195
epoch2: step0/4680
step 1000: accuracy:0.0949999988079071, confidence:0.9912376999855042, loss:6.950660705566406
epoch2: step500/4680
step 2000: accuracy:0.10999999940395355, confidence:0.9999001622200012, loss:11.045132637023926
epoch2: step1000/4680
step 3000: accuracy:0.10300000011920929, confidence:0.9960048198699951, loss:7.982293605804443
epoch2: step1500/4680
step 4000: accuracy:0.0989999994635582, confidence:0.8759751915931702, loss:4.562008380889893
epoch2: step2000/4680
step 5000: accuracy:0.11400000005960464, confidence:0.9504014849662781, loss:5.222372531890869
epoch2: step2500/4680
step 6000: accuracy:0.12800000607967377, confidence:0.9904554486274719, loss:6.9250874519348145
epoch2: step3000/4680
step 7000: accuracy:0.0949999988079071, confidence:0.9962314963340759, loss:7.8867902755737305
epoch2: step3500/4680
step 8000: accuracy:0.08699999749660492, confidence:0.9834628701210022, loss:6.721899509429932
epoch2: step4000/4680
step 9000: accuracy:0.0949999988079071, confidence:0.991127610206604, loss:6.811653137207031
epoch2: step4500/4680
step 0: accuracy:0.11599999666213989, confidence:0.9972246885299683, loss:7.667281627655029
epoch3: step0/4680
step 1500: accuracy:0.08299999684095383, confidence:0.9926629066467285, loss:7.667134761810303
epoch3: step500/4680
step 3000: accuracy:0.10899999737739563, confidence:0.9989036917686462, loss:8.920361518859863
epoch3: step1000/4680
step 4500: accuracy:0.10700000077486038, confidence:0.974476158618927, loss:6.701501369476318
epoch3: step1500/4680
step 6000: accuracy:0.10000000149011612, confidence:0.8379963636398315, loss:5.030627727508545
epoch3: step2000/4680
step 7500: accuracy:0.09799999743700027, confidence:0.8851791620254517, loss:4.842259407043457
epoch3: step2500/4680
step 9000: accuracy:0.10300000011920929, confidence:0.9762309193611145, loss:6.250233173370361
epoch3: step3000/4680
step 10500: accuracy:0.09399999678134918, confidence:0.9605625867843628, loss:5.566420555114746
epoch3: step3500/4680
step 12000: accuracy:0.09200000017881393, confidence:0.959004819393158, loss:6.079118728637695
epoch3: step4000/4680
step 13500: accuracy:0.09799999743700027, confidence:0.9883077144622803, loss:6.626218318939209
epoch3: step4500/4680
step 0: accuracy:0.09300000220537186, confidence:0.9948660731315613, loss:7.4059367179870605
epoch4: step0/4680
step 2000: accuracy:0.08299999684095383, confidence:0.9543740153312683, loss:5.983771800994873
epoch4: step500/4680
step 4000: accuracy:0.10000000149011612, confidence:0.9789716005325317, loss:8.570613861083984
epoch4: step1000/4680
step 6000: accuracy:0.10400000214576721, confidence:0.9887291193008423, loss:7.945515155792236
epoch4: step1500/4680
step 8000: accuracy:0.10199999809265137, confidence:0.8849435448646545, loss:5.486999034881592
epoch4: step2000/4680
step 10000: accuracy:0.10700000077486038, confidence:0.7789912223815918, loss:4.265302658081055
epoch4: step2500/4680
step 12000: accuracy:0.11699999868869781, confidence:0.983035147190094, loss:6.689291000366211
epoch4: step3000/4680
step 14000: accuracy:0.10999999940395355, confidence:0.9044859409332275, loss:5.105525970458984
epoch4: step3500/4680
step 16000: accuracy:0.09099999815225601, confidence:0.9386166334152222, loss:5.913577079772949
epoch4: step4000/4680
step 18000: accuracy:0.10700000077486038, confidence:0.9497212171554565, loss:5.714216232299805
epoch4: step4500/4680
step 0: accuracy:0.08399999886751175, confidence:0.9784432649612427, loss:6.688347339630127
epoch5: step0/4680
step 2500: accuracy:0.10199999809265137, confidence:0.9676978588104248, loss:7.226197719573975
epoch5: step500/4680
step 5000: accuracy:0.10700000077486038, confidence:0.9617917537689209, loss:9.44579792022705
epoch5: step1000/4680
step 7500: accuracy:0.09600000083446503, confidence:0.9313294887542725, loss:7.728206157684326
epoch5: step1500/4680
step 10000: accuracy:0.10300000011920929, confidence:0.9706922769546509, loss:7.540521621704102
epoch5: step2000/4680
step 12500: accuracy:0.10499999672174454, confidence:0.721998929977417, loss:4.271115779876709
epoch5: step2500/4680
step 15000: accuracy:0.10199999809265137, confidence:0.980638861656189, loss:7.274747848510742
epoch5: step3000/4680
step 17500: accuracy:0.09700000286102295, confidence:0.7673807144165039, loss:4.431619167327881
epoch5: step3500/4680
step 20000: accuracy:0.09399999678134918, confidence:0.8516703844070435, loss:4.94765567779541
epoch5: step4000/4680
step 22500: accuracy:0.09700000286102295, confidence:0.9590869545936584, loss:6.17966890335083
epoch5: step4500/4680
step 0: accuracy:0.11500000208616257, confidence:0.9807232618331909, loss:6.684061050415039
epoch6: step0/4680
step 3000: accuracy:0.0949999988079071, confidence:0.9655210971832275, loss:8.288761138916016
epoch6: step500/4680
step 6000: accuracy:0.10700000077486038, confidence:0.9838824272155762, loss:10.33836841583252
epoch6: step1000/4680
step 9000: accuracy:0.12200000137090683, confidence:0.7232824563980103, loss:7.933213710784912
epoch6: step1500/4680
step 12000: accuracy:0.11500000208616257, confidence:0.9917113184928894, loss:9.5728120803833
epoch6: step2000/4680
step 15000: accuracy:0.11500000208616257, confidence:0.7351198792457581, loss:4.4295973777771
epoch6: step2500/4680
step 18000: accuracy:0.125, confidence:0.9898440837860107, loss:7.986383438110352
epoch6: step3000/4680
step 21000: accuracy:0.1509999930858612, confidence:0.7354477643966675, loss:4.925315856933594
epoch6: step3500/4680
step 24000: accuracy:0.11800000071525574, confidence:0.8199644684791565, loss:5.181220531463623
epoch6: step4000/4680
step 27000: accuracy:0.0820000022649765, confidence:0.9580028057098389, loss:6.282978534698486
epoch6: step4500/4680
step 0: accuracy:0.11900000274181366, confidence:0.9810411930084229, loss:6.84078311920166
epoch7: step0/4680
step 3500: accuracy:0.09799999743700027, confidence:0.8781823515892029, loss:7.569145202636719
epoch7: step500/4680
step 7000: accuracy:0.08699999749660492, confidence:0.9784453511238098, loss:9.337661743164062
epoch7: step1000/4680
step 10500: accuracy:0.125, confidence:0.8233885169029236, loss:8.585609436035156
epoch7: step1500/4680
step 14000: accuracy:0.11100000143051147, confidence:0.9161538481712341, loss:9.636159896850586
epoch7: step2000/4680
step 17500: accuracy:0.09099999815225601, confidence:0.7592629790306091, loss:5.100042343139648
epoch7: step2500/4680
step 21000: accuracy:0.09399999678134918, confidence:0.9956346154212952, loss:9.492805480957031
epoch7: step3000/4680
step 24500: accuracy:0.17000000178813934, confidence:0.7060818076133728, loss:4.9123759269714355
epoch7: step3500/4680
step 28000: accuracy:0.17299999296665192, confidence:0.8050283193588257, loss:5.18172550201416
epoch7: step4000/4680
step 31500: accuracy:0.0820000022649765, confidence:0.8804724216461182, loss:5.428894519805908
epoch7: step4500/4680
step 0: accuracy:0.09300000220537186, confidence:0.936975359916687, loss:6.038032054901123
epoch8: step0/4680
step 4000: accuracy:0.07800000160932541, confidence:0.8173521757125854, loss:6.501039505004883
epoch8: step500/4680
step 8000: accuracy:0.08299999684095383, confidence:0.9459759593009949, loss:16.11304473876953
epoch8: step1000/4680
step 12000: accuracy:0.10100000351667404, confidence:0.7982752919197083, loss:6.1546311378479
epoch8: step1500/4680
step 16000: accuracy:0.09200000017881393, confidence:0.9974588751792908, loss:13.017088890075684
epoch8: step2000/4680
step 20000: accuracy:0.11400000005960464, confidence:0.9893901348114014, loss:8.502561569213867
epoch8: step2500/4680
step 24000: accuracy:0.1080000028014183, confidence:0.9958633184432983, loss:9.304234504699707
epoch8: step3000/4680
step 28000: accuracy:0.1509999930858612, confidence:0.7848976850509644, loss:6.402312755584717
epoch8: step3500/4680
step 32000: accuracy:0.17599999904632568, confidence:0.78339684009552, loss:5.408618927001953
epoch8: step4000/4680
step 36000: accuracy:0.09300000220537186, confidence:0.8975774645805359, loss:6.133377552032471
epoch8: step4500/4680
step 0: accuracy:0.10199999809265137, confidence:0.9304848313331604, loss:6.363952159881592
epoch9: step0/4680
step 4500: accuracy:0.057999998331069946, confidence:0.8202561140060425, loss:6.8239970207214355
epoch9: step500/4680
step 9000: accuracy:0.10400000214576721, confidence:0.9635352492332458, loss:11.287165641784668
epoch9: step1000/4680
step 13500: accuracy:0.11900000274181366, confidence:0.925380289554596, loss:8.556975364685059
epoch9: step1500/4680
step 18000: accuracy:0.06800000369548798, confidence:0.8166053295135498, loss:7.8842902183532715
epoch9: step2000/4680
step 22500: accuracy:0.10899999737739563, confidence:0.9961867928504944, loss:10.456149101257324
epoch9: step2500/4680
step 27000: accuracy:0.09799999743700027, confidence:0.9982348680496216, loss:10.25418472290039
epoch9: step3000/4680
step 31500: accuracy:0.1770000010728836, confidence:0.7270230650901794, loss:5.318073749542236
epoch9: step3500/4680
step 36000: accuracy:0.18000000715255737, confidence:0.7744277119636536, loss:5.228891372680664
epoch9: step4000/4680
step 40500: accuracy:0.10700000077486038, confidence:0.8784724473953247, loss:5.852113723754883
epoch9: step4500/4680
step 0: accuracy:0.10300000011920929, confidence:0.9232051968574524, loss:6.367752552032471
epoch10: step0/4680
step 5000: accuracy:0.09099999815225601, confidence:0.8044593334197998, loss:6.766927719116211
epoch10: step500/4680
step 10000: accuracy:0.10400000214576721, confidence:0.8778097629547119, loss:15.028759956359863
epoch10: step1000/4680
step 15000: accuracy:0.0949999988079071, confidence:0.75978684425354, loss:5.98914098739624
epoch10: step1500/4680
step 20000: accuracy:0.10499999672174454, confidence:0.9743530750274658, loss:10.534246444702148
epoch10: step2000/4680
step 25000: accuracy:0.09799999743700027, confidence:0.9350714087486267, loss:7.56210994720459
epoch10: step2500/4680
step 30000: accuracy:0.1120000034570694, confidence:0.9881603717803955, loss:9.0860013961792
epoch10: step3000/4680
step 35000: accuracy:0.15299999713897705, confidence:0.7402128577232361, loss:5.492823123931885
epoch10: step3500/4680
step 40000: accuracy:0.20900000631809235, confidence:0.7828283309936523, loss:5.410141944885254
epoch10: step4000/4680
step 45000: accuracy:0.10199999809265137, confidence:0.8586077690124512, loss:5.423872947692871
epoch10: step4500/4680
step 0: accuracy:0.09300000220537186, confidence:0.9129955768585205, loss:6.283977031707764
epoch11: step0/4680
step 5500: accuracy:0.03999999910593033, confidence:0.8592414259910583, loss:6.985319137573242
epoch11: step500/4680
step 11000: accuracy:0.11599999666213989, confidence:0.9311859011650085, loss:9.771422386169434
epoch11: step1000/4680
step 16500: accuracy:0.1080000028014183, confidence:0.8872213959693909, loss:9.505931854248047
epoch11: step1500/4680
step 22000: accuracy:0.0430000014603138, confidence:0.817854106426239, loss:8.575803756713867
epoch11: step2000/4680
step 27500: accuracy:0.11500000208616257, confidence:0.923354983329773, loss:7.26549768447876
epoch11: step2500/4680
step 33000: accuracy:0.12300000339746475, confidence:0.9989625215530396, loss:10.685636520385742
epoch11: step3000/4680
step 38500: accuracy:0.13899999856948853, confidence:0.7454583644866943, loss:5.509374141693115
epoch11: step3500/4680
step 44000: accuracy:0.19300000369548798, confidence:0.7134268283843994, loss:4.674421787261963
epoch11: step4000/4680
step 49500: accuracy:0.125, confidence:0.8009457588195801, loss:5.6441144943237305
epoch11: step4500/4680
step 0: accuracy:0.10000000149011612, confidence:0.8588742613792419, loss:6.278303623199463
epoch12: step0/4680
step 6000: accuracy:0.050999999046325684, confidence:0.8078369498252869, loss:7.057620525360107
epoch12: step500/4680
step 12000: accuracy:0.07199999690055847, confidence:0.8785700798034668, loss:17.32210350036621
epoch12: step1000/4680
step 18000: accuracy:0.052000001072883606, confidence:0.7547997236251831, loss:6.694246292114258
epoch12: step1500/4680
step 24000: accuracy:0.08299999684095383, confidence:0.9826430678367615, loss:11.576309204101562
epoch12: step2000/4680
step 30000: accuracy:0.08799999952316284, confidence:0.9581423997879028, loss:8.922179222106934
epoch12: step2500/4680
step 36000: accuracy:0.11100000143051147, confidence:0.9993345737457275, loss:12.006789207458496
epoch12: step3000/4680
step 42000: accuracy:0.15299999713897705, confidence:0.7771252393722534, loss:5.919987678527832
epoch12: step3500/4680
step 48000: accuracy:0.18799999356269836, confidence:0.7617219686508179, loss:5.078672409057617
epoch12: step4000/4680
step 54000: accuracy:0.1080000028014183, confidence:0.7521898746490479, loss:5.164000034332275
epoch12: step4500/4680
step 0: accuracy:0.1080000028014183, confidence:0.796461820602417, loss:5.33208703994751
epoch13: step0/4680
step 6500: accuracy:0.04399999976158142, confidence:0.8251872658729553, loss:7.499495506286621
epoch13: step500/4680
step 13000: accuracy:0.10400000214576721, confidence:0.9084326028823853, loss:10.676997184753418
epoch13: step1000/4680
step 19500: accuracy:0.10199999809265137, confidence:0.8873507976531982, loss:10.18016242980957
epoch13: step1500/4680
step 26000: accuracy:0.05999999865889549, confidence:0.9264042377471924, loss:13.0774507522583
epoch13: step2000/4680
step 32500: accuracy:0.10100000351667404, confidence:0.9365330934524536, loss:8.07736873626709
epoch13: step2500/4680
step 39000: accuracy:0.1120000034570694, confidence:0.9994208216667175, loss:12.027204513549805
epoch13: step3000/4680
step 45500: accuracy:0.14499999582767487, confidence:0.7714108824729919, loss:5.797970771789551
epoch13: step3500/4680
step 52000: accuracy:0.15600000321865082, confidence:0.7167990207672119, loss:4.970564365386963
epoch13: step4000/4680
step 58500: accuracy:0.11400000005960464, confidence:0.8285060524940491, loss:6.058139324188232
epoch13: step4500/4680
step 0: accuracy:0.09700000286102295, confidence:0.8671062588691711, loss:6.761392593383789
epoch14: step0/4680
step 7000: accuracy:0.0430000014603138, confidence:0.8339117765426636, loss:7.071696758270264
epoch14: step500/4680
step 14000: accuracy:0.08299999684095383, confidence:0.8868764638900757, loss:18.33725357055664
epoch14: step1000/4680
step 21000: accuracy:0.04500000178813934, confidence:0.8239354491233826, loss:8.892561912536621
epoch14: step1500/4680
step 28000: accuracy:0.08100000023841858, confidence:0.9297309517860413, loss:11.036757469177246
epoch14: step2000/4680
step 35000: accuracy:0.08900000154972076, confidence:0.9852551221847534, loss:12.169626235961914
epoch14: step2500/4680
step 42000: accuracy:0.11500000208616257, confidence:0.9998900294303894, loss:13.077027320861816
epoch14: step3000/4680
step 49000: accuracy:0.12700000405311584, confidence:0.8565767407417297, loss:7.74819278717041
epoch14: step3500/4680
step 56000: accuracy:0.19099999964237213, confidence:0.82032310962677, loss:5.714178085327148
epoch14: step4000/4680
step 63000: accuracy:0.13099999725818634, confidence:0.8199673295021057, loss:6.540267467498779
epoch14: step4500/4680
step 0: accuracy:0.1289999932050705, confidence:0.8509044647216797, loss:6.601078033447266
epoch15: step0/4680
step 7500: accuracy:0.10700000077486038, confidence:0.8748658299446106, loss:9.486242294311523
epoch15: step500/4680
step 15000: accuracy:0.10599999874830246, confidence:0.9509576559066772, loss:15.314292907714844
epoch15: step1000/4680
step 22500: accuracy:0.04800000041723251, confidence:0.7529826760292053, loss:6.459516525268555
epoch15: step1500/4680
step 30000: accuracy:0.04100000113248825, confidence:0.8478795289993286, loss:7.855330467224121
epoch15: step2000/4680
step 37500: accuracy:0.10499999672174454, confidence:0.9927123188972473, loss:12.349536895751953
epoch15: step2500/4680
step 45000: accuracy:0.10999999940395355, confidence:0.9999827742576599, loss:15.14686107635498
epoch15: step3000/4680
step 52500: accuracy:0.17599999904632568, confidence:0.7944499254226685, loss:6.633264541625977
epoch15: step3500/4680
step 60000: accuracy:0.18400000035762787, confidence:0.7402119636535645, loss:4.862197399139404
epoch15: step4000/4680
step 67500: accuracy:0.11599999666213989, confidence:0.7980127930641174, loss:6.033906936645508
epoch15: step4500/4680
step 0: accuracy:0.10300000011920929, confidence:0.840407133102417, loss:6.332528114318848
epoch16: step0/4680
step 8000: accuracy:0.0729999989271164, confidence:0.8108568787574768, loss:8.079001426696777
epoch16: step500/4680
step 16000: accuracy:0.09700000286102295, confidence:0.9275469183921814, loss:16.842592239379883
epoch16: step1000/4680
step 24000: accuracy:0.05400000140070915, confidence:0.7783780694007874, loss:7.299861431121826
epoch16: step1500/4680
step 32000: accuracy:0.05900000035762787, confidence:0.8971688151359558, loss:9.52946662902832
epoch16: step2000/4680
step 40000: accuracy:0.10400000214576721, confidence:0.8571476936340332, loss:7.707221508026123
epoch16: step2500/4680
step 48000: accuracy:0.12399999797344208, confidence:0.9989549517631531, loss:13.039843559265137
epoch16: step3000/4680
step 56000: accuracy:0.09799999743700027, confidence:0.8572207689285278, loss:7.788792610168457
epoch16: step3500/4680
step 64000: accuracy:0.19499999284744263, confidence:0.8001540899276733, loss:5.110864639282227
epoch16: step4000/4680
step 72000: accuracy:0.0989999994635582, confidence:0.8462566137313843, loss:6.3914794921875
epoch16: step4500/4680
step 0: accuracy:0.10999999940395355, confidence:0.8863282799720764, loss:6.691289901733398
epoch17: step0/4680
step 8500: accuracy:0.07699999958276749, confidence:0.8338949680328369, loss:7.877191543579102
epoch17: step500/4680
step 17000: accuracy:0.09799999743700027, confidence:0.9567394852638245, loss:16.602540969848633
epoch17: step1000/4680
step 25500: accuracy:0.041999999433755875, confidence:0.7584884166717529, loss:6.403636455535889
epoch17: step1500/4680
step 34000: accuracy:0.09300000220537186, confidence:0.9876512885093689, loss:12.254439353942871
epoch17: step2000/4680
step 42500: accuracy:0.08699999749660492, confidence:0.9788575172424316, loss:9.64200496673584
epoch17: step2500/4680
step 51000: accuracy:0.10700000077486038, confidence:0.999677300453186, loss:13.362778663635254
epoch17: step3000/4680
step 59500: accuracy:0.1720000058412552, confidence:0.7558882832527161, loss:6.063329696655273
epoch17: step3500/4680
step 68000: accuracy:0.17100000381469727, confidence:0.6727983951568604, loss:4.530110836029053
epoch17: step4000/4680
step 76500: accuracy:0.10999999940395355, confidence:0.8060100078582764, loss:5.8856072425842285
epoch17: step4500/4680
step 0: accuracy:0.09200000017881393, confidence:0.8573400378227234, loss:6.649753570556641
epoch18: step0/4680
step 9000: accuracy:0.05400000140070915, confidence:0.8287122249603271, loss:8.39613151550293
epoch18: step500/4680
step 18000: accuracy:0.10300000011920929, confidence:0.9297177195549011, loss:15.164957046508789
epoch18: step1000/4680
step 27000: accuracy:0.07900000363588333, confidence:0.8700538873672485, loss:13.134918212890625
epoch18: step1500/4680
step 36000: accuracy:0.11100000143051147, confidence:0.9872801899909973, loss:16.573156356811523
epoch18: step2000/4680
step 45000: accuracy:0.0949999988079071, confidence:0.8248598575592041, loss:6.805886268615723
epoch18: step2500/4680
step 54000: accuracy:0.11299999803304672, confidence:0.9875553250312805, loss:12.773672103881836
epoch18: step3000/4680
step 63000: accuracy:0.1459999978542328, confidence:0.8191187977790833, loss:6.976193428039551
epoch18: step3500/4680
step 72000: accuracy:0.19699999690055847, confidence:0.8043838143348694, loss:5.3140740394592285
epoch18: step4000/4680
step 81000: accuracy:0.10000000149011612, confidence:0.8365907669067383, loss:6.596377849578857
epoch18: step4500/4680
step 0: accuracy:0.08399999886751175, confidence:0.8560278415679932, loss:6.928011894226074
epoch19: step0/4680
step 9500: accuracy:0.04500000178813934, confidence:0.8306889533996582, loss:7.483008861541748
epoch19: step500/4680
step 19000: accuracy:0.08500000089406967, confidence:0.9278815388679504, loss:17.47232437133789
epoch19: step1000/4680
step 28500: accuracy:0.03999999910593033, confidence:0.8302791118621826, loss:8.744211196899414
epoch19: step1500/4680
step 38000: accuracy:0.061000000685453415, confidence:0.8858253359794617, loss:9.810193061828613
epoch19: step2000/4680
step 47500: accuracy:0.09000000357627869, confidence:0.9932248592376709, loss:12.919441223144531
epoch19: step2500/4680
step 57000: accuracy:0.11500000208616257, confidence:0.9997286796569824, loss:12.030292510986328
epoch19: step3000/4680
step 66500: accuracy:0.14499999582767487, confidence:0.8362945318222046, loss:7.525967121124268
epoch19: step3500/4680
step 76000: accuracy:0.17000000178813934, confidence:0.7969804406166077, loss:5.17238712310791
epoch19: step4000/4680
step 85500: accuracy:0.16599999368190765, confidence:0.776270866394043, loss:5.981301307678223
epoch19: step4500/4680
step 0: accuracy:0.1469999998807907, confidence:0.8138134479522705, loss:6.554947853088379
epoch20: step0/4680
step 10000: accuracy:0.11800000071525574, confidence:0.9165147542953491, loss:10.513638496398926
epoch20: step500/4680
step 20000: accuracy:0.09600000083446503, confidence:0.9760242700576782, loss:17.148740768432617
epoch20: step1000/4680
step 30000: accuracy:0.07199999690055847, confidence:0.7797767519950867, loss:7.858589172363281
epoch20: step1500/4680
step 40000: accuracy:0.03500000014901161, confidence:0.8423239588737488, loss:8.892019271850586
epoch20: step2000/4680
step 50000: accuracy:0.10700000077486038, confidence:0.9655053019523621, loss:10.755370140075684
epoch20: step2500/4680
step 60000: accuracy:0.09399999678134918, confidence:0.9999786615371704, loss:15.65676212310791
epoch20: step3000/4680
step 70000: accuracy:0.12399999797344208, confidence:0.8168445825576782, loss:7.75160551071167
epoch20: step3500/4680
step 80000: accuracy:0.14499999582767487, confidence:0.8010668754577637, loss:5.395817756652832
epoch20: step4000/4680
step 90000: accuracy:0.16200000047683716, confidence:0.7887731790542603, loss:6.28761625289917
epoch20: step4500/4680
step 0: accuracy:0.12300000339746475, confidence:0.8102089762687683, loss:6.66208028793335
epoch21: step0/4680
step 10500: accuracy:0.06199999898672104, confidence:0.8341068625450134, loss:8.115665435791016
epoch21: step500/4680
step 21000: accuracy:0.11699999868869781, confidence:0.945348858833313, loss:16.2730770111084
epoch21: step1000/4680
step 31500: accuracy:0.05400000140070915, confidence:0.78200763463974, loss:8.948089599609375
epoch21: step1500/4680
step 42000: accuracy:0.10899999737739563, confidence:0.9993614554405212, loss:16.75773048400879
epoch21: step2000/4680
step 52500: accuracy:0.10100000351667404, confidence:0.8616474866867065, loss:7.987420558929443
epoch21: step2500/4680
step 63000: accuracy:0.12600000202655792, confidence:0.9487977623939514, loss:13.44294261932373
epoch21: step3000/4680
step 73500: accuracy:0.15000000596046448, confidence:0.7511991858482361, loss:7.209261417388916
epoch21: step3500/4680
step 84000: accuracy:0.13500000536441803, confidence:0.6847501397132874, loss:5.0249223709106445
epoch21: step4000/4680
step 94500: accuracy:0.12099999934434891, confidence:0.7875070571899414, loss:6.055334091186523
epoch21: step4500/4680
step 0: accuracy:0.10199999809265137, confidence:0.8187881708145142, loss:6.434386253356934
epoch22: step0/4680
step 11000: accuracy:0.04500000178813934, confidence:0.856421709060669, loss:8.020515441894531
epoch22: step500/4680
step 22000: accuracy:0.10199999809265137, confidence:0.9317175149917603, loss:13.719801902770996
epoch22: step1000/4680
step 33000: accuracy:0.10100000351667404, confidence:0.8758650422096252, loss:10.59782600402832
epoch22: step1500/4680
step 44000: accuracy:0.032999999821186066, confidence:0.860578715801239, loss:10.329598426818848
epoch22: step2000/4680
step 55000: accuracy:0.10100000351667404, confidence:0.9627957344055176, loss:10.356583595275879
epoch22: step2500/4680
step 66000: accuracy:0.11299999803304672, confidence:0.9997976422309875, loss:13.284137725830078
epoch22: step3000/4680
step 77000: accuracy:0.09300000220537186, confidence:0.8572837114334106, loss:7.6361083984375
epoch22: step3500/4680
step 88000: accuracy:0.14499999582767487, confidence:0.801311194896698, loss:5.627267360687256
epoch22: step4000/4680
step 99000: accuracy:0.1289999932050705, confidence:0.8022056221961975, loss:6.463000297546387
epoch22: step4500/4680
step 0: accuracy:0.11699999868869781, confidence:0.8379345536231995, loss:7.018868923187256
epoch23: step0/4680
step 11500: accuracy:0.10400000214576721, confidence:0.8449077606201172, loss:8.877751350402832
epoch23: step500/4680
step 23000: accuracy:0.10300000011920929, confidence:0.9011632204055786, loss:15.695191383361816
epoch23: step1000/4680
step 34500: accuracy:0.057999998331069946, confidence:0.7922004461288452, loss:8.426945686340332
epoch23: step1500/4680
step 46000: accuracy:0.10499999672174454, confidence:0.9800847768783569, loss:12.115392684936523
epoch23: step2000/4680
step 57500: accuracy:0.09099999815225601, confidence:0.8796320557594299, loss:8.959975242614746
epoch23: step2500/4680
step 69000: accuracy:0.10999999940395355, confidence:0.9755781888961792, loss:16.752023696899414
epoch23: step3000/4680
step 80500: accuracy:0.1289999932050705, confidence:0.8023051619529724, loss:8.334890365600586
epoch23: step3500/4680
step 92000: accuracy:0.15199999511241913, confidence:0.7591021656990051, loss:5.317526817321777
epoch23: step4000/4680
step 103500: accuracy:0.11299999803304672, confidence:0.7969931364059448, loss:6.092243671417236
epoch23: step4500/4680
step 0: accuracy:0.1080000028014183, confidence:0.828332781791687, loss:6.40563440322876
epoch24: step0/4680
step 12000: accuracy:0.07400000095367432, confidence:0.8527236580848694, loss:8.245741844177246
epoch24: step500/4680
step 24000: accuracy:0.11999999731779099, confidence:0.9169206619262695, loss:13.560994148254395
epoch24: step1000/4680
step 36000: accuracy:0.0560000017285347, confidence:0.8300978541374207, loss:9.036104202270508
epoch24: step1500/4680
step 48000: accuracy:0.10400000214576721, confidence:0.9939505457878113, loss:13.83426570892334
epoch24: step2000/4680
step 60000: accuracy:0.09700000286102295, confidence:0.8930664658546448, loss:8.326339721679688
epoch24: step2500/4680
step 72000: accuracy:0.0989999994635582, confidence:0.9984419345855713, loss:14.404058456420898
epoch24: step3000/4680
step 84000: accuracy:0.13600000739097595, confidence:0.8652681708335876, loss:8.193741798400879
epoch24: step3500/4680
step 96000: accuracy:0.15000000596046448, confidence:0.8385019898414612, loss:5.520115852355957
epoch24: step4000/4680
step 108000: accuracy:0.1979999989271164, confidence:0.7573248744010925, loss:5.774711608886719
epoch24: step4500/4680
step 0: accuracy:0.16599999368190765, confidence:0.7837851047515869, loss:6.325686454772949
epoch25: step0/4680
step 12500: accuracy:0.07599999755620956, confidence:0.8486379384994507, loss:8.169608116149902
epoch25: step500/4680
step 25000: accuracy:0.1120000034570694, confidence:0.9575353860855103, loss:14.263263702392578
epoch25: step1000/4680
step 37500: accuracy:0.11900000274181366, confidence:0.8559349775314331, loss:10.965576171875
epoch25: step1500/4680
step 50000: accuracy:0.050999999046325684, confidence:0.8708323240280151, loss:10.847002029418945
epoch25: step2000/4680
step 62500: accuracy:0.09000000357627869, confidence:0.9474708437919617, loss:9.995701789855957
epoch25: step2500/4680
step 75000: accuracy:0.11999999731779099, confidence:0.9995517134666443, loss:13.581480026245117
epoch25: step3000/4680
step 87500: accuracy:0.164000004529953, confidence:0.7951546311378479, loss:6.971573352813721
epoch25: step3500/4680
step 100000: accuracy:0.1340000033378601, confidence:0.7479566335678101, loss:4.893701076507568
epoch25: step4000/4680
step 112500: accuracy:0.11599999666213989, confidence:0.8126950860023499, loss:6.633885860443115
epoch25: step4500/4680
step 0: accuracy:0.09600000083446503, confidence:0.8511220216751099, loss:7.223571300506592
epoch26: step0/4680
step 13000: accuracy:0.10100000351667404, confidence:0.9058693647384644, loss:8.638343811035156
epoch26: step500/4680
step 26000: accuracy:0.08299999684095383, confidence:0.8614248037338257, loss:13.062651634216309
epoch26: step1000/4680
step 39000: accuracy:0.06300000101327896, confidence:0.850612223148346, loss:9.379751205444336
epoch26: step1500/4680
step 52000: accuracy:0.10300000011920929, confidence:0.9905703067779541, loss:12.97478199005127
epoch26: step2000/4680
step 65000: accuracy:0.10899999737739563, confidence:0.8162460923194885, loss:9.202032089233398
epoch26: step2500/4680
step 78000: accuracy:0.10499999672174454, confidence:0.9954277276992798, loss:15.960800170898438
epoch26: step3000/4680
step 91000: accuracy:0.16899999976158142, confidence:0.7877717018127441, loss:7.524756908416748
epoch26: step3500/4680
step 104000: accuracy:0.1589999943971634, confidence:0.767390787601471, loss:5.129858493804932
epoch26: step4000/4680
step 117000: accuracy:0.17800000309944153, confidence:0.7162246704101562, loss:5.34678840637207
epoch26: step4500/4680
step 0: accuracy:0.16599999368190765, confidence:0.7629402875900269, loss:5.772761821746826
epoch27: step0/4680
step 13500: accuracy:0.07599999755620956, confidence:0.8530203700065613, loss:8.32812786102295
epoch27: step500/4680
step 27000: accuracy:0.11299999803304672, confidence:0.9728153347969055, loss:12.211668014526367
epoch27: step1000/4680
step 40500: accuracy:0.1340000033378601, confidence:0.9038121700286865, loss:9.152828216552734
epoch27: step1500/4680
step 54000: accuracy:0.061000000685453415, confidence:0.8541038036346436, loss:9.040752410888672
epoch27: step2000/4680
step 67500: accuracy:0.0820000022649765, confidence:0.9639995098114014, loss:11.560247421264648
epoch27: step2500/4680
step 81000: accuracy:0.1080000028014183, confidence:0.9948427081108093, loss:14.46607780456543
epoch27: step3000/4680
step 94500: accuracy:0.11599999666213989, confidence:0.8300658464431763, loss:8.334113121032715
epoch27: step3500/4680
step 108000: accuracy:0.1289999932050705, confidence:0.8113358616828918, loss:5.431006908416748
epoch27: step4000/4680
step 121500: accuracy:0.19099999964237213, confidence:0.7743892669677734, loss:6.12819242477417
epoch27: step4500/4680
step 0: accuracy:0.14900000393390656, confidence:0.7886313796043396, loss:6.643394947052002
epoch28: step0/4680
step 14000: accuracy:0.11500000208616257, confidence:0.887499988079071, loss:8.369431495666504
epoch28: step500/4680
step 28000: accuracy:0.09099999815225601, confidence:0.8439627289772034, loss:13.895764350891113
epoch28: step1000/4680
step 42000: accuracy:0.08399999886751175, confidence:0.8420805931091309, loss:9.235445022583008
epoch28: step1500/4680
step 56000: accuracy:0.0860000029206276, confidence:0.9926836490631104, loss:14.194421768188477
epoch28: step2000/4680
step 70000: accuracy:0.10000000149011612, confidence:0.7939630746841431, loss:9.479141235351562
epoch28: step2500/4680
step 84000: accuracy:0.12099999934434891, confidence:0.9942878484725952, loss:14.035506248474121
epoch28: step3000/4680
step 98000: accuracy:0.16200000047683716, confidence:0.7650647163391113, loss:7.353813648223877
epoch28: step3500/4680
step 112000: accuracy:0.125, confidence:0.7020037174224854, loss:4.69381046295166
epoch28: step4000/4680
step 126000: accuracy:0.1720000058412552, confidence:0.7285458445549011, loss:5.5213704109191895
epoch28: step4500/4680
step 0: accuracy:0.14900000393390656, confidence:0.7472333908081055, loss:5.899880886077881
epoch29: step0/4680
step 14500: accuracy:0.0820000022649765, confidence:0.8529463410377502, loss:8.384479522705078
epoch29: step500/4680
step 29000: accuracy:0.10400000214576721, confidence:0.9720063209533691, loss:13.25644302368164
epoch29: step1000/4680
step 43500: accuracy:0.125, confidence:0.8990680575370789, loss:9.695877075195312
epoch29: step1500/4680
step 58000: accuracy:0.054999999701976776, confidence:0.8644775152206421, loss:10.171117782592773
epoch29: step2000/4680
step 72500: accuracy:0.08699999749660492, confidence:0.943893551826477, loss:10.906637191772461
epoch29: step2500/4680
step 87000: accuracy:0.11699999868869781, confidence:0.9938617944717407, loss:15.735048294067383
epoch29: step3000/4680
step 101500: accuracy:0.17000000178813934, confidence:0.7979846596717834, loss:7.680602550506592
epoch29: step3500/4680
step 116000: accuracy:0.1550000011920929, confidence:0.77363121509552, loss:5.112217426300049
epoch29: step4000/4680
step 130500: accuracy:0.10499999672174454, confidence:0.8165993094444275, loss:6.562203884124756
epoch29: step4500/4680
2018-06-15 19:59:07.797158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000b:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:59:07.797370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000b:01:00.0, compute capability: 6.0)
2018-06-15 19:59:09.257316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000b:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
CHEKPOINT DIR: uniform
model has been loaded from fashion-mnist_classifier.pkl
---------
Variables: name (type shape) [size]
---------
Variable:0 (float32_ref 5x5x1x32) [800, bytes: 3200]
Variable_1:0 (float32_ref 32) [32, bytes: 128]
Variable_2:0 (float32_ref 5x5x32x64) [51200, bytes: 204800]
Variable_3:0 (float32_ref 64) [64, bytes: 256]
Variable_4:0 (float32_ref 3136x1024) [3211264, bytes: 12845056]
Variable_5:0 (float32_ref 1024) [1024, bytes: 4096]
Variable_6:0 (float32_ref 1024x10) [10240, bytes: 40960]
Variable_7:0 (float32_ref 10) [10, bytes: 40]
discriminator/d_conv1/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]
discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]
discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]
discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]
generator/g_fc1/Matrix:0 (float32_ref 74x1024) [75776, bytes: 303104]
generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]
generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]
generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]
classifier/c_fc1/Matrix:0 (float32_ref 1024x64) [65536, bytes: 262144]
classifier/c_fc1/bias:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/beta:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/gamma:0 (float32_ref 64) [64, bytes: 256]
classifier/c_fc2/Matrix:0 (float32_ref 64x12) [768, bytes: 3072]
classifier/c_fc2/bias:0 (float32_ref 12) [12, bytes: 48]
Total size of variables: 16552792
Total bytes of variables: 66211168
 [*] Reading checkpoints...
 [*] Success to read MultiModalInfoGAN.model-43721
 [*] Load SUCCESS


SAMPLES SIZE=4680,LABELS=299520


SAVED TRAINING SET generated_training_set_fashion-mnist_UniformSample_mu_0_sigma_0.15_rzcc_rzrc_czcc_czrc
 [*] Training finished!
 [*] Testing finished!
2018-06-15 19:59:51.471812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000b:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:59:51.472007: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000b:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
model has been loaded from fashion-mnist_classifier.pkl
argmax:[0 0 0 ..., 0 0 0]
step 0: accuracy:0.0, confidence:0.9807767868041992, loss:15.714167594909668
Assinging:1
[9619    0    0    0    0    0  381]
argmax:[1 1 1 ..., 1 1 1]
step 0: accuracy:0.0, confidence:0.998201847076416, loss:16.853164672851562
Assinging:2
[   0 9991    0    8    0    0    1]
argmax:[3 3 3 ..., 6 8 0]
step 0: accuracy:0.0, confidence:0.8672425746917725, loss:8.725852012634277
Assinging:4
[ 392    0   54 8895    4    0  569    0   86]
argmax:[7 7 7 ..., 9 9 9]
step 0: accuracy:0.08990000188350677, confidence:0.9162143468856812, loss:2.97359561920166
Assinging:8
[   0    0    0    0    0   10    0 9091    0  899]
argmax:[6 6 4 ..., 2 2 2]
step 0: accuracy:0.0, confidence:0.8948755860328674, loss:14.643941879272461
Assinging:7
[   0    0 1652    0 3319    0 5029]
argmax:[7 7 7 ..., 5 5 5]
step 0: accuracy:0.0, confidence:0.9215529561042786, loss:8.067928314208984
Assinging:8
[   0    0    0    0    0 1512    0 8488]
argmax:[4 2 4 ..., 4 4 2]
step 0: accuracy:0.0, confidence:0.874945878982544, loss:9.981771469116211
Assinging:5
[   3    0 2279  955 6745    0   18]
argmax:[9 9 9 ..., 9 5 5]
step 0: accuracy:0.9043999910354614, confidence:0.9978193640708923, loss:1.4107974767684937
Assinging:10
[   0    0    0    0    0  956    0    0    0 9044]
argmax:[4 4 4 ..., 2 2 2]
step 0: accuracy:0.0, confidence:0.8876534104347229, loss:12.586216926574707
Assinging:3
[   0    0 4933    0 4848    0  219]
argmax:[8 8 8 ..., 8 8 8]
step 0: accuracy:0.0, confidence:0.9977681636810303, loss:14.745987892150879
Assinging:9
[   0    0    3    0    0    0    0    0 9997]
2018-06-15 20:00:07.825106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000b:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 20:00:07.825278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000b:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
step 0: accuracy:0.05700000002980232, confidence:0.6853466629981995, loss:7.084020614624023
epoch0: step0/4680
step 0: accuracy:0.10199999809265137, confidence:1.0, loss:28.568632125854492
epoch0: step500/4680
step 0: accuracy:0.08799999952316284, confidence:1.0, loss:36.22395706176758
epoch0: step1000/4680
step 0: accuracy:0.09799999743700027, confidence:0.9999068975448608, loss:11.055994033813477
epoch0: step1500/4680
step 0: accuracy:0.0860000029206276, confidence:0.9888112545013428, loss:6.714567184448242
epoch0: step2000/4680
step 0: accuracy:0.10700000077486038, confidence:0.998726487159729, loss:9.266956329345703
epoch0: step2500/4680
step 0: accuracy:0.10199999809265137, confidence:0.9887847304344177, loss:7.202200889587402
epoch0: step3000/4680
step 0: accuracy:0.10300000011920929, confidence:0.9923102855682373, loss:7.492825984954834
epoch0: step3500/4680
step 0: accuracy:0.10199999809265137, confidence:0.9994317293167114, loss:9.658479690551758
epoch0: step4000/4680
step 0: accuracy:0.09700000286102295, confidence:0.9913045763969421, loss:6.789392471313477
epoch0: step4500/4680
step 0: accuracy:0.0860000029206276, confidence:0.9949771165847778, loss:7.380995750427246
epoch1: step0/4680
step 500: accuracy:0.09000000357627869, confidence:0.9998459815979004, loss:11.22223949432373
epoch1: step500/4680
step 1000: accuracy:0.1120000034570694, confidence:0.9999980330467224, loss:15.909113883972168
epoch1: step1000/4680
step 1500: accuracy:0.10100000351667404, confidence:0.9964361786842346, loss:7.5363078117370605
epoch1: step1500/4680
step 2000: accuracy:0.11500000208616257, confidence:0.9994473457336426, loss:9.722116470336914
epoch1: step2000/4680
step 2500: accuracy:0.10499999672174454, confidence:0.9879978895187378, loss:6.723875999450684
epoch1: step2500/4680
step 3000: accuracy:0.09700000286102295, confidence:0.9144051671028137, loss:5.073917865753174
epoch1: step3000/4680
step 3500: accuracy:0.09600000083446503, confidence:0.9987533688545227, loss:9.89979362487793
epoch1: step3500/4680
step 4000: accuracy:0.10100000351667404, confidence:0.9951613545417786, loss:7.506718158721924
epoch1: step4000/4680
step 4500: accuracy:0.09200000017881393, confidence:0.9970218539237976, loss:8.250926971435547
epoch1: step4500/4680
step 0: accuracy:0.10899999737739563, confidence:0.9974886178970337, loss:8.175619125366211
epoch2: step0/4680
step 1000: accuracy:0.11999999731779099, confidence:0.9678987860679626, loss:5.670087814331055
epoch2: step500/4680
step 2000: accuracy:0.11500000208616257, confidence:0.9986456036567688, loss:8.910386085510254
epoch2: step1000/4680
step 3000: accuracy:0.10000000149011612, confidence:0.9992513656616211, loss:9.016325950622559
epoch2: step1500/4680
step 4000: accuracy:0.0989999994635582, confidence:0.9513353705406189, loss:5.294946670532227
epoch2: step2000/4680
step 5000: accuracy:0.09799999743700027, confidence:0.9881839752197266, loss:6.88444709777832
epoch2: step2500/4680
step 6000: accuracy:0.10400000214576721, confidence:0.9088523387908936, loss:5.32123327255249
epoch2: step3000/4680
step 7000: accuracy:0.09200000017881393, confidence:0.9994164705276489, loss:11.257349967956543
epoch2: step3500/4680
step 8000: accuracy:0.08399999886751175, confidence:0.997230589389801, loss:8.50917911529541
epoch2: step4000/4680
step 9000: accuracy:0.0949999988079071, confidence:0.9883160591125488, loss:6.848747730255127
epoch2: step4500/4680
step 0: accuracy:0.11599999666213989, confidence:0.9951419234275818, loss:7.417141914367676
epoch3: step0/4680
step 1500: accuracy:0.10400000214576721, confidence:0.9863412976264954, loss:6.575613498687744
epoch3: step500/4680
step 3000: accuracy:0.09200000017881393, confidence:0.9986093044281006, loss:9.600255966186523
epoch3: step1000/4680
step 4500: accuracy:0.09799999743700027, confidence:0.9982962608337402, loss:9.140466690063477
epoch3: step1500/4680
step 6000: accuracy:0.10199999809265137, confidence:0.9993557333946228, loss:10.020529747009277
epoch3: step2000/4680
step 7500: accuracy:0.10899999737739563, confidence:0.9958459138870239, loss:7.773115158081055
epoch3: step2500/4680
step 9000: accuracy:0.1080000028014183, confidence:0.8815072774887085, loss:4.94866418838501
epoch3: step3000/4680
step 10500: accuracy:0.10400000214576721, confidence:0.9339818954467773, loss:7.813571929931641
epoch3: step3500/4680
step 12000: accuracy:0.09000000357627869, confidence:0.998691976070404, loss:9.6048583984375
epoch3: step4000/4680
step 13500: accuracy:0.09799999743700027, confidence:0.9922307729721069, loss:7.525944232940674
epoch3: step4500/4680
step 0: accuracy:0.09300000220537186, confidence:0.994365394115448, loss:7.773978233337402
epoch4: step0/4680
step 2000: accuracy:0.08699999749660492, confidence:0.95940101146698, loss:5.680340766906738
epoch4: step500/4680
step 4000: accuracy:0.12099999934434891, confidence:0.9988462924957275, loss:9.045709609985352
epoch4: step1000/4680
step 6000: accuracy:0.09000000357627869, confidence:0.9992703199386597, loss:9.747182846069336
epoch4: step1500/4680
step 8000: accuracy:0.10199999809265137, confidence:0.993581235408783, loss:7.6534881591796875
epoch4: step2000/4680
step 10000: accuracy:0.09200000017881393, confidence:0.9512860774993896, loss:6.31146764755249
epoch4: step2500/4680
step 12000: accuracy:0.03200000151991844, confidence:0.7620382308959961, loss:5.832738399505615
epoch4: step3000/4680
step 14000: accuracy:0.08399999886751175, confidence:0.9944480657577515, loss:8.652070999145508
epoch4: step3500/4680
step 16000: accuracy:0.10199999809265137, confidence:0.9980459213256836, loss:8.907648086547852
epoch4: step4000/4680
step 18000: accuracy:0.1080000028014183, confidence:0.9908409714698792, loss:7.462128162384033
epoch4: step4500/4680
step 0: accuracy:0.08500000089406967, confidence:0.994498074054718, loss:8.154394149780273
epoch5: step0/4680
step 2500: accuracy:0.10599999874830246, confidence:0.9225113391876221, loss:5.182835102081299
epoch5: step500/4680
step 5000: accuracy:0.12600000202655792, confidence:0.9900504946708679, loss:7.275407314300537
epoch5: step1000/4680
step 7500: accuracy:0.12700000405311584, confidence:0.9956980347633362, loss:7.864685535430908
epoch5: step1500/4680
step 10000: accuracy:0.10300000011920929, confidence:0.9929490685462952, loss:7.523256301879883
epoch5: step2000/4680
step 12500: accuracy:0.09300000220537186, confidence:0.9850439429283142, loss:7.520196914672852
epoch5: step2500/4680
step 15000: accuracy:0.11299999803304672, confidence:0.923258364200592, loss:6.50654411315918
epoch5: step3000/4680
step 17500: accuracy:0.09799999743700027, confidence:0.994685709476471, loss:8.816559791564941
epoch5: step3500/4680
step 20000: accuracy:0.0860000029206276, confidence:0.9971405267715454, loss:8.780296325683594
epoch5: step4000/4680
step 22500: accuracy:0.0989999994635582, confidence:0.9997492432594299, loss:11.379582405090332
epoch5: step4500/4680
step 0: accuracy:0.11500000208616257, confidence:0.9998254776000977, loss:11.144061088562012
epoch6: step0/4680
step 3000: accuracy:0.11699999868869781, confidence:0.9902703166007996, loss:6.811250686645508
epoch6: step500/4680
step 6000: accuracy:0.11500000208616257, confidence:0.9989017248153687, loss:9.581439018249512
epoch6: step1000/4680
step 9000: accuracy:0.10400000214576721, confidence:0.9976111650466919, loss:8.698358535766602
epoch6: step1500/4680
step 12000: accuracy:0.11500000208616257, confidence:0.992847204208374, loss:7.842194557189941
epoch6: step2000/4680
step 15000: accuracy:0.08500000089406967, confidence:0.9182618856430054, loss:5.634661674499512
epoch6: step2500/4680
step 18000: accuracy:0.09300000220537186, confidence:0.7877830862998962, loss:6.371374130249023
epoch6: step3000/4680
step 21000: accuracy:0.11100000143051147, confidence:0.9980692267417908, loss:9.249605178833008
epoch6: step3500/4680
step 24000: accuracy:0.08900000154972076, confidence:0.9996637105941772, loss:10.92420768737793
epoch6: step4000/4680
step 27000: accuracy:0.0820000022649765, confidence:0.9799204468727112, loss:8.463582992553711
epoch6: step4500/4680
step 0: accuracy:0.11900000274181366, confidence:0.9837996363639832, loss:8.338471412658691
epoch7: step0/4680
step 3500: accuracy:0.10400000214576721, confidence:0.9917880892753601, loss:8.244991302490234
epoch7: step500/4680
step 7000: accuracy:0.1080000028014183, confidence:0.9959588646888733, loss:8.295248031616211
epoch7: step1000/4680
step 10500: accuracy:0.10199999809265137, confidence:0.9924806952476501, loss:7.368895530700684
epoch7: step1500/4680
step 14000: accuracy:0.10999999940395355, confidence:0.9712288975715637, loss:6.5944294929504395
epoch7: step2000/4680
step 17500: accuracy:0.09000000357627869, confidence:0.8435625433921814, loss:7.480746269226074
epoch7: step2500/4680
step 21000: accuracy:0.08299999684095383, confidence:0.775698721408844, loss:7.220749378204346
epoch7: step3000/4680
step 24500: accuracy:0.1120000034570694, confidence:0.9781094193458557, loss:6.72320556640625
epoch7: step3500/4680
step 28000: accuracy:0.09099999815225601, confidence:0.9884122610092163, loss:7.447751522064209
epoch7: step4000/4680
step 31500: accuracy:0.0860000029206276, confidence:0.9824599623680115, loss:9.309266090393066
epoch7: step4500/4680
step 0: accuracy:0.0949999988079071, confidence:0.986296534538269, loss:9.346882820129395
epoch8: step0/4680
step 4000: accuracy:0.11400000005960464, confidence:0.8316375613212585, loss:4.298678398132324
epoch8: step500/4680
step 8000: accuracy:0.09799999743700027, confidence:0.9954360127449036, loss:8.416397094726562
epoch8: step1000/4680
step 12000: accuracy:0.10700000077486038, confidence:0.9765831828117371, loss:6.287447452545166
epoch8: step1500/4680
step 16000: accuracy:0.09200000017881393, confidence:0.9312021136283875, loss:5.867552280426025
epoch8: step2000/4680
step 20000: accuracy:0.09200000017881393, confidence:0.8466403484344482, loss:5.928679466247559
epoch8: step2500/4680
step 24000: accuracy:0.11100000143051147, confidence:0.7905764579772949, loss:6.529785633087158
epoch8: step3000/4680
step 28000: accuracy:0.10400000214576721, confidence:0.9889771938323975, loss:7.221068859100342
epoch8: step3500/4680
step 32000: accuracy:0.09600000083446503, confidence:0.9898020625114441, loss:7.451644420623779
epoch8: step4000/4680
step 36000: accuracy:0.10400000214576721, confidence:0.9742968678474426, loss:7.9351019859313965
epoch8: step4500/4680
step 0: accuracy:0.10700000077486038, confidence:0.9793376326560974, loss:7.966421127319336
epoch9: step0/4680
step 4500: accuracy:0.08399999886751175, confidence:0.8389298319816589, loss:4.676980972290039
epoch9: step500/4680
step 9000: accuracy:0.11599999666213989, confidence:0.9901596903800964, loss:7.12677526473999
epoch9: step1000/4680
step 13500: accuracy:0.11400000005960464, confidence:0.9838832020759583, loss:6.466985702514648
epoch9: step1500/4680
step 18000: accuracy:0.11699999868869781, confidence:0.9549189805984497, loss:5.633327484130859
epoch9: step2000/4680
step 22500: accuracy:0.08900000154972076, confidence:0.8101761341094971, loss:7.292143821716309
epoch9: step2500/4680
step 27000: accuracy:0.10199999809265137, confidence:0.8723154664039612, loss:7.713456153869629
epoch9: step3000/4680
step 31500: accuracy:0.07699999958276749, confidence:0.9990422129631042, loss:9.934483528137207
epoch9: step3500/4680
step 36000: accuracy:0.09000000357627869, confidence:0.9950206279754639, loss:8.196928024291992
epoch9: step4000/4680
step 40500: accuracy:0.10700000077486038, confidence:0.9195603728294373, loss:7.4350056648254395
epoch9: step4500/4680
step 0: accuracy:0.10300000011920929, confidence:0.9498516917228699, loss:7.820276737213135
epoch10: step0/4680
step 5000: accuracy:0.10499999672174454, confidence:0.7142423391342163, loss:4.418758392333984
epoch10: step500/4680
step 10000: accuracy:0.11800000071525574, confidence:0.9694409370422363, loss:6.156203269958496
epoch10: step1000/4680
step 15000: accuracy:0.08100000023841858, confidence:0.9451135396957397, loss:5.696416854858398
epoch10: step1500/4680
step 20000: accuracy:0.12399999797344208, confidence:0.7557996511459351, loss:4.780993461608887
epoch10: step2000/4680
step 25000: accuracy:0.0820000022649765, confidence:0.8259981870651245, loss:7.75875997543335
epoch10: step2500/4680
step 30000: accuracy:0.11999999731779099, confidence:0.8817682266235352, loss:7.252477645874023
epoch10: step3000/4680
step 35000: accuracy:0.09799999743700027, confidence:0.9934029579162598, loss:7.830581188201904
epoch10: step3500/4680
step 40000: accuracy:0.0820000022649765, confidence:0.9935375452041626, loss:8.177604675292969
epoch10: step4000/4680
step 45000: accuracy:0.1080000028014183, confidence:0.972832441329956, loss:8.714621543884277
epoch10: step4500/4680
step 0: accuracy:0.10199999809265137, confidence:0.9771415591239929, loss:8.91451358795166
epoch11: step0/4680
step 5500: accuracy:0.09300000220537186, confidence:0.5641208291053772, loss:4.00677490234375
epoch11: step500/4680
step 11000: accuracy:0.1080000028014183, confidence:0.931520402431488, loss:5.679945468902588
epoch11: step1000/4680
step 16500: accuracy:0.10100000351667404, confidence:0.98126620054245, loss:6.718792915344238
epoch11: step1500/4680
step 22000: accuracy:0.10499999672174454, confidence:0.9324619770050049, loss:5.929468631744385
epoch11: step2000/4680
step 27500: accuracy:0.10000000149011612, confidence:0.8206458687782288, loss:7.478458404541016
epoch11: step2500/4680
step 33000: accuracy:0.11800000071525574, confidence:0.9061474204063416, loss:7.826520919799805
epoch11: step3000/4680
step 38500: accuracy:0.09099999815225601, confidence:0.9994013905525208, loss:10.074196815490723
epoch11: step3500/4680
step 44000: accuracy:0.09700000286102295, confidence:0.9983144998550415, loss:8.876567840576172
epoch11: step4000/4680
step 49500: accuracy:0.10400000214576721, confidence:0.8987175822257996, loss:8.088698387145996
epoch11: step4500/4680
step 0: accuracy:0.0989999994635582, confidence:0.9135522246360779, loss:8.195687294006348
epoch12: step0/4680
step 6000: accuracy:0.0989999994635582, confidence:0.8203420042991638, loss:5.160015106201172
epoch12: step500/4680
step 12000: accuracy:0.12200000137090683, confidence:0.990170955657959, loss:7.76078462600708
epoch12: step1000/4680
step 18000: accuracy:0.1080000028014183, confidence:0.9920704364776611, loss:7.500614166259766
epoch12: step1500/4680
step 24000: accuracy:0.09300000220537186, confidence:0.8876786828041077, loss:5.979413032531738
epoch12: step2000/4680
step 30000: accuracy:0.09700000286102295, confidence:0.8162782192230225, loss:7.308544635772705
epoch12: step2500/4680
step 36000: accuracy:0.10999999940395355, confidence:0.909192681312561, loss:7.939770698547363
epoch12: step3000/4680
step 42000: accuracy:0.10999999940395355, confidence:0.9954344630241394, loss:8.404168128967285
epoch12: step3500/4680
step 48000: accuracy:0.09200000017881393, confidence:0.9958413243293762, loss:8.693819999694824
epoch12: step4000/4680
step 54000: accuracy:0.08699999749660492, confidence:0.9798369407653809, loss:8.612950325012207
epoch12: step4500/4680
step 0: accuracy:0.09799999743700027, confidence:0.9785903692245483, loss:8.467087745666504
epoch13: step0/4680
step 6500: accuracy:0.09799999743700027, confidence:0.75667804479599, loss:4.85467004776001
epoch13: step500/4680
step 13000: accuracy:0.13199999928474426, confidence:0.9025075435638428, loss:5.607666969299316
epoch13: step1000/4680
step 19500: accuracy:0.10300000011920929, confidence:0.927297055721283, loss:5.47211217880249
epoch13: step1500/4680
step 26000: accuracy:0.10599999874830246, confidence:0.7767583131790161, loss:7.037328720092773
epoch13: step2000/4680
step 32500: accuracy:0.10000000149011612, confidence:0.9398383498191833, loss:9.606072425842285
epoch13: step2500/4680
step 39000: accuracy:0.10599999874830246, confidence:0.9526312351226807, loss:8.206231117248535
epoch13: step3000/4680
step 45500: accuracy:0.09200000017881393, confidence:0.9592151045799255, loss:6.28745174407959
epoch13: step3500/4680
step 52000: accuracy:0.11299999803304672, confidence:0.9842773079872131, loss:8.32887077331543
epoch13: step4000/4680
step 58500: accuracy:0.11800000071525574, confidence:0.9921830892562866, loss:8.39475154876709
epoch13: step4500/4680
step 0: accuracy:0.10000000149011612, confidence:0.9937695860862732, loss:8.95130443572998
epoch14: step0/4680
step 7000: accuracy:0.0820000022649765, confidence:0.8730000257492065, loss:5.401584148406982
epoch14: step500/4680
step 14000: accuracy:0.11299999803304672, confidence:0.9043917059898376, loss:5.553158283233643
epoch14: step1000/4680
step 21000: accuracy:0.10199999809265137, confidence:0.8383905291557312, loss:4.762630462646484
epoch14: step1500/4680
step 28000: accuracy:0.11699999868869781, confidence:0.9032250046730042, loss:6.101228713989258
epoch14: step2000/4680
step 35000: accuracy:0.10700000077486038, confidence:0.7877163887023926, loss:7.065045356750488
epoch14: step2500/4680
step 42000: accuracy:0.12600000202655792, confidence:0.902320921421051, loss:7.414297103881836
epoch14: step3000/4680
step 49000: accuracy:0.0860000029206276, confidence:0.968633234500885, loss:7.381943225860596
epoch14: step3500/4680
step 56000: accuracy:0.09000000357627869, confidence:0.9938737750053406, loss:9.9630765914917
epoch14: step4000/4680
step 63000: accuracy:0.07900000363588333, confidence:0.9968894124031067, loss:10.599682807922363
epoch14: step4500/4680
step 0: accuracy:0.10999999940395355, confidence:0.9963717460632324, loss:10.219550132751465
epoch15: step0/4680
step 7500: accuracy:0.09399999678134918, confidence:0.819111704826355, loss:5.139086723327637
epoch15: step500/4680
step 15000: accuracy:0.12300000339746475, confidence:0.9170148372650146, loss:5.841506004333496
epoch15: step1000/4680
step 22500: accuracy:0.10599999874830246, confidence:0.8453441858291626, loss:4.807117938995361
epoch15: step1500/4680
step 30000: accuracy:0.12600000202655792, confidence:0.7450107336044312, loss:6.296647548675537
epoch15: step2000/4680
step 37500: accuracy:0.10100000351667404, confidence:0.7692791819572449, loss:7.172635555267334
epoch15: step2500/4680
step 45000: accuracy:0.11100000143051147, confidence:0.9029473066329956, loss:7.7042460441589355
epoch15: step3000/4680
step 52500: accuracy:0.10599999874830246, confidence:0.9857747554779053, loss:7.701260566711426
epoch15: step3500/4680
step 60000: accuracy:0.09399999678134918, confidence:0.9888555407524109, loss:8.163006782531738
epoch15: step4000/4680
step 67500: accuracy:0.09399999678134918, confidence:0.9825266003608704, loss:9.480875015258789
epoch15: step4500/4680
step 0: accuracy:0.09799999743700027, confidence:0.9807214736938477, loss:9.228289604187012
epoch16: step0/4680
step 8000: accuracy:0.11900000274181366, confidence:0.8068455457687378, loss:4.69722843170166
epoch16: step500/4680
step 16000: accuracy:0.10100000351667404, confidence:0.946635901927948, loss:6.395201206207275
epoch16: step1000/4680
step 24000: accuracy:0.11599999666213989, confidence:0.9355015158653259, loss:5.768407344818115
epoch16: step1500/4680
step 32000: accuracy:0.10400000214576721, confidence:0.7580885291099548, loss:6.011026382446289
epoch16: step2000/4680
step 40000: accuracy:0.11999999731779099, confidence:0.725555419921875, loss:7.094499111175537
epoch16: step2500/4680
step 48000: accuracy:0.13500000536441803, confidence:0.8134981393814087, loss:6.983363628387451
epoch16: step3000/4680
step 56000: accuracy:0.10300000011920929, confidence:0.9931809306144714, loss:8.350358009338379
epoch16: step3500/4680
step 64000: accuracy:0.0860000029206276, confidence:0.9899035096168518, loss:8.163613319396973
epoch16: step4000/4680
step 72000: accuracy:0.08900000154972076, confidence:0.9527480602264404, loss:8.835644721984863
epoch16: step4500/4680
step 0: accuracy:0.10999999940395355, confidence:0.9493585228919983, loss:8.263114929199219
epoch17: step0/4680
step 8500: accuracy:0.09200000017881393, confidence:0.8372238874435425, loss:5.046252250671387
epoch17: step500/4680
step 17000: accuracy:0.13500000536441803, confidence:0.9078517556190491, loss:5.758622169494629
epoch17: step1000/4680
step 25500: accuracy:0.09600000083446503, confidence:0.8535082936286926, loss:5.157129764556885
epoch17: step1500/4680
step 34000: accuracy:0.10000000149011612, confidence:0.8399979472160339, loss:7.755703926086426
epoch17: step2000/4680
step 42500: accuracy:0.08900000154972076, confidence:0.7874171733856201, loss:6.863384246826172
epoch17: step2500/4680
step 51000: accuracy:0.10499999672174454, confidence:0.9250341653823853, loss:7.73341178894043
epoch17: step3000/4680
step 59500: accuracy:0.10499999672174454, confidence:0.9962349534034729, loss:9.236181259155273
epoch17: step3500/4680
step 68000: accuracy:0.10599999874830246, confidence:0.9968008399009705, loss:9.4312162399292
epoch17: step4000/4680
step 76500: accuracy:0.09799999743700027, confidence:0.8841295838356018, loss:7.96794319152832
epoch17: step4500/4680
step 0: accuracy:0.09000000357627869, confidence:0.8958483338356018, loss:8.177367210388184
epoch18: step0/4680
step 9000: accuracy:0.10599999874830246, confidence:0.9697343707084656, loss:7.359752178192139
epoch18: step500/4680
step 18000: accuracy:0.10700000077486038, confidence:0.9672225713729858, loss:7.2725067138671875
epoch18: step1000/4680
step 27000: accuracy:0.08799999952316284, confidence:0.9145063757896423, loss:6.3462653160095215
epoch18: step1500/4680
step 36000: accuracy:0.10199999809265137, confidence:0.8315471410751343, loss:7.065792560577393
epoch18: step2000/4680
step 45000: accuracy:0.10000000149011612, confidence:0.7153701186180115, loss:7.6318864822387695
epoch18: step2500/4680
step 54000: accuracy:0.12099999934434891, confidence:0.7382427453994751, loss:7.246362686157227
epoch18: step3000/4680
step 63000: accuracy:0.08100000023841858, confidence:0.9465171098709106, loss:7.597496509552002
epoch18: step3500/4680
step 72000: accuracy:0.08299999684095383, confidence:0.9625864028930664, loss:8.598751068115234
epoch18: step4000/4680
step 81000: accuracy:0.09099999815225601, confidence:0.9308238625526428, loss:10.089177131652832
epoch18: step4500/4680
step 0: accuracy:0.08299999684095383, confidence:0.9337000846862793, loss:9.652496337890625
epoch19: step0/4680
step 9500: accuracy:0.08399999886751175, confidence:0.8126783967018127, loss:5.552018165588379
epoch19: step500/4680
step 19000: accuracy:0.13699999451637268, confidence:0.8804648518562317, loss:5.708650588989258
epoch19: step1000/4680
step 28500: accuracy:0.08699999749660492, confidence:0.8381800651550293, loss:5.6065897941589355
epoch19: step1500/4680
step 38000: accuracy:0.10100000351667404, confidence:0.813799262046814, loss:7.52271032333374
epoch19: step2000/4680
step 47500: accuracy:0.09000000357627869, confidence:0.6859609484672546, loss:7.463634967803955
epoch19: step2500/4680
step 57000: accuracy:0.11100000143051147, confidence:0.8414516448974609, loss:8.100900650024414
epoch19: step3000/4680
step 66500: accuracy:0.0989999994635582, confidence:0.9836617112159729, loss:8.669265747070312
epoch19: step3500/4680
step 76000: accuracy:0.10199999809265137, confidence:0.9697989225387573, loss:8.588950157165527
epoch19: step4000/4680
step 85500: accuracy:0.10899999737739563, confidence:0.9368299841880798, loss:8.376150131225586
epoch19: step4500/4680
step 0: accuracy:0.10000000149011612, confidence:0.9448763132095337, loss:8.737846374511719
epoch20: step0/4680
step 10000: accuracy:0.09000000357627869, confidence:0.7765821814537048, loss:4.95343542098999
epoch20: step500/4680
step 20000: accuracy:0.125, confidence:0.941190779209137, loss:6.643878936767578
epoch20: step1000/4680
step 30000: accuracy:0.09099999815225601, confidence:0.8066117167472839, loss:5.3580241203308105
epoch20: step1500/4680
step 40000: accuracy:0.0989999994635582, confidence:0.775932252407074, loss:7.851061820983887
epoch20: step2000/4680
step 50000: accuracy:0.10400000214576721, confidence:0.6984999179840088, loss:7.708195209503174
epoch20: step2500/4680
step 60000: accuracy:0.10999999940395355, confidence:0.8262911438941956, loss:7.920572280883789
epoch20: step3000/4680
step 70000: accuracy:0.10199999809265137, confidence:0.9712720513343811, loss:8.678388595581055
epoch20: step3500/4680
step 80000: accuracy:0.12800000607967377, confidence:0.9379750490188599, loss:8.369956016540527
epoch20: step4000/4680
step 90000: accuracy:0.11400000005960464, confidence:0.8531768918037415, loss:8.684416770935059
epoch20: step4500/4680
step 0: accuracy:0.08500000089406967, confidence:0.8735646605491638, loss:8.874786376953125
epoch21: step0/4680
step 10500: accuracy:0.10199999809265137, confidence:0.8492124676704407, loss:5.551823139190674
epoch21: step500/4680
step 21000: accuracy:0.11100000143051147, confidence:0.9223230481147766, loss:6.031003475189209
epoch21: step1000/4680
step 31500: accuracy:0.08799999952316284, confidence:0.8425198793411255, loss:5.6520562171936035
epoch21: step1500/4680
step 42000: accuracy:0.12200000137090683, confidence:0.7396388649940491, loss:6.052460193634033
epoch21: step2000/4680
step 52500: accuracy:0.0860000029206276, confidence:0.6953179240226746, loss:8.050102233886719
epoch21: step2500/4680
step 63000: accuracy:0.10599999874830246, confidence:0.8223262429237366, loss:8.24345874786377
epoch21: step3000/4680
step 73500: accuracy:0.11400000005960464, confidence:0.9935107827186584, loss:9.18101978302002
epoch21: step3500/4680
step 84000: accuracy:0.0989999994635582, confidence:0.9583150744438171, loss:7.944436550140381
epoch21: step4000/4680
step 94500: accuracy:0.12300000339746475, confidence:0.8216869235038757, loss:8.676170349121094
epoch21: step4500/4680
step 0: accuracy:0.11100000143051147, confidence:0.8538010120391846, loss:8.695174217224121
epoch22: step0/4680
step 11000: accuracy:0.09399999678134918, confidence:0.9000905156135559, loss:6.780184745788574
epoch22: step500/4680
step 22000: accuracy:0.10700000077486038, confidence:0.8712539076805115, loss:5.917971611022949
epoch22: step1000/4680
step 33000: accuracy:0.11800000071525574, confidence:0.8675506711006165, loss:5.65320348739624
epoch22: step1500/4680
step 44000: accuracy:0.11900000274181366, confidence:0.7734403014183044, loss:6.9354658126831055
epoch22: step2000/4680
step 55000: accuracy:0.08399999886751175, confidence:0.6888983249664307, loss:8.189940452575684
epoch22: step2500/4680
step 66000: accuracy:0.10999999940395355, confidence:0.8185843229293823, loss:8.531440734863281
epoch22: step3000/4680
step 77000: accuracy:0.10599999874830246, confidence:0.9958307147026062, loss:9.839029312133789
epoch22: step3500/4680
step 88000: accuracy:0.10400000214576721, confidence:0.9301069974899292, loss:8.121686935424805
epoch22: step4000/4680
step 99000: accuracy:0.09600000083446503, confidence:0.8672453165054321, loss:8.690555572509766
epoch22: step4500/4680
step 0: accuracy:0.0989999994635582, confidence:0.8776171803474426, loss:8.574560165405273
epoch23: step0/4680
step 11500: accuracy:0.11800000071525574, confidence:0.7564883828163147, loss:4.880980014801025
epoch23: step500/4680
step 23000: accuracy:0.14000000059604645, confidence:0.8924165964126587, loss:6.188247203826904
epoch23: step1000/4680
step 34500: accuracy:0.10599999874830246, confidence:0.852816104888916, loss:5.608898162841797
epoch23: step1500/4680
step 46000: accuracy:0.13199999928474426, confidence:0.7727878093719482, loss:7.472238540649414
epoch23: step2000/4680
step 57500: accuracy:0.09399999678134918, confidence:0.69093918800354, loss:7.665505409240723
epoch23: step2500/4680
step 69000: accuracy:0.10999999940395355, confidence:0.8387347459793091, loss:7.841123580932617
epoch23: step3000/4680
step 80500: accuracy:0.09200000017881393, confidence:0.9878439903259277, loss:9.307490348815918
epoch23: step3500/4680
step 92000: accuracy:0.09799999743700027, confidence:0.9214602112770081, loss:9.114891052246094
epoch23: step4000/4680
step 103500: accuracy:0.09399999678134918, confidence:0.8698137402534485, loss:8.729595184326172
epoch23: step4500/4680
step 0: accuracy:0.10199999809265137, confidence:0.8934025764465332, loss:8.787202835083008
epoch24: step0/4680
step 12000: accuracy:0.11500000208616257, confidence:0.8723611831665039, loss:5.798996925354004
epoch24: step500/4680
step 24000: accuracy:0.125, confidence:0.9043965935707092, loss:6.345829010009766
epoch24: step1000/4680
step 36000: accuracy:0.0949999988079071, confidence:0.7798516750335693, loss:5.178106307983398
epoch24: step1500/4680
step 48000: accuracy:0.13199999928474426, confidence:0.763247013092041, loss:7.263812065124512
epoch24: step2000/4680
step 60000: accuracy:0.1120000034570694, confidence:0.7147487998008728, loss:6.83328104019165
epoch24: step2500/4680
step 72000: accuracy:0.11100000143051147, confidence:0.7432419657707214, loss:7.231728553771973
epoch24: step3000/4680
step 84000: accuracy:0.0989999994635582, confidence:0.9624054431915283, loss:8.573917388916016
epoch24: step3500/4680
step 96000: accuracy:0.11100000143051147, confidence:0.932582676410675, loss:8.60793399810791
epoch24: step4000/4680
step 108000: accuracy:0.10199999809265137, confidence:0.9257634282112122, loss:9.491388320922852
epoch24: step4500/4680
step 0: accuracy:0.09700000286102295, confidence:0.9254753589630127, loss:9.613938331604004
epoch25: step0/4680
step 12500: accuracy:0.10199999809265137, confidence:0.86702561378479, loss:5.770592212677002
epoch25: step500/4680
step 25000: accuracy:0.11999999731779099, confidence:0.8626934885978699, loss:5.9195427894592285
epoch25: step1000/4680
step 37500: accuracy:0.10499999672174454, confidence:0.7691805958747864, loss:5.2779316902160645
epoch25: step1500/4680
step 50000: accuracy:0.10199999809265137, confidence:0.8304876089096069, loss:7.444169998168945
epoch25: step2000/4680
step 62500: accuracy:0.07800000160932541, confidence:0.6833414435386658, loss:7.781594753265381
epoch25: step2500/4680
step 75000: accuracy:0.11500000208616257, confidence:0.7529041767120361, loss:7.530167102813721
epoch25: step3000/4680
step 87500: accuracy:0.08799999952316284, confidence:0.9627329111099243, loss:8.645185470581055
epoch25: step3500/4680
step 100000: accuracy:0.10199999809265137, confidence:0.9387813806533813, loss:7.55783748626709
epoch25: step4000/4680
step 112500: accuracy:0.1120000034570694, confidence:0.7965555787086487, loss:7.8458476066589355
epoch25: step4500/4680
step 0: accuracy:0.10400000214576721, confidence:0.8247336745262146, loss:7.917819023132324
epoch26: step0/4680
step 13000: accuracy:0.09000000357627869, confidence:0.8330309391021729, loss:5.706436634063721
epoch26: step500/4680
step 26000: accuracy:0.164000004529953, confidence:0.8654817938804626, loss:5.778365135192871
epoch26: step1000/4680
step 39000: accuracy:0.09700000286102295, confidence:0.7911360263824463, loss:5.510849475860596
epoch26: step1500/4680
step 52000: accuracy:0.12099999934434891, confidence:0.8226577639579773, loss:6.916791915893555
epoch26: step2000/4680
step 65000: accuracy:0.08299999684095383, confidence:0.752625584602356, loss:9.167296409606934
epoch26: step2500/4680
step 78000: accuracy:0.12200000137090683, confidence:0.8200061321258545, loss:9.284025192260742
epoch26: step3000/4680
step 91000: accuracy:0.10100000351667404, confidence:0.9928293228149414, loss:10.238173484802246
epoch26: step3500/4680
step 104000: accuracy:0.09200000017881393, confidence:0.9799646139144897, loss:8.970490455627441
epoch26: step4000/4680
step 117000: accuracy:0.10100000351667404, confidence:0.8469040393829346, loss:7.8265838623046875
epoch26: step4500/4680
step 0: accuracy:0.10199999809265137, confidence:0.8673376441001892, loss:8.097291946411133
epoch27: step0/4680
step 13500: accuracy:0.10100000351667404, confidence:0.8446078896522522, loss:6.2862372398376465
epoch27: step500/4680
step 27000: accuracy:0.11800000071525574, confidence:0.9237566590309143, loss:7.020245552062988
epoch27: step1000/4680
step 40500: accuracy:0.10000000149011612, confidence:0.7570585012435913, loss:5.3603901863098145
epoch27: step1500/4680
step 54000: accuracy:0.0989999994635582, confidence:0.8733459711074829, loss:7.635834217071533
epoch27: step2000/4680
step 67500: accuracy:0.10199999809265137, confidence:0.7718296647071838, loss:7.4413042068481445
epoch27: step2500/4680
step 81000: accuracy:0.14100000262260437, confidence:0.7788001894950867, loss:7.478148460388184
epoch27: step3000/4680
step 94500: accuracy:0.09000000357627869, confidence:0.9513786435127258, loss:9.394057273864746
epoch27: step3500/4680
step 108000: accuracy:0.08799999952316284, confidence:0.9106020927429199, loss:10.393566131591797
epoch27: step4000/4680
step 121500: accuracy:0.11699999868869781, confidence:0.9416890740394592, loss:11.014370918273926
epoch27: step4500/4680
step 0: accuracy:0.10700000077486038, confidence:0.9433524012565613, loss:10.759041786193848
epoch28: step0/4680
step 14000: accuracy:0.11400000005960464, confidence:0.6658554673194885, loss:4.07852029800415
epoch28: step500/4680
step 28000: accuracy:0.1080000028014183, confidence:0.9422993659973145, loss:6.587532043457031
epoch28: step1000/4680
step 42000: accuracy:0.07800000160932541, confidence:0.7797778844833374, loss:5.581908702850342
epoch28: step1500/4680
step 56000: accuracy:0.11699999868869781, confidence:0.7398648262023926, loss:6.897352695465088
epoch28: step2000/4680
step 70000: accuracy:0.08500000089406967, confidence:0.7584624886512756, loss:7.400571823120117
epoch28: step2500/4680
step 84000: accuracy:0.14499999582767487, confidence:0.7699616551399231, loss:7.132448196411133
epoch28: step3000/4680
step 98000: accuracy:0.09799999743700027, confidence:0.9061992764472961, loss:8.733452796936035
epoch28: step3500/4680
step 112000: accuracy:0.10899999737739563, confidence:0.8755934238433838, loss:9.299745559692383
epoch28: step4000/4680
step 126000: accuracy:0.10899999737739563, confidence:0.8685662150382996, loss:9.883012771606445
epoch28: step4500/4680
step 0: accuracy:0.09799999743700027, confidence:0.8818660378456116, loss:9.782231330871582
epoch29: step0/4680
step 14500: accuracy:0.08799999952316284, confidence:0.9585018157958984, loss:8.517600059509277
epoch29: step500/4680
step 29000: accuracy:0.024000000208616257, confidence:0.8479190468788147, loss:6.113825798034668
epoch29: step1000/4680
step 43500: accuracy:0.10199999809265137, confidence:0.7512789368629456, loss:5.277893543243408
epoch29: step1500/4680
step 58000: accuracy:0.12600000202655792, confidence:0.730125904083252, loss:6.0956339836120605
epoch29: step2000/4680
step 72500: accuracy:0.07599999755620956, confidence:0.703446626663208, loss:7.3642168045043945
epoch29: step2500/4680
step 87000: accuracy:0.11500000208616257, confidence:0.7626084685325623, loss:7.097691059112549
epoch29: step3000/4680
step 101500: accuracy:0.09099999815225601, confidence:0.9740012288093567, loss:9.689512252807617
epoch29: step3500/4680
step 116000: accuracy:0.0989999994635582, confidence:0.9227178692817688, loss:8.051363945007324
epoch29: step4000/4680
step 130500: accuracy:0.1120000034570694, confidence:0.819010853767395, loss:8.390645980834961
epoch29: step4500/4680
2018-06-15 20:08:37.057676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000b:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 20:08:37.057854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000b:01:00.0, compute capability: 6.0)
2018-06-15 20:08:38.471963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000b:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
CHEKPOINT DIR: uniform
model has been loaded from fashion-mnist_classifier.pkl
---------
Variables: name (type shape) [size]
---------
Variable:0 (float32_ref 5x5x1x32) [800, bytes: 3200]
Variable_1:0 (float32_ref 32) [32, bytes: 128]
Variable_2:0 (float32_ref 5x5x32x64) [51200, bytes: 204800]
Variable_3:0 (float32_ref 64) [64, bytes: 256]
Variable_4:0 (float32_ref 3136x1024) [3211264, bytes: 12845056]
Variable_5:0 (float32_ref 1024) [1024, bytes: 4096]
Variable_6:0 (float32_ref 1024x10) [10240, bytes: 40960]
Variable_7:0 (float32_ref 10) [10, bytes: 40]
discriminator/d_conv1/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]
discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]
discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]
discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]
generator/g_fc1/Matrix:0 (float32_ref 74x1024) [75776, bytes: 303104]
generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]
generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]
generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]
classifier/c_fc1/Matrix:0 (float32_ref 1024x64) [65536, bytes: 262144]
classifier/c_fc1/bias:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/beta:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/gamma:0 (float32_ref 64) [64, bytes: 256]
classifier/c_fc2/Matrix:0 (float32_ref 64x12) [768, bytes: 3072]
classifier/c_fc2/bias:0 (float32_ref 12) [12, bytes: 48]
Total size of variables: 16552792
Total bytes of variables: 66211168
 [*] Reading checkpoints...
 [*] Success to read MultiModalInfoGAN.model-43721
 [*] Load SUCCESS


SAMPLES SIZE=4680,LABELS=299520


SAVED TRAINING SET generated_training_set_fashion-mnist_UniformSample_mu_0_sigma_0.15_czrc_czcc_rzcc_rzrc
 [*] Training finished!
 [*] Testing finished!
2018-06-15 20:09:14.442671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000b:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 20:09:14.442856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000b:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
model has been loaded from fashion-mnist_classifier.pkl
argmax:[4 4 4 ..., 6 6 6]
step 0: accuracy:0.0, confidence:0.7425218820571899, loss:10.56462287902832
Assinging:5
[   0    0  936    0 4563  234 4267]
argmax:[1 1 1 ..., 1 1 1]
step 0: accuracy:0.0, confidence:0.9996853470802307, loss:18.799694061279297
Assinging:2
[    0 10000]
argmax:[2 2 3 ..., 2 2 2]
step 0: accuracy:0.0, confidence:0.8626964688301086, loss:10.94996166229248
Assinging:3
[ 702    0 6958 1989  117    0  234]
argmax:[8 5 5 ..., 7 7 7]
step 0: accuracy:0.0, confidence:0.8979995250701904, loss:6.534582614898682
Assinging:8
[   0    0    0    0    0 4212    0 5671  117]
argmax:[9 9 9 ..., 9 9 9]
step 0: accuracy:0.9765999913215637, confidence:0.9588537812232971, loss:0.06044063717126846
Assinging:10
[   0    0    0    0    0    0    0  234    0 9766]
argmax:[4 4 4 ..., 4 4 4]
step 0: accuracy:0.0, confidence:0.7177536487579346, loss:11.704702377319336
Assinging:5
[   0    0  234    0 4150    0 2106    0 3510]
argmax:[3 3 3 ..., 3 3 3]
step 0: accuracy:0.0, confidence:0.9216770529747009, loss:14.689303398132324
Assinging:4
[ 936    0  117 7426    0    0    0    0 1521]
argmax:[0 0 0 ..., 0 0 0]
step 0: accuracy:0.0, confidence:0.9577694535255432, loss:16.625940322875977
Assinging:1
[8128    0    0    0    0    0 1872]
argmax:[3 3 3 ..., 3 3 3]
step 0: accuracy:0.0, confidence:0.9612778425216675, loss:13.25690746307373
Assinging:4
[ 117    0    0 5320    0    0  819    0 3744]
argmax:[7 7 5 ..., 7 7 7]
step 0: accuracy:0.0, confidence:0.9746702909469604, loss:11.080684661865234
Assinging:8
[   0    0    0    0    0 1755    0 8245]
2018-06-15 20:09:29.187590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000b:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 20:09:29.187778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000b:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
step 0: accuracy:0.0820000022649765, confidence:0.8448357582092285, loss:8.38196086883545
epoch0: step0/4680
step 0: accuracy:0.08799999952316284, confidence:1.0, loss:41.310665130615234
epoch0: step500/4680
step 0: accuracy:0.08799999952316284, confidence:1.0, loss:50.042808532714844
epoch0: step1000/4680
step 0: accuracy:0.10400000214576721, confidence:0.9995164275169373, loss:9.576704025268555
epoch0: step1500/4680
step 0: accuracy:0.0860000029206276, confidence:0.9999969005584717, loss:14.233768463134766
epoch0: step2000/4680
step 0: accuracy:0.10400000214576721, confidence:0.999664306640625, loss:9.763884544372559
epoch0: step2500/4680
step 0: accuracy:0.07900000363588333, confidence:0.999397337436676, loss:10.133216857910156
epoch0: step3000/4680
step 0: accuracy:0.11599999666213989, confidence:0.9999842047691345, loss:12.594537734985352
epoch0: step3500/4680
step 0: accuracy:0.10100000351667404, confidence:0.9999601244926453, loss:11.928348541259766
epoch0: step4000/4680
step 0: accuracy:0.0860000029206276, confidence:0.9996747970581055, loss:10.128704071044922
epoch0: step4500/4680
step 0: accuracy:0.09399999678134918, confidence:0.9996914267539978, loss:10.104364395141602
epoch1: step0/4680
step 500: accuracy:0.10400000214576721, confidence:0.9979151487350464, loss:8.378228187561035
epoch1: step500/4680
step 1000: accuracy:0.1120000034570694, confidence:0.9998491406440735, loss:11.563666343688965
epoch1: step1000/4680
step 1500: accuracy:0.10300000011920929, confidence:0.9996764063835144, loss:10.369672775268555
epoch1: step1500/4680
step 2000: accuracy:0.11500000208616257, confidence:0.9998573064804077, loss:10.296584129333496
epoch1: step2000/4680
step 2500: accuracy:0.10300000011920929, confidence:0.9968846440315247, loss:7.520419120788574
epoch1: step2500/4680
step 3000: accuracy:0.10400000214576721, confidence:0.9951819777488708, loss:7.965249538421631
epoch1: step3000/4680
step 3500: accuracy:0.08900000154972076, confidence:0.9928116202354431, loss:7.311490058898926
epoch1: step3500/4680
step 4000: accuracy:0.10400000214576721, confidence:0.9974642992019653, loss:7.974383354187012
epoch1: step4000/4680
step 4500: accuracy:0.1080000028014183, confidence:0.9999638795852661, loss:11.971352577209473
epoch1: step4500/4680
step 0: accuracy:0.09099999815225601, confidence:0.9999447464942932, loss:11.644372940063477
epoch2: step0/4680
step 1000: accuracy:0.10499999672174454, confidence:0.9814983010292053, loss:6.671085357666016
epoch2: step500/4680
step 2000: accuracy:0.11500000208616257, confidence:0.9991563558578491, loss:9.463830947875977
epoch2: step1000/4680
step 3000: accuracy:0.10999999940395355, confidence:0.9999410510063171, loss:12.231074333190918
epoch2: step1500/4680
step 4000: accuracy:0.0989999994635582, confidence:0.9999678730964661, loss:12.236026763916016
epoch2: step2000/4680
step 5000: accuracy:0.10499999672174454, confidence:0.9978387355804443, loss:8.011244773864746
epoch2: step2500/4680
step 6000: accuracy:0.0949999988079071, confidence:0.9672479033470154, loss:5.857813835144043
epoch2: step3000/4680
step 7000: accuracy:0.10000000149011612, confidence:0.9981734752655029, loss:8.427811622619629
epoch2: step3500/4680
step 8000: accuracy:0.10199999809265137, confidence:0.9988667964935303, loss:9.02904987335205
epoch2: step4000/4680
step 9000: accuracy:0.10499999672174454, confidence:0.999988853931427, loss:12.866623878479004
epoch2: step4500/4680
step 0: accuracy:0.09000000357627869, confidence:0.9999635815620422, loss:11.834222793579102
epoch3: step0/4680
step 1500: accuracy:0.10000000149011612, confidence:0.979453980922699, loss:6.652602195739746
epoch3: step500/4680
step 3000: accuracy:0.09200000017881393, confidence:0.9916850328445435, loss:7.676151275634766
epoch3: step1000/4680
step 4500: accuracy:0.08699999749660492, confidence:0.996113121509552, loss:8.08126449584961
epoch3: step1500/4680
step 6000: accuracy:0.10199999809265137, confidence:0.9999803304672241, loss:12.882686614990234
epoch3: step2000/4680
step 7500: accuracy:0.09300000220537186, confidence:0.9861554503440857, loss:6.276172161102295
epoch3: step2500/4680
step 9000: accuracy:0.0989999994635582, confidence:0.9567229151725769, loss:5.768826961517334
epoch3: step3000/4680
step 10500: accuracy:0.09700000286102295, confidence:0.9848695993423462, loss:6.68007755279541
epoch3: step3500/4680
step 12000: accuracy:0.11299999803304672, confidence:0.9910284876823425, loss:6.791438579559326
epoch3: step4000/4680
step 13500: accuracy:0.10100000351667404, confidence:0.9998946785926819, loss:10.827262878417969
epoch3: step4500/4680
step 0: accuracy:0.10700000077486038, confidence:0.9998255968093872, loss:10.156156539916992
epoch4: step0/4680
step 2000: accuracy:0.10999999940395355, confidence:0.9922553896903992, loss:7.37325382232666
epoch4: step500/4680
step 4000: accuracy:0.12099999934434891, confidence:0.9884868264198303, loss:7.076606273651123
epoch4: step1000/4680
step 6000: accuracy:0.09000000357627869, confidence:0.9990535974502563, loss:9.689123153686523
epoch4: step1500/4680
step 8000: accuracy:0.10199999809265137, confidence:0.9996294975280762, loss:9.949106216430664
epoch4: step2000/4680
step 10000: accuracy:0.09799999743700027, confidence:0.9995408058166504, loss:9.4872407913208
epoch4: step2500/4680
step 12000: accuracy:0.09600000083446503, confidence:0.8945464491844177, loss:5.3082194328308105
epoch4: step3000/4680
step 14000: accuracy:0.1080000028014183, confidence:0.9688869714736938, loss:6.012498378753662
epoch4: step3500/4680
step 16000: accuracy:0.09399999678134918, confidence:0.9675611257553101, loss:5.8757805824279785
epoch4: step4000/4680
step 18000: accuracy:0.09799999743700027, confidence:0.9998129606246948, loss:10.54861068725586
epoch4: step4500/4680
step 0: accuracy:0.08699999749660492, confidence:0.9998361468315125, loss:10.664121627807617
epoch5: step0/4680
step 2500: accuracy:0.09799999743700027, confidence:0.9364835619926453, loss:5.978207588195801
epoch5: step500/4680
step 5000: accuracy:0.12600000202655792, confidence:0.9651111364364624, loss:7.045685768127441
epoch5: step1000/4680
step 7500: accuracy:0.10499999672174454, confidence:0.9964095950126648, loss:8.729482650756836
epoch5: step1500/4680
step 10000: accuracy:0.10300000011920929, confidence:0.9995680451393127, loss:10.026577949523926
epoch5: step2000/4680
step 12500: accuracy:0.10199999809265137, confidence:0.9916265606880188, loss:6.808223724365234
epoch5: step2500/4680
step 15000: accuracy:0.0860000029206276, confidence:0.8446547389030457, loss:4.855653762817383
epoch5: step3000/4680
step 17500: accuracy:0.10400000214576721, confidence:0.9682696461677551, loss:6.530889987945557
epoch5: step3500/4680
step 20000: accuracy:0.09200000017881393, confidence:0.9471323490142822, loss:5.901646137237549
epoch5: step4000/4680
step 22500: accuracy:0.10999999940395355, confidence:0.9995500445365906, loss:9.867883682250977
epoch5: step4500/4680
step 0: accuracy:0.1080000028014183, confidence:0.9996183514595032, loss:9.85935115814209
epoch6: step0/4680
step 3000: accuracy:0.09399999678134918, confidence:0.9173300862312317, loss:5.93025016784668
epoch6: step500/4680
step 6000: accuracy:0.11599999666213989, confidence:0.967469334602356, loss:7.5352983474731445
epoch6: step1000/4680
step 9000: accuracy:0.11299999803304672, confidence:0.9809830188751221, loss:7.351498126983643
epoch6: step1500/4680
step 12000: accuracy:0.11500000208616257, confidence:0.9994972348213196, loss:9.723615646362305
epoch6: step2000/4680
step 15000: accuracy:0.0949999988079071, confidence:0.9882283210754395, loss:6.717072486877441
epoch6: step2500/4680
step 18000: accuracy:0.0949999988079071, confidence:0.7946401238441467, loss:4.769642353057861
epoch6: step3000/4680
step 21000: accuracy:0.1080000028014183, confidence:0.9713302850723267, loss:6.641390800476074
epoch6: step3500/4680
step 24000: accuracy:0.10199999809265137, confidence:0.8739400506019592, loss:5.301882743835449
epoch6: step4000/4680
step 27000: accuracy:0.10599999874830246, confidence:0.9995419979095459, loss:10.301194190979004
epoch6: step4500/4680
step 0: accuracy:0.10899999737739563, confidence:0.9994739294052124, loss:9.965824127197266
epoch7: step0/4680
step 3500: accuracy:0.1080000028014183, confidence:0.8695949912071228, loss:5.469244956970215
epoch7: step500/4680
step 7000: accuracy:0.11100000143051147, confidence:0.964348316192627, loss:7.959323406219482
epoch7: step1000/4680
step 10500: accuracy:0.09200000017881393, confidence:0.9865444898605347, loss:8.005050659179688
epoch7: step1500/4680
step 14000: accuracy:0.10999999940395355, confidence:0.9989770650863647, loss:9.345903396606445
epoch7: step2000/4680
step 17500: accuracy:0.08299999684095383, confidence:0.9749913215637207, loss:6.367328643798828
epoch7: step2500/4680
step 21000: accuracy:0.11100000143051147, confidence:0.7653409242630005, loss:4.73883056640625
epoch7: step3000/4680
step 24500: accuracy:0.11299999803304672, confidence:0.9339109659194946, loss:6.148420333862305
epoch7: step3500/4680
step 28000: accuracy:0.10400000214576721, confidence:0.8634832501411438, loss:5.014395236968994
epoch7: step4000/4680
step 31500: accuracy:0.0989999994635582, confidence:0.9988969564437866, loss:9.739095687866211
epoch7: step4500/4680
step 0: accuracy:0.09000000357627869, confidence:0.9990001320838928, loss:9.725357055664062
epoch8: step0/4680
step 4000: accuracy:0.10199999809265137, confidence:0.7522284388542175, loss:5.16421365737915
epoch8: step500/4680
step 8000: accuracy:0.10000000149011612, confidence:0.9678665995597839, loss:8.625454902648926
epoch8: step1000/4680
step 12000: accuracy:0.10000000149011612, confidence:0.9982009530067444, loss:9.809599876403809
epoch8: step1500/4680
step 16000: accuracy:0.09200000017881393, confidence:0.9984861016273499, loss:9.119937896728516
epoch8: step2000/4680
step 20000: accuracy:0.07900000363588333, confidence:0.9522429704666138, loss:5.89985990524292
epoch8: step2500/4680
step 24000: accuracy:0.10300000011920929, confidence:0.715255081653595, loss:4.416156768798828
epoch8: step3000/4680
step 28000: accuracy:0.08699999749660492, confidence:0.9389781355857849, loss:6.720961570739746
epoch8: step3500/4680
step 32000: accuracy:0.09600000083446503, confidence:0.8114400506019592, loss:4.676867485046387
epoch8: step4000/4680
step 36000: accuracy:0.09000000357627869, confidence:0.9990048408508301, loss:9.897829055786133
epoch8: step4500/4680
step 0: accuracy:0.0989999994635582, confidence:0.9988312721252441, loss:9.578068733215332
epoch9: step0/4680
step 4500: accuracy:0.08799999952316284, confidence:0.7755149006843567, loss:5.444928169250488
epoch9: step500/4680
step 9000: accuracy:0.1289999932050705, confidence:0.9491276741027832, loss:7.902451992034912
epoch9: step1000/4680
step 13500: accuracy:0.09399999678134918, confidence:0.9982861280441284, loss:10.403799057006836
epoch9: step1500/4680
step 18000: accuracy:0.11699999868869781, confidence:0.9993746876716614, loss:9.804717063903809
epoch9: step2000/4680
step 22500: accuracy:0.09099999815225601, confidence:0.9612012505531311, loss:6.283519268035889
epoch9: step2500/4680
step 27000: accuracy:0.11100000143051147, confidence:0.7649558186531067, loss:4.844505310058594
epoch9: step3000/4680
step 31500: accuracy:0.10599999874830246, confidence:0.9518361687660217, loss:6.97843074798584
epoch9: step3500/4680
step 36000: accuracy:0.11900000274181366, confidence:0.853059709072113, loss:5.097609519958496
epoch9: step4000/4680
step 40500: accuracy:0.1120000034570694, confidence:0.9825907349586487, loss:7.481797218322754
epoch9: step4500/4680
step 0: accuracy:0.10100000351667404, confidence:0.982043981552124, loss:7.533604621887207
epoch10: step0/4680
step 5000: accuracy:0.07199999690055847, confidence:0.8614634275436401, loss:8.607587814331055
epoch10: step500/4680
step 10000: accuracy:0.12399999797344208, confidence:0.9291223883628845, loss:6.377897262573242
epoch10: step1000/4680
step 15000: accuracy:0.11400000005960464, confidence:0.9988048672676086, loss:11.332708358764648
epoch10: step1500/4680
step 20000: accuracy:0.11500000208616257, confidence:0.9891349077224731, loss:7.845686912536621
epoch10: step2000/4680
step 25000: accuracy:0.09099999815225601, confidence:0.9024767875671387, loss:5.659328460693359
epoch10: step2500/4680
step 30000: accuracy:0.11800000071525574, confidence:0.7495870590209961, loss:4.808250904083252
epoch10: step3000/4680
step 35000: accuracy:0.11100000143051147, confidence:0.9450108408927917, loss:7.085694313049316
epoch10: step3500/4680
step 40000: accuracy:0.11500000208616257, confidence:0.7771956324577332, loss:4.869919300079346
epoch10: step4000/4680
step 45000: accuracy:0.09099999815225601, confidence:0.9941983222961426, loss:9.322361946105957
epoch10: step4500/4680
step 0: accuracy:0.11500000208616257, confidence:0.9909939765930176, loss:8.723503112792969
epoch11: step0/4680
step 5500: accuracy:0.10300000011920929, confidence:0.724448561668396, loss:5.203964710235596
epoch11: step500/4680
step 11000: accuracy:0.12999999523162842, confidence:0.8641297817230225, loss:8.609227180480957
epoch11: step1000/4680
step 16500: accuracy:0.10400000214576721, confidence:0.9905334711074829, loss:9.802743911743164
epoch11: step1500/4680
step 22000: accuracy:0.10400000214576721, confidence:0.9994222521781921, loss:10.839524269104004
epoch11: step2000/4680
step 27500: accuracy:0.0820000022649765, confidence:0.8842702507972717, loss:5.250462055206299
epoch11: step2500/4680
step 33000: accuracy:0.12700000405311584, confidence:0.7239876389503479, loss:4.446078777313232
epoch11: step3000/4680
step 38500: accuracy:0.08299999684095383, confidence:0.9398916959762573, loss:7.114509582519531
epoch11: step3500/4680
step 44000: accuracy:0.1080000028014183, confidence:0.7803647518157959, loss:4.915214538574219
epoch11: step4000/4680
step 49500: accuracy:0.10499999672174454, confidence:0.9892579317092896, loss:8.552393913269043
epoch11: step4500/4680
step 0: accuracy:0.10400000214576721, confidence:0.9849554300308228, loss:8.343449592590332
epoch12: step0/4680
step 6000: accuracy:0.10700000077486038, confidence:0.7291266918182373, loss:5.1642374992370605
epoch12: step500/4680
step 12000: accuracy:0.13099999725818634, confidence:0.9175642728805542, loss:7.808602333068848
epoch12: step1000/4680
step 18000: accuracy:0.09600000083446503, confidence:0.9938572645187378, loss:10.668968200683594
epoch12: step1500/4680
step 24000: accuracy:0.09099999815225601, confidence:0.9990278482437134, loss:10.27558422088623
epoch12: step2000/4680
step 30000: accuracy:0.0989999994635582, confidence:0.9525710344314575, loss:6.398754596710205
epoch12: step2500/4680
step 36000: accuracy:0.12099999934434891, confidence:0.7570827007293701, loss:4.819192409515381
epoch12: step3000/4680
step 42000: accuracy:0.08900000154972076, confidence:0.9448558688163757, loss:7.570512294769287
epoch12: step3500/4680
step 48000: accuracy:0.125, confidence:0.8016071915626526, loss:5.155287265777588
epoch12: step4000/4680
step 54000: accuracy:0.10300000011920929, confidence:0.9688857793807983, loss:7.635598182678223
epoch12: step4500/4680
step 0: accuracy:0.09600000083446503, confidence:0.970695972442627, loss:7.8573503494262695
epoch13: step0/4680
step 6500: accuracy:0.09799999743700027, confidence:0.7614406943321228, loss:5.878633975982666
epoch13: step500/4680
step 13000: accuracy:0.13500000536441803, confidence:0.9091415405273438, loss:8.329837799072266
epoch13: step1000/4680
step 19500: accuracy:0.09300000220537186, confidence:0.9952287673950195, loss:11.651372909545898
epoch13: step1500/4680
step 26000: accuracy:0.0989999994635582, confidence:0.9985230565071106, loss:9.742560386657715
epoch13: step2000/4680
step 32500: accuracy:0.0989999994635582, confidence:0.9348763823509216, loss:6.317729949951172
epoch13: step2500/4680
step 39000: accuracy:0.1550000011920929, confidence:0.7545484900474548, loss:4.623204708099365
epoch13: step3000/4680
step 45500: accuracy:0.09099999815225601, confidence:0.9368022680282593, loss:7.80308198928833
epoch13: step3500/4680
step 52000: accuracy:0.12399999797344208, confidence:0.8284792304039001, loss:5.1765055656433105
epoch13: step4000/4680
step 58500: accuracy:0.10100000351667404, confidence:0.9894910454750061, loss:9.08907699584961
epoch13: step4500/4680
step 0: accuracy:0.10899999737739563, confidence:0.9852203726768494, loss:8.771206855773926
epoch14: step0/4680
step 7000: accuracy:0.12700000405311584, confidence:0.7215579748153687, loss:4.843847274780273
epoch14: step500/4680
step 14000: accuracy:0.13300000131130219, confidence:0.8819971084594727, loss:9.4376220703125
epoch14: step1000/4680
step 21000: accuracy:0.09399999678134918, confidence:0.9948218464851379, loss:11.32411003112793
epoch14: step1500/4680
step 28000: accuracy:0.11699999868869781, confidence:0.9995418787002563, loss:10.891839981079102
epoch14: step2000/4680
step 35000: accuracy:0.10599999874830246, confidence:0.9248293042182922, loss:5.80678129196167
epoch14: step2500/4680
step 42000: accuracy:0.12999999523162842, confidence:0.728518009185791, loss:4.478029251098633
epoch14: step3000/4680
step 49000: accuracy:0.11599999666213989, confidence:0.9512020349502563, loss:7.735095500946045
epoch14: step3500/4680
step 56000: accuracy:0.11500000208616257, confidence:0.7929850816726685, loss:5.155036449432373
epoch14: step4000/4680
step 63000: accuracy:0.09600000083446503, confidence:0.9754037857055664, loss:8.747485160827637
epoch14: step4500/4680
step 0: accuracy:0.1120000034570694, confidence:0.9763838052749634, loss:8.062065124511719
epoch15: step0/4680
step 7500: accuracy:0.10300000011920929, confidence:0.7654865980148315, loss:5.661940097808838
epoch15: step500/4680
step 15000: accuracy:0.12600000202655792, confidence:0.92206209897995, loss:8.00314712524414
epoch15: step1000/4680
step 22500: accuracy:0.09700000286102295, confidence:0.9996939897537231, loss:14.7732572555542
epoch15: step1500/4680
step 30000: accuracy:0.09000000357627869, confidence:0.9992499947547913, loss:11.218634605407715
epoch15: step2000/4680
step 37500: accuracy:0.09799999743700027, confidence:0.9606512188911438, loss:6.559048175811768
epoch15: step2500/4680
step 45000: accuracy:0.10700000077486038, confidence:0.7333592176437378, loss:4.57794713973999
epoch15: step3000/4680
step 52500: accuracy:0.10700000077486038, confidence:0.9542903900146484, loss:7.900286674499512
epoch15: step3500/4680
step 60000: accuracy:0.10999999940395355, confidence:0.8253069519996643, loss:5.547370910644531
epoch15: step4000/4680
step 67500: accuracy:0.10499999672174454, confidence:0.9681124687194824, loss:8.569014549255371
epoch15: step4500/4680
step 0: accuracy:0.11800000071525574, confidence:0.9750787019729614, loss:8.402880668640137
epoch16: step0/4680
step 8000: accuracy:0.0989999994635582, confidence:0.8187658786773682, loss:6.810886383056641
epoch16: step500/4680
step 16000: accuracy:0.11699999868869781, confidence:0.9458644390106201, loss:9.252761840820312
epoch16: step1000/4680
step 24000: accuracy:0.0989999994635582, confidence:0.9884156584739685, loss:10.928712844848633
epoch16: step1500/4680
step 32000: accuracy:0.08799999952316284, confidence:0.9842481017112732, loss:9.040082931518555
epoch16: step2000/4680
step 40000: accuracy:0.10899999737739563, confidence:0.9386709928512573, loss:6.3626227378845215
epoch16: step2500/4680
step 48000: accuracy:0.10999999940395355, confidence:0.7052473425865173, loss:4.534763336181641
epoch16: step3000/4680
step 56000: accuracy:0.10100000351667404, confidence:0.9536670446395874, loss:8.049198150634766
epoch16: step3500/4680
step 64000: accuracy:0.12300000339746475, confidence:0.7998777031898499, loss:5.240438461303711
epoch16: step4000/4680
step 72000: accuracy:0.10000000149011612, confidence:0.9807331562042236, loss:9.101729393005371
epoch16: step4500/4680
step 0: accuracy:0.08399999886751175, confidence:0.9765611290931702, loss:9.0515718460083
epoch17: step0/4680
step 8500: accuracy:0.12800000607967377, confidence:0.7185700535774231, loss:4.882794380187988
epoch17: step500/4680
step 17000: accuracy:0.12600000202655792, confidence:0.8744603991508484, loss:8.799332618713379
epoch17: step1000/4680
step 25500: accuracy:0.10000000149011612, confidence:0.9981510043144226, loss:13.377763748168945
epoch17: step1500/4680
step 34000: accuracy:0.09399999678134918, confidence:0.9996692538261414, loss:12.429266929626465
epoch17: step2000/4680
step 42500: accuracy:0.08500000089406967, confidence:0.8927461504936218, loss:5.6232218742370605
epoch17: step2500/4680
step 51000: accuracy:0.10599999874830246, confidence:0.6851435899734497, loss:4.273720741271973
epoch17: step3000/4680
step 59500: accuracy:0.11599999666213989, confidence:0.9694373607635498, loss:8.580718040466309
epoch17: step3500/4680
step 68000: accuracy:0.1509999930858612, confidence:0.8129509687423706, loss:5.168361663818359
epoch17: step4000/4680
step 76500: accuracy:0.10199999809265137, confidence:0.9895419478416443, loss:10.044049263000488
epoch17: step4500/4680
step 0: accuracy:0.12099999934434891, confidence:0.9798529744148254, loss:9.101529121398926
epoch18: step0/4680
step 9000: accuracy:0.12200000137090683, confidence:0.7139618992805481, loss:5.022285461425781
epoch18: step500/4680
step 18000: accuracy:0.10199999809265137, confidence:0.8463355898857117, loss:7.313211917877197
epoch18: step1000/4680
step 27000: accuracy:0.1120000034570694, confidence:0.9876701831817627, loss:11.038322448730469
epoch18: step1500/4680
step 36000: accuracy:0.11100000143051147, confidence:0.9984306693077087, loss:11.232013702392578
epoch18: step2000/4680
step 45000: accuracy:0.1120000034570694, confidence:0.9196867346763611, loss:6.1056928634643555
epoch18: step2500/4680
step 54000: accuracy:0.11500000208616257, confidence:0.7515813708305359, loss:4.744326591491699
epoch18: step3000/4680
step 63000: accuracy:0.10400000214576721, confidence:0.9271048307418823, loss:7.5974578857421875
epoch18: step3500/4680
step 72000: accuracy:0.13899999856948853, confidence:0.8083988428115845, loss:5.361732006072998
epoch18: step4000/4680
step 81000: accuracy:0.11299999803304672, confidence:0.97588050365448, loss:8.988807678222656
epoch18: step4500/4680
step 0: accuracy:0.1120000034570694, confidence:0.9700515866279602, loss:8.544249534606934
epoch19: step0/4680
step 9500: accuracy:0.10899999737739563, confidence:0.8918471336364746, loss:7.8489484786987305
epoch19: step500/4680
step 19000: accuracy:0.13099999725818634, confidence:0.9367249608039856, loss:7.0324015617370605
epoch19: step1000/4680
step 28500: accuracy:0.0989999994635582, confidence:0.9774341583251953, loss:10.533161163330078
epoch19: step1500/4680
step 38000: accuracy:0.10000000149011612, confidence:0.9635097980499268, loss:8.990411758422852
epoch19: step2000/4680
step 47500: accuracy:0.11100000143051147, confidence:0.9115706086158752, loss:5.886364459991455
epoch19: step2500/4680
step 57000: accuracy:0.12999999523162842, confidence:0.7123077511787415, loss:4.584897518157959
epoch19: step3000/4680
step 66500: accuracy:0.09099999815225601, confidence:0.9524298310279846, loss:8.500855445861816
epoch19: step3500/4680
step 76000: accuracy:0.12600000202655792, confidence:0.7975770235061646, loss:5.349459171295166
epoch19: step4000/4680
step 85500: accuracy:0.10400000214576721, confidence:0.988000750541687, loss:9.952610969543457
epoch19: step4500/4680
step 0: accuracy:0.10999999940395355, confidence:0.9855222105979919, loss:9.47911548614502
epoch20: step0/4680
step 10000: accuracy:0.11699999868869781, confidence:0.737925112247467, loss:5.5044026374816895
epoch20: step500/4680
step 20000: accuracy:0.10199999809265137, confidence:0.8654630780220032, loss:10.231033325195312
epoch20: step1000/4680
step 30000: accuracy:0.08500000089406967, confidence:0.9995598793029785, loss:14.334857940673828
epoch20: step1500/4680
step 40000: accuracy:0.07000000029802322, confidence:0.9984694719314575, loss:11.20112419128418
epoch20: step2000/4680
step 50000: accuracy:0.07800000160932541, confidence:0.889471173286438, loss:5.926913261413574
epoch20: step2500/4680
step 60000: accuracy:0.10599999874830246, confidence:0.7019004225730896, loss:4.4252238273620605
epoch20: step3000/4680
step 70000: accuracy:0.10700000077486038, confidence:0.9294512271881104, loss:7.633072853088379
epoch20: step3500/4680
step 80000: accuracy:0.1509999930858612, confidence:0.7506438493728638, loss:4.907435417175293
epoch20: step4000/4680
step 90000: accuracy:0.0989999994635582, confidence:0.9636141061782837, loss:8.92197322845459
epoch20: step4500/4680
step 0: accuracy:0.10199999809265137, confidence:0.9646213054656982, loss:8.702832221984863
epoch21: step0/4680
step 10500: accuracy:0.10100000351667404, confidence:0.7825318574905396, loss:5.765382766723633
epoch21: step500/4680
step 21000: accuracy:0.11800000071525574, confidence:0.9122352004051208, loss:8.429157257080078
epoch21: step1000/4680
step 31500: accuracy:0.09200000017881393, confidence:0.9952144026756287, loss:14.280519485473633
epoch21: step1500/4680
step 42000: accuracy:0.10899999737739563, confidence:0.9986236691474915, loss:11.81937026977539
epoch21: step2000/4680
step 52500: accuracy:0.09799999743700027, confidence:0.9391489028930664, loss:6.643298625946045
epoch21: step2500/4680
step 63000: accuracy:0.12600000202655792, confidence:0.7206749320030212, loss:4.501584053039551
epoch21: step3000/4680
step 73500: accuracy:0.10000000149011612, confidence:0.9408987760543823, loss:8.037277221679688
epoch21: step3500/4680
step 84000: accuracy:0.11999999731779099, confidence:0.7760502099990845, loss:5.213343143463135
epoch21: step4000/4680
step 94500: accuracy:0.09799999743700027, confidence:0.9754983186721802, loss:9.38973617553711
epoch21: step4500/4680
step 0: accuracy:0.11699999868869781, confidence:0.9720935821533203, loss:8.758691787719727
epoch22: step0/4680
step 11000: accuracy:0.10499999672174454, confidence:0.8540412187576294, loss:6.842357635498047
epoch22: step500/4680
step 22000: accuracy:0.11400000005960464, confidence:0.9353230595588684, loss:8.824262619018555
epoch22: step1000/4680
step 33000: accuracy:0.09300000220537186, confidence:0.9981669783592224, loss:13.075543403625488
epoch22: step1500/4680
step 44000: accuracy:0.1120000034570694, confidence:0.9922243356704712, loss:10.72727108001709
epoch22: step2000/4680
step 55000: accuracy:0.09600000083446503, confidence:0.9264510273933411, loss:6.824913024902344
epoch22: step2500/4680
step 66000: accuracy:0.11999999731779099, confidence:0.7235859036445618, loss:4.682616710662842
epoch22: step3000/4680
step 77000: accuracy:0.10599999874830246, confidence:0.9519664645195007, loss:8.265281677246094
epoch22: step3500/4680
step 88000: accuracy:0.12999999523162842, confidence:0.7554765939712524, loss:5.1410956382751465
epoch22: step4000/4680
step 99000: accuracy:0.11100000143051147, confidence:0.9542709589004517, loss:8.36117172241211
epoch22: step4500/4680
step 0: accuracy:0.11699999868869781, confidence:0.9514228701591492, loss:8.097918510437012
epoch23: step0/4680
step 11500: accuracy:0.10999999940395355, confidence:0.8187720775604248, loss:6.796063423156738
epoch23: step500/4680
step 23000: accuracy:0.14000000059604645, confidence:0.9196778535842896, loss:9.020739555358887
epoch23: step1000/4680
step 34500: accuracy:0.08900000154972076, confidence:0.9990197420120239, loss:15.398853302001953
epoch23: step1500/4680
step 46000: accuracy:0.10499999672174454, confidence:0.9965886473655701, loss:11.153824806213379
epoch23: step2000/4680
step 57500: accuracy:0.0820000022649765, confidence:0.8609790205955505, loss:5.800795078277588
epoch23: step2500/4680
step 69000: accuracy:0.11999999731779099, confidence:0.7031136751174927, loss:4.473556041717529
epoch23: step3000/4680
step 80500: accuracy:0.10300000011920929, confidence:0.9331440925598145, loss:7.809215068817139
epoch23: step3500/4680
step 92000: accuracy:0.14800000190734863, confidence:0.7458560466766357, loss:4.975278377532959
epoch23: step4000/4680
step 103500: accuracy:0.10700000077486038, confidence:0.9494211673736572, loss:8.344423294067383
epoch23: step4500/4680
step 0: accuracy:0.10000000149011612, confidence:0.9494178295135498, loss:8.296070098876953
epoch24: step0/4680
step 12000: accuracy:0.09200000017881393, confidence:0.8149498701095581, loss:6.677638530731201
epoch24: step500/4680
step 24000: accuracy:0.12999999523162842, confidence:0.929786741733551, loss:9.089639663696289
epoch24: step1000/4680
step 36000: accuracy:0.10999999940395355, confidence:0.9821677803993225, loss:10.824763298034668
epoch24: step1500/4680
step 48000: accuracy:0.10400000214576721, confidence:0.9977442622184753, loss:12.328874588012695
epoch24: step2000/4680
step 60000: accuracy:0.07800000160932541, confidence:0.9562041163444519, loss:7.354114532470703
epoch24: step2500/4680
step 72000: accuracy:0.1379999965429306, confidence:0.7279311418533325, loss:4.673070907592773
epoch24: step3000/4680
step 84000: accuracy:0.09799999743700027, confidence:0.9321539402008057, loss:7.774916648864746
epoch24: step3500/4680
step 96000: accuracy:0.1459999978542328, confidence:0.7793146967887878, loss:5.107762813568115
epoch24: step4000/4680
step 108000: accuracy:0.1080000028014183, confidence:0.9486222863197327, loss:8.191838264465332
epoch24: step4500/4680
step 0: accuracy:0.11299999803304672, confidence:0.9467026591300964, loss:8.064478874206543
epoch25: step0/4680
step 12500: accuracy:0.12600000202655792, confidence:0.8190693855285645, loss:6.28774356842041
epoch25: step500/4680
step 25000: accuracy:0.08100000023841858, confidence:0.8868574500083923, loss:8.933937072753906
epoch25: step1000/4680
step 37500: accuracy:0.10599999874830246, confidence:0.9991006851196289, loss:14.448726654052734
epoch25: step1500/4680
step 50000: accuracy:0.10000000149011612, confidence:0.9974547028541565, loss:11.71888256072998
epoch25: step2000/4680
step 62500: accuracy:0.08699999749660492, confidence:0.8946648836135864, loss:6.582722187042236
epoch25: step2500/4680
step 75000: accuracy:0.12600000202655792, confidence:0.7503259778022766, loss:4.999848365783691
epoch25: step3000/4680
step 87500: accuracy:0.09600000083446503, confidence:0.9344275593757629, loss:8.31421947479248
epoch25: step3500/4680
step 100000: accuracy:0.14900000393390656, confidence:0.7478057146072388, loss:5.106691360473633
epoch25: step4000/4680
step 112500: accuracy:0.11100000143051147, confidence:0.9586215615272522, loss:8.96362590789795
epoch25: step4500/4680
step 0: accuracy:0.10999999940395355, confidence:0.956550657749176, loss:8.674939155578613
epoch26: step0/4680
step 13000: accuracy:0.14900000393390656, confidence:0.7541996836662292, loss:5.288157939910889
epoch26: step500/4680
step 26000: accuracy:0.08900000154972076, confidence:0.8664990067481995, loss:8.914655685424805
epoch26: step1000/4680
step 39000: accuracy:0.10400000214576721, confidence:0.9844793677330017, loss:12.203763961791992
epoch26: step1500/4680
step 52000: accuracy:0.10300000011920929, confidence:0.9993977546691895, loss:13.961665153503418
epoch26: step2000/4680
step 65000: accuracy:0.08900000154972076, confidence:0.9014589786529541, loss:6.604724884033203
epoch26: step2500/4680
step 78000: accuracy:0.10899999737739563, confidence:0.7601900696754456, loss:4.750633239746094
epoch26: step3000/4680
step 91000: accuracy:0.08900000154972076, confidence:0.9368687868118286, loss:8.380001068115234
epoch26: step3500/4680
step 104000: accuracy:0.1289999932050705, confidence:0.7890872955322266, loss:5.247649192810059
epoch26: step4000/4680
step 117000: accuracy:0.10300000011920929, confidence:0.9679297208786011, loss:8.968145370483398
epoch26: step4500/4680
step 0: accuracy:0.13199999928474426, confidence:0.9672821164131165, loss:8.389691352844238
epoch27: step0/4680
step 13500: accuracy:0.12099999934434891, confidence:0.8346841335296631, loss:6.212088108062744
epoch27: step500/4680
step 27000: accuracy:0.08299999684095383, confidence:0.8765206336975098, loss:8.191577911376953
epoch27: step1000/4680
step 40500: accuracy:0.0989999994635582, confidence:0.993872880935669, loss:11.87191390991211
epoch27: step1500/4680
step 54000: accuracy:0.11599999666213989, confidence:0.9997114539146423, loss:14.507770538330078
epoch27: step2000/4680
step 67500: accuracy:0.09300000220537186, confidence:0.9420990347862244, loss:7.1189494132995605
epoch27: step2500/4680
step 81000: accuracy:0.13500000536441803, confidence:0.7643882036209106, loss:4.874035358428955
epoch27: step3000/4680
step 94500: accuracy:0.10599999874830246, confidence:0.9386722445487976, loss:8.279858589172363
epoch27: step3500/4680
step 108000: accuracy:0.1860000044107437, confidence:0.7392946481704712, loss:4.638789176940918
epoch27: step4000/4680
step 121500: accuracy:0.11500000208616257, confidence:0.94132000207901, loss:8.104536056518555
epoch27: step4500/4680
step 0: accuracy:0.12399999797344208, confidence:0.9356350302696228, loss:7.919393062591553
epoch28: step0/4680
step 14000: accuracy:0.11699999868869781, confidence:0.8298177123069763, loss:6.706201553344727
epoch28: step500/4680
step 28000: accuracy:0.11299999803304672, confidence:0.9402852654457092, loss:10.248820304870605
epoch28: step1000/4680
step 42000: accuracy:0.09399999678134918, confidence:0.9889166951179504, loss:12.216886520385742
epoch28: step1500/4680
step 56000: accuracy:0.0860000029206276, confidence:0.9983755350112915, loss:13.39147663116455
epoch28: step2000/4680
step 70000: accuracy:0.10100000351667404, confidence:0.8679725527763367, loss:6.092703342437744
epoch28: step2500/4680
step 84000: accuracy:0.1340000033378601, confidence:0.7202349901199341, loss:4.616027355194092
epoch28: step3000/4680
step 98000: accuracy:0.09000000357627869, confidence:0.9346559643745422, loss:8.637969017028809
epoch28: step3500/4680
step 112000: accuracy:0.16099999845027924, confidence:0.7411253452301025, loss:4.92953634262085
epoch28: step4000/4680
step 126000: accuracy:0.1120000034570694, confidence:0.9425873160362244, loss:8.541077613830566
epoch28: step4500/4680
step 0: accuracy:0.11500000208616257, confidence:0.9303160309791565, loss:8.109416961669922
epoch29: step0/4680
step 14500: accuracy:0.12800000607967377, confidence:0.8146811723709106, loss:6.35896110534668
epoch29: step500/4680
step 29000: accuracy:0.1340000033378601, confidence:0.9492325782775879, loss:10.791291236877441
epoch29: step1000/4680
step 43500: accuracy:0.10000000149011612, confidence:0.9806149005889893, loss:10.092042922973633
epoch29: step1500/4680
step 58000: accuracy:0.12200000137090683, confidence:0.9878308176994324, loss:10.644125938415527
epoch29: step2000/4680
step 72500: accuracy:0.09399999678134918, confidence:0.8640917539596558, loss:6.202034950256348
epoch29: step2500/4680
step 87000: accuracy:0.11500000208616257, confidence:0.7510004639625549, loss:5.043104648590088
epoch29: step3000/4680
step 101500: accuracy:0.09600000083446503, confidence:0.9241139888763428, loss:8.279845237731934
epoch29: step3500/4680
step 116000: accuracy:0.1469999998807907, confidence:0.7373732328414917, loss:4.951197147369385
epoch29: step4000/4680
step 130500: accuracy:0.11999999731779099, confidence:0.9306155443191528, loss:8.182709693908691
epoch29: step4500/4680
