2018-06-15 19:39:50.284394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000a:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:39:50.284581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
2018-06-15 19:39:52.004012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
CHEKPOINT DIR: multi-gaussian/mu_0.1_sigma0.3
model has been loaded from fashion-mnist_classifier.pkl
---------
Variables: name (type shape) [size]
---------
Variable:0 (float32_ref 5x5x1x32) [800, bytes: 3200]
Variable_1:0 (float32_ref 32) [32, bytes: 128]
Variable_2:0 (float32_ref 5x5x32x64) [51200, bytes: 204800]
Variable_3:0 (float32_ref 64) [64, bytes: 256]
Variable_4:0 (float32_ref 3136x1024) [3211264, bytes: 12845056]
Variable_5:0 (float32_ref 1024) [1024, bytes: 4096]
Variable_6:0 (float32_ref 1024x10) [10240, bytes: 40960]
Variable_7:0 (float32_ref 10) [10, bytes: 40]
discriminator/d_conv1/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]
discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]
discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]
discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]
generator/g_fc1/Matrix:0 (float32_ref 74x1024) [75776, bytes: 303104]
generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]
generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]
generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]
classifier/c_fc1/Matrix:0 (float32_ref 1024x64) [65536, bytes: 262144]
classifier/c_fc1/bias:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/beta:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/gamma:0 (float32_ref 64) [64, bytes: 256]
classifier/c_fc2/Matrix:0 (float32_ref 64x12) [768, bytes: 3072]
classifier/c_fc2/bias:0 (float32_ref 12) [12, bytes: 48]
Total size of variables: 16552792
Total bytes of variables: 66211168
 [*] Reading checkpoints...
 [*] Success to read MultiModalInfoGAN.model-43721
 [*] Load SUCCESS


SAMPLES SIZE=4680,LABELS=299520


SAVED TRAINING SET generated_training_set_fashion-mnist_MultivariateGaussianSampler_mu_0.1_sigma_0.3_czcc_czrc_rzcc_rzrc
 [*] Training finished!
 [*] Testing finished!
2018-06-15 19:40:45.546280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000a:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:40:45.587412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
2018-06-15 19:40:47.256229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
CHEKPOINT DIR: multi-gaussian/mu_0.1_sigma0.3
model has been loaded from fashion-mnist_classifier.pkl
---------
Variables: name (type shape) [size]
---------
Variable:0 (float32_ref 5x5x1x32) [800, bytes: 3200]
Variable_1:0 (float32_ref 32) [32, bytes: 128]
Variable_2:0 (float32_ref 5x5x32x64) [51200, bytes: 204800]
Variable_3:0 (float32_ref 64) [64, bytes: 256]
Variable_4:0 (float32_ref 3136x1024) [3211264, bytes: 12845056]
Variable_5:0 (float32_ref 1024) [1024, bytes: 4096]
Variable_6:0 (float32_ref 1024x10) [10240, bytes: 40960]
Variable_7:0 (float32_ref 10) [10, bytes: 40]
discriminator/d_conv1/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]
discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]
discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]
discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]
generator/g_fc1/Matrix:0 (float32_ref 74x1024) [75776, bytes: 303104]
generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]
generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]
generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]
classifier/c_fc1/Matrix:0 (float32_ref 1024x64) [65536, bytes: 262144]
classifier/c_fc1/bias:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/beta:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/gamma:0 (float32_ref 64) [64, bytes: 256]
classifier/c_fc2/Matrix:0 (float32_ref 64x12) [768, bytes: 3072]
classifier/c_fc2/bias:0 (float32_ref 12) [12, bytes: 48]
Total size of variables: 16552792
Total bytes of variables: 66211168
 [*] Reading checkpoints...
 [*] Success to read MultiModalInfoGAN.model-43721
 [*] Load SUCCESS


SAMPLES SIZE=4680,LABELS=299520


SAVED TRAINING SET generated_training_set_fashion-mnist_MultivariateGaussianSampler_mu_0.1_sigma_0.3_czcc_rzcc_czrc_rzrc
 [*] Training finished!
 [*] Testing finished!
2018-06-15 19:41:34.215088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000a:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:41:34.215140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
2018-06-15 19:41:35.646164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
CHEKPOINT DIR: multi-gaussian/mu_0.1_sigma0.3
model has been loaded from fashion-mnist_classifier.pkl
---------
Variables: name (type shape) [size]
---------
Variable:0 (float32_ref 5x5x1x32) [800, bytes: 3200]
Variable_1:0 (float32_ref 32) [32, bytes: 128]
Variable_2:0 (float32_ref 5x5x32x64) [51200, bytes: 204800]
Variable_3:0 (float32_ref 64) [64, bytes: 256]
Variable_4:0 (float32_ref 3136x1024) [3211264, bytes: 12845056]
Variable_5:0 (float32_ref 1024) [1024, bytes: 4096]
Variable_6:0 (float32_ref 1024x10) [10240, bytes: 40960]
Variable_7:0 (float32_ref 10) [10, bytes: 40]
discriminator/d_conv1/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]
discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]
discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]
discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]
generator/g_fc1/Matrix:0 (float32_ref 74x1024) [75776, bytes: 303104]
generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]
generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]
generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]
classifier/c_fc1/Matrix:0 (float32_ref 1024x64) [65536, bytes: 262144]
classifier/c_fc1/bias:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/beta:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/gamma:0 (float32_ref 64) [64, bytes: 256]
classifier/c_fc2/Matrix:0 (float32_ref 64x12) [768, bytes: 3072]
classifier/c_fc2/bias:0 (float32_ref 12) [12, bytes: 48]
Total size of variables: 16552792
Total bytes of variables: 66211168
 [*] Reading checkpoints...
 [*] Success to read MultiModalInfoGAN.model-43721
 [*] Load SUCCESS


SAMPLES SIZE=4680,LABELS=299520


SAVED TRAINING SET generated_training_set_fashion-mnist_MultivariateGaussianSampler_mu_0.1_sigma_0.3_rzcc_rzrc_czcc_czrc
 [*] Training finished!
 [*] Testing finished!
2018-06-15 19:42:23.536852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000a:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:42:23.537032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
2018-06-15 19:42:24.981877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
CHEKPOINT DIR: multi-gaussian/mu_0.1_sigma0.3
model has been loaded from fashion-mnist_classifier.pkl
---------
Variables: name (type shape) [size]
---------
Variable:0 (float32_ref 5x5x1x32) [800, bytes: 3200]
Variable_1:0 (float32_ref 32) [32, bytes: 128]
Variable_2:0 (float32_ref 5x5x32x64) [51200, bytes: 204800]
Variable_3:0 (float32_ref 64) [64, bytes: 256]
Variable_4:0 (float32_ref 3136x1024) [3211264, bytes: 12845056]
Variable_5:0 (float32_ref 1024) [1024, bytes: 4096]
Variable_6:0 (float32_ref 1024x10) [10240, bytes: 40960]
Variable_7:0 (float32_ref 10) [10, bytes: 40]
discriminator/d_conv1/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]
discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]
discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]
discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]
generator/g_fc1/Matrix:0 (float32_ref 74x1024) [75776, bytes: 303104]
generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]
generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]
generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]
classifier/c_fc1/Matrix:0 (float32_ref 1024x64) [65536, bytes: 262144]
classifier/c_fc1/bias:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/beta:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/gamma:0 (float32_ref 64) [64, bytes: 256]
classifier/c_fc2/Matrix:0 (float32_ref 64x12) [768, bytes: 3072]
classifier/c_fc2/bias:0 (float32_ref 12) [12, bytes: 48]
Total size of variables: 16552792
Total bytes of variables: 66211168
 [*] Reading checkpoints...
 [*] Success to read MultiModalInfoGAN.model-43721
 [*] Load SUCCESS


SAMPLES SIZE=4680,LABELS=299520


SAVED TRAINING SET generated_training_set_fashion-mnist_MultivariateGaussianSampler_mu_0.1_sigma_0.3_czrc_czcc_rzcc_rzrc
 [*] Training finished!
 [*] Testing finished!
2018-06-15 19:43:11.775625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000a:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:43:11.775812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
model has been loaded from fashion-mnist_classifier.pkl
argmax:[1 1 1 ..., 1 1 1]
step 0: accuracy:0.0, confidence:0.999897837638855, loss:15.28077507019043
Assinging:2
[    0 10000]
argmax:[7 7 7 ..., 7 7 7]
step 0: accuracy:0.0, confidence:0.9864776134490967, loss:9.444979667663574
Assinging:8
[   0    0    0    0    0 1014    0 8986]
argmax:[6 6 6 ..., 0 0 0]
step 0: accuracy:0.0, confidence:0.8829773664474487, loss:8.891048431396484
Assinging:7
[1342  234    0  624    0  117 7683]
argmax:[9 9 9 ..., 9 7 7]
step 0: accuracy:0.8671000003814697, confidence:0.9430063366889954, loss:1.084765911102295
Assinging:10
[   0    0    0    0    0 1092    0  237    0 8671]
argmax:[8 8 8 ..., 8 8 8]
step 0: accuracy:0.0, confidence:0.9806717038154602, loss:11.119180679321289
Assinging:9
[ 273    0   39    0    0    0   78    0 9610]
argmax:[7 7 7 ..., 7 7 7]
step 0: accuracy:0.08290000259876251, confidence:0.8628005981445312, loss:2.5400335788726807
Assinging:8
[   0    0    0    0    0  819    0 8274   78  829]
argmax:[6 6 6 ..., 2 2 2]
step 0: accuracy:0.0, confidence:0.9600427746772766, loss:12.709464073181152
Assinging:7
[   0    0 1147    0  507    0 8346]
argmax:[6 6 6 ..., 4 4 4]
step 0: accuracy:0.0, confidence:0.9765031337738037, loss:17.493059158325195
Assinging:7
[   0    0    0    0 1221    0 8580    0  199]
argmax:[0 0 0 ..., 0 0 0]
step 0: accuracy:0.0, confidence:0.950979471206665, loss:11.9262113571167
Assinging:1
[9253  117    0  351    0    0  279]
argmax:[6 6 6 ..., 6 6 6]
step 0: accuracy:0.0, confidence:0.9581445455551147, loss:16.62432289123535
Assinging:7
[   0    0    0    0  819   39 9142]
2018-06-15 19:43:28.783720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000a:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:43:28.783917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
model has been loaded from fashion-mnist_classifier.pkl
argmax:[4 4 4 ..., 4 4 4]
step 0: accuracy:0.0, confidence:0.9977971911430359, loss:18.35471534729004
Assinging:5
[   0    0    6    0 9994]
argmax:[7 7 7 ..., 7 7 7]
step 0: accuracy:0.0, confidence:0.9993608593940735, loss:12.057779312133789
Assinging:8
[    0     0     0     0     0     0     0 10000]
argmax:[0 0 0 ..., 0 0 0]
step 0: accuracy:0.0, confidence:0.9864873290061951, loss:13.75218391418457
Assinging:1
[9923    0    0    0    0    0   77]
argmax:[8 8 8 ..., 8 8 8]
step 0: accuracy:0.0, confidence:0.8212214112281799, loss:7.991454601287842
Assinging:9
[   0    0    0    0    0    0  127    0 9873]
argmax:[8 8 8 ..., 8 8 8]
step 0: accuracy:0.0, confidence:0.9914863109588623, loss:11.657610893249512
Assinging:9
[    0     0     0     0     0     0     0     0 10000]
argmax:[9 9 9 ..., 9 9 9]
step 0: accuracy:1.0, confidence:0.9999857544898987, loss:1.4256258509703912e-05
Assinging:10
[    0     0     0     0     0     0     0     0     0 10000]
argmax:[1 1 1 ..., 1 1 1]
step 0: accuracy:0.0, confidence:0.9955205917358398, loss:14.231959342956543
Assinging:2
[   0 9998    0    0    2]
argmax:[3 3 3 ..., 3 3 3]
step 0: accuracy:0.0, confidence:0.9888784289360046, loss:10.511598587036133
Assinging:4
[   0    0    0 9942    0    0   58]
argmax:[8 8 8 ..., 8 8 9]
step 0: accuracy:0.018799999728798866, confidence:0.9350570440292358, loss:3.139451503753662
Assinging:9
[   0    0    0    0    0    0    0    0 9812  188]
argmax:[4 4 4 ..., 4 4 4]
step 0: accuracy:0.0, confidence:0.7810956835746765, loss:8.712372779846191
Assinging:5
[   0    0   29    0 9297    0  674]
2018-06-15 19:43:45.303757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000a:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:43:45.303939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
model has been loaded from fashion-mnist_classifier.pkl
argmax:[5 5 5 ..., 9 9 9]
step 0: accuracy:0.0357000008225441, confidence:0.9884958267211914, loss:10.936187744140625
Assinging:6
[   0    0  117    0    0 9479    0    0   47  357]
argmax:[4 2 4 ..., 4 4 2]
step 0: accuracy:0.0, confidence:0.8789455890655518, loss:14.363668441772461
Assinging:3
[  10    0 7275    0 1720    0  995]
argmax:[7 7 7 ..., 7 7 7]
step 0: accuracy:0.0, confidence:0.9759185314178467, loss:7.7003631591796875
Assinging:8
[   0    0    0    0    0 1207    0 8793]
argmax:[9 9 9 ..., 7 7 7]
step 0: accuracy:0.9283000230789185, confidence:0.9914513826370239, loss:0.28056055307388306
Assinging:10
[   0    0    0    0    0    0    0  717    0 9283]
argmax:[8 8 8 ..., 8 8 8]
step 0: accuracy:0.0, confidence:0.9777480363845825, loss:9.194150924682617
Assinging:9
[   0    0    0    0    0  181   13    0 9806]
argmax:[4 4 4 ..., 4 4 4]
step 0: accuracy:0.0, confidence:0.962287187576294, loss:13.436195373535156
Assinging:5
[   0    0  328  748 8832    0   92]
argmax:[2 2 2 ..., 4 4 4]
step 0: accuracy:0.0, confidence:0.8729100823402405, loss:12.525693893432617
Assinging:3
[   2    0 3810    0 3804    0 2383    0    1]
argmax:[3 3 3 ..., 3 3 3]
step 0: accuracy:0.0, confidence:0.9725704193115234, loss:14.20338249206543
Assinging:4
[  82    0   41 9551    0    0  326]
argmax:[1 1 1 ..., 1 1 1]
step 0: accuracy:0.0, confidence:0.9839283227920532, loss:18.197507858276367
Assinging:2
[ 206 9675    8  111]
argmax:[0 0 0 ..., 6 0 8]
step 0: accuracy:0.0, confidence:0.9759458899497986, loss:18.115047454833984
Assinging:1
[9274    0    0    0    0    0  592    0  134]
2018-06-15 19:44:00.693862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000a:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:44:00.694037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
model has been loaded from fashion-mnist_classifier.pkl
argmax:[9 9 9 ..., 9 9 9]
step 0: accuracy:0.7893999814987183, confidence:0.9946613311767578, loss:2.9247732162475586
Assinging:10
[   0    0    0    0    0 2106    0    0    0 7894]
argmax:[6 6 6 ..., 0 0 0]
step 0: accuracy:0.0, confidence:0.9576290845870972, loss:14.038328170776367
Assinging:1
[7777    0    0    0    0    0 2223]
argmax:[5 5 5 ..., 8 8 8]
step 0: accuracy:0.09359999746084213, confidence:0.9337403178215027, loss:11.45617389678955
Assinging:9
[   0    0    0    0    0 1287    0    0 7777  936]
argmax:[3 3 3 ..., 3 3 3]
step 0: accuracy:0.0, confidence:0.9434236288070679, loss:11.17377758026123
Assinging:4
[   0    0    0 6373 1404    0  351    0 1872]
argmax:[1 1 1 ..., 1 1 1]
step 0: accuracy:0.0, confidence:0.9919816255569458, loss:18.605253219604492
Assinging:2
[    0 10000]
argmax:[8 8 8 ..., 2 2 2]
step 0: accuracy:0.0, confidence:0.9772146344184875, loss:14.911778450012207
Assinging:3
[   0    0 7309    0  234    0  234    0 2223]
argmax:[6 6 0 ..., 6 6 6]
step 0: accuracy:0.0, confidence:0.8764494061470032, loss:16.019250869750977
Assinging:7
[1287    0 1989    0  234    0 6490]
argmax:[6 6 6 ..., 6 6 6]
step 0: accuracy:0.0, confidence:0.7669894695281982, loss:10.658970832824707
Assinging:7
[ 117    0  234    0 4446    0 4969    0  234]
argmax:[9 9 9 ..., 7 7 7]
step 0: accuracy:0.21060000360012054, confidence:0.9335705041885376, loss:9.493553161621094
Assinging:8
[   0    0    0    0    0    0    0 7894    0 2106]
argmax:[5 5 5 ..., 5 5 5]
step 0: accuracy:0.0, confidence:0.82163006067276, loss:12.287144660949707
Assinging:6
[   0    0  117 1989  936 6256  234  468]
2018-06-15 19:44:18.406199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000a:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:44:18.406377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
step 0: accuracy:0.11400000005960464, confidence:0.7916774749755859, loss:6.318097114562988
epoch0: step0/4680
step 0: accuracy:0.11800000071525574, confidence:1.0, loss:37.53705978393555
epoch0: step500/4680
step 0: accuracy:0.12300000339746475, confidence:1.0, loss:52.673736572265625
epoch0: step1000/4680
step 0: accuracy:0.08500000089406967, confidence:0.9987502098083496, loss:9.147778511047363
epoch0: step1500/4680
step 0: accuracy:0.08699999749660492, confidence:0.9999746084213257, loss:12.583880424499512
epoch0: step2000/4680
step 0: accuracy:0.10199999809265137, confidence:0.9998728036880493, loss:11.534943580627441
epoch0: step2500/4680
step 0: accuracy:0.10199999809265137, confidence:0.9999677538871765, loss:12.019225120544434
epoch0: step3000/4680
step 0: accuracy:0.07900000363588333, confidence:0.9983999133110046, loss:9.85152530670166
epoch0: step3500/4680
step 0: accuracy:0.0949999988079071, confidence:0.9984891414642334, loss:9.53611946105957
epoch0: step4000/4680
step 0: accuracy:0.08500000089406967, confidence:0.9991989731788635, loss:9.377976417541504
epoch0: step4500/4680
step 0: accuracy:0.08299999684095383, confidence:0.9995792508125305, loss:10.007451057434082
epoch1: step0/4680
step 500: accuracy:0.09700000286102295, confidence:0.9903586506843567, loss:6.596695423126221
epoch1: step500/4680
step 1000: accuracy:0.09000000357627869, confidence:0.9991886615753174, loss:9.089864730834961
epoch1: step1000/4680
step 1500: accuracy:0.08799999952316284, confidence:0.9860958456993103, loss:7.122259616851807
epoch1: step1500/4680
step 2000: accuracy:0.0949999988079071, confidence:0.9939630031585693, loss:7.051948547363281
epoch1: step2000/4680
step 2500: accuracy:0.10599999874830246, confidence:0.998749315738678, loss:8.907949447631836
epoch1: step2500/4680
step 3000: accuracy:0.09700000286102295, confidence:0.9993796348571777, loss:9.356226921081543
epoch1: step3000/4680
step 3500: accuracy:0.11400000005960464, confidence:0.9516390562057495, loss:5.404726028442383
epoch1: step3500/4680
step 4000: accuracy:0.08799999952316284, confidence:0.9899821877479553, loss:7.046299934387207
epoch1: step4000/4680
step 4500: accuracy:0.09000000357627869, confidence:0.9951346516609192, loss:7.571101665496826
epoch1: step4500/4680
step 0: accuracy:0.10100000351667404, confidence:0.9981771111488342, loss:8.297999382019043
epoch2: step0/4680
step 1000: accuracy:0.08399999886751175, confidence:0.9773529171943665, loss:6.218717098236084
epoch2: step500/4680
step 2000: accuracy:0.11500000208616257, confidence:0.988310694694519, loss:6.5978193283081055
epoch2: step1000/4680
step 3000: accuracy:0.09600000083446503, confidence:0.9829391241073608, loss:7.044753551483154
epoch2: step1500/4680
step 4000: accuracy:0.0949999988079071, confidence:0.9713073968887329, loss:5.8832478523254395
epoch2: step2000/4680
step 5000: accuracy:0.11400000005960464, confidence:0.9996326565742493, loss:9.402267456054688
epoch2: step2500/4680
step 6000: accuracy:0.10400000214576721, confidence:0.9989635348320007, loss:8.779454231262207
epoch2: step3000/4680
step 7000: accuracy:0.1080000028014183, confidence:0.7891469597816467, loss:3.908803701400757
epoch2: step3500/4680
step 8000: accuracy:0.1080000028014183, confidence:0.9297861456871033, loss:5.067020416259766
epoch2: step4000/4680
step 9000: accuracy:0.09700000286102295, confidence:0.9966712594032288, loss:7.6332244873046875
epoch2: step4500/4680
step 0: accuracy:0.09799999743700027, confidence:0.9994712471961975, loss:9.21042537689209
epoch3: step0/4680
step 1500: accuracy:0.13699999451637268, confidence:0.971980631351471, loss:5.7219648361206055
epoch3: step500/4680
step 3000: accuracy:0.1120000034570694, confidence:0.9683893918991089, loss:5.954338073730469
epoch3: step1000/4680
step 4500: accuracy:0.09799999743700027, confidence:0.9660159349441528, loss:6.554664134979248
epoch3: step1500/4680
step 6000: accuracy:0.10400000214576721, confidence:0.9426029324531555, loss:6.686990261077881
epoch3: step2000/4680
step 7500: accuracy:0.09799999743700027, confidence:0.998441755771637, loss:8.478852272033691
epoch3: step2500/4680
step 9000: accuracy:0.11100000143051147, confidence:0.9987302422523499, loss:8.385140419006348
epoch3: step3000/4680
step 10500: accuracy:0.11500000208616257, confidence:0.8168138265609741, loss:4.222428798675537
epoch3: step3500/4680
step 12000: accuracy:0.10499999672174454, confidence:0.9294649362564087, loss:5.3464837074279785
epoch3: step4000/4680
step 13500: accuracy:0.10499999672174454, confidence:0.9962278008460999, loss:7.290748596191406
epoch3: step4500/4680
step 0: accuracy:0.10000000149011612, confidence:0.9989798069000244, loss:8.44404125213623
epoch4: step0/4680
step 2000: accuracy:0.12999999523162842, confidence:0.9257368445396423, loss:4.983343124389648
epoch4: step500/4680
step 4000: accuracy:0.10000000149011612, confidence:0.8034133911132812, loss:4.8752827644348145
epoch4: step1000/4680
step 6000: accuracy:0.10999999940395355, confidence:0.9647704362869263, loss:7.070618629455566
epoch4: step1500/4680
step 8000: accuracy:0.0949999988079071, confidence:0.8671835064888, loss:5.8072052001953125
epoch4: step2000/4680
step 10000: accuracy:0.10599999874830246, confidence:0.9986498951911926, loss:8.694003105163574
epoch4: step2500/4680
step 12000: accuracy:0.10000000149011612, confidence:0.9986697435379028, loss:8.745172500610352
epoch4: step3000/4680
step 14000: accuracy:0.10400000214576721, confidence:0.7715219855308533, loss:4.281276226043701
epoch4: step3500/4680
step 16000: accuracy:0.10499999672174454, confidence:0.8781566023826599, loss:5.134895324707031
epoch4: step4000/4680
step 18000: accuracy:0.11100000143051147, confidence:0.9969164133071899, loss:7.602187156677246
epoch4: step4500/4680
step 0: accuracy:0.09600000083446503, confidence:0.9992675185203552, loss:8.945928573608398
epoch5: step0/4680
step 2500: accuracy:0.09600000083446503, confidence:0.9378212094306946, loss:5.9516496658325195
epoch5: step500/4680
step 5000: accuracy:0.09600000083446503, confidence:0.8992054462432861, loss:5.7554144859313965
epoch5: step1000/4680
step 7500: accuracy:0.0989999994635582, confidence:0.9674245715141296, loss:6.796609878540039
epoch5: step1500/4680
step 10000: accuracy:0.11999999731779099, confidence:0.8337764143943787, loss:5.767396450042725
epoch5: step2000/4680
step 12500: accuracy:0.10499999672174454, confidence:0.9967548251152039, loss:8.41447925567627
epoch5: step2500/4680
step 15000: accuracy:0.11299999803304672, confidence:0.9973651170730591, loss:8.484285354614258
epoch5: step3000/4680
step 17500: accuracy:0.09399999678134918, confidence:0.738930344581604, loss:4.295938491821289
epoch5: step3500/4680
step 20000: accuracy:0.09099999815225601, confidence:0.8567654490470886, loss:5.304739475250244
epoch5: step4000/4680
step 22500: accuracy:0.10199999809265137, confidence:0.9963380694389343, loss:7.72467565536499
epoch5: step4500/4680
step 0: accuracy:0.10100000351667404, confidence:0.9992064833641052, loss:8.911391258239746
epoch6: step0/4680
step 3000: accuracy:0.1120000034570694, confidence:0.90865558385849, loss:5.3476243019104
epoch6: step500/4680
step 6000: accuracy:0.07100000232458115, confidence:0.749971866607666, loss:5.413662433624268
epoch6: step1000/4680
step 9000: accuracy:0.10199999809265137, confidence:0.8909838795661926, loss:7.333066940307617
epoch6: step1500/4680
step 12000: accuracy:0.13300000131130219, confidence:0.8187243938446045, loss:5.751784324645996
epoch6: step2000/4680
step 15000: accuracy:0.11299999803304672, confidence:0.9988086819648743, loss:9.366835594177246
epoch6: step2500/4680
step 18000: accuracy:0.10100000351667404, confidence:0.9996492266654968, loss:9.919499397277832
epoch6: step3000/4680
step 21000: accuracy:0.09700000286102295, confidence:0.7413210272789001, loss:4.385010242462158
epoch6: step3500/4680
step 24000: accuracy:0.10199999809265137, confidence:0.8236671090126038, loss:4.967665195465088
epoch6: step4000/4680
step 27000: accuracy:0.10999999940395355, confidence:0.9965855479240417, loss:7.907207489013672
epoch6: step4500/4680
step 0: accuracy:0.10700000077486038, confidence:0.998958945274353, loss:8.90442943572998
epoch7: step0/4680
step 3500: accuracy:0.12600000202655792, confidence:0.931160569190979, loss:8.207313537597656
epoch7: step500/4680
step 7000: accuracy:0.10899999737739563, confidence:0.9672766327857971, loss:7.5259881019592285
epoch7: step1000/4680
step 10500: accuracy:0.09300000220537186, confidence:0.8531126976013184, loss:6.08724308013916
epoch7: step1500/4680
step 14000: accuracy:0.11599999666213989, confidence:0.844718873500824, loss:6.490071773529053
epoch7: step2000/4680
step 17500: accuracy:0.08900000154972076, confidence:0.9957538843154907, loss:9.181746482849121
epoch7: step2500/4680
step 21000: accuracy:0.09200000017881393, confidence:0.9900082349777222, loss:8.698911666870117
epoch7: step3000/4680
step 24500: accuracy:0.09799999743700027, confidence:0.858781099319458, loss:5.361366271972656
epoch7: step3500/4680
step 28000: accuracy:0.0949999988079071, confidence:0.911189079284668, loss:6.466826915740967
epoch7: step4000/4680
step 31500: accuracy:0.10000000149011612, confidence:0.9971920251846313, loss:8.896666526794434
epoch7: step4500/4680
step 0: accuracy:0.09399999678134918, confidence:0.9989514350891113, loss:9.426115989685059
epoch8: step0/4680
step 4000: accuracy:0.10400000214576721, confidence:0.9295170307159424, loss:5.799098491668701
epoch8: step500/4680
step 8000: accuracy:0.04500000178813934, confidence:0.7710067629814148, loss:5.923542022705078
epoch8: step1000/4680
step 12000: accuracy:0.10700000077486038, confidence:0.9774239659309387, loss:10.293767929077148
epoch8: step1500/4680
step 16000: accuracy:0.11299999803304672, confidence:0.8410957455635071, loss:5.20171594619751
epoch8: step2000/4680
step 20000: accuracy:0.11400000005960464, confidence:0.9907485246658325, loss:9.721631050109863
epoch8: step2500/4680
step 24000: accuracy:0.11100000143051147, confidence:0.9985146522521973, loss:9.92911148071289
epoch8: step3000/4680
step 28000: accuracy:0.10899999737739563, confidence:0.7841922640800476, loss:4.412665367126465
epoch8: step3500/4680
step 32000: accuracy:0.10300000011920929, confidence:0.8091219067573547, loss:5.244675159454346
epoch8: step4000/4680
step 36000: accuracy:0.10999999940395355, confidence:0.9979550242424011, loss:9.214251518249512
epoch8: step4500/4680
step 0: accuracy:0.09200000017881393, confidence:0.998928964138031, loss:9.54626178741455
epoch9: step0/4680
step 4500: accuracy:0.1080000028014183, confidence:0.9427251219749451, loss:8.4921875
epoch9: step500/4680
step 9000: accuracy:0.10899999737739563, confidence:0.9681605696678162, loss:7.9928178787231445
epoch9: step1000/4680
step 13500: accuracy:0.0949999988079071, confidence:0.8616704940795898, loss:6.825347423553467
epoch9: step1500/4680
step 18000: accuracy:0.10999999940395355, confidence:0.8726001381874084, loss:9.17419147491455
epoch9: step2000/4680
step 22500: accuracy:0.10899999737739563, confidence:0.9988435506820679, loss:9.662842750549316
epoch9: step2500/4680
step 27000: accuracy:0.09600000083446503, confidence:0.99289470911026, loss:8.866876602172852
epoch9: step3000/4680
step 31500: accuracy:0.08299999684095383, confidence:0.8276354074478149, loss:4.903906345367432
epoch9: step3500/4680
step 36000: accuracy:0.0949999988079071, confidence:0.8530030250549316, loss:5.683751106262207
epoch9: step4000/4680
step 40500: accuracy:0.10199999809265137, confidence:0.9970275163650513, loss:10.306265830993652
epoch9: step4500/4680
step 0: accuracy:0.09200000017881393, confidence:0.998831033706665, loss:10.620863914489746
epoch10: step0/4680
step 5000: accuracy:0.11299999803304672, confidence:0.940712034702301, loss:6.149017810821533
epoch10: step500/4680
step 10000: accuracy:0.04800000041723251, confidence:0.7777795791625977, loss:6.067399978637695
epoch10: step1000/4680
step 15000: accuracy:0.11100000143051147, confidence:0.9929433465003967, loss:9.858230590820312
epoch10: step1500/4680
step 20000: accuracy:0.10700000077486038, confidence:0.8637088537216187, loss:5.237348556518555
epoch10: step2000/4680
step 25000: accuracy:0.09799999743700027, confidence:0.9707314372062683, loss:9.666388511657715
epoch10: step2500/4680
step 30000: accuracy:0.11299999803304672, confidence:0.9775540232658386, loss:9.259347915649414
epoch10: step3000/4680
step 35000: accuracy:0.10300000011920929, confidence:0.8492560386657715, loss:5.206859111785889
epoch10: step3500/4680
step 40000: accuracy:0.09200000017881393, confidence:0.8640931844711304, loss:5.82703971862793
epoch10: step4000/4680
step 45000: accuracy:0.0989999994635582, confidence:0.9978272914886475, loss:10.328317642211914
epoch10: step4500/4680
step 0: accuracy:0.11599999666213989, confidence:0.9992004632949829, loss:10.148356437683105
epoch11: step0/4680
step 5500: accuracy:0.12600000202655792, confidence:0.9290849566459656, loss:7.708807945251465
epoch11: step500/4680
step 11000: accuracy:0.10300000011920929, confidence:0.9019843935966492, loss:7.131895542144775
epoch11: step1000/4680
step 16500: accuracy:0.08699999749660492, confidence:0.9278643727302551, loss:8.844992637634277
epoch11: step1500/4680
step 22000: accuracy:0.1120000034570694, confidence:0.8687169551849365, loss:9.823220252990723
epoch11: step2000/4680
step 27500: accuracy:0.11500000208616257, confidence:0.9991385340690613, loss:10.074546813964844
epoch11: step2500/4680
step 33000: accuracy:0.1080000028014183, confidence:0.9953613877296448, loss:9.630175590515137
epoch11: step3000/4680
step 38500: accuracy:0.10499999672174454, confidence:0.8113428354263306, loss:4.81926155090332
epoch11: step3500/4680
step 44000: accuracy:0.09200000017881393, confidence:0.7638307809829712, loss:4.850356101989746
epoch11: step4000/4680
step 49500: accuracy:0.10999999940395355, confidence:0.996878445148468, loss:10.878645896911621
epoch11: step4500/4680
step 0: accuracy:0.10999999940395355, confidence:0.9982248544692993, loss:10.527207374572754
epoch12: step0/4680
step 6000: accuracy:0.12200000137090683, confidence:0.9406116008758545, loss:7.117095470428467
epoch12: step500/4680
step 12000: accuracy:0.10199999809265137, confidence:0.8136382102966309, loss:6.447930812835693
epoch12: step1000/4680
step 18000: accuracy:0.08900000154972076, confidence:0.9749349355697632, loss:10.4526948928833
epoch12: step1500/4680
step 24000: accuracy:0.10599999874830246, confidence:0.8561769723892212, loss:5.785696506500244
epoch12: step2000/4680
step 30000: accuracy:0.08799999952316284, confidence:0.9716626405715942, loss:11.083739280700684
epoch12: step2500/4680
step 36000: accuracy:0.10400000214576721, confidence:0.9822876453399658, loss:10.523734092712402
epoch12: step3000/4680
step 42000: accuracy:0.09300000220537186, confidence:0.8525853753089905, loss:5.671107292175293
epoch12: step3500/4680
step 48000: accuracy:0.09200000017881393, confidence:0.8557301759719849, loss:5.947755336761475
epoch12: step4000/4680
step 54000: accuracy:0.10899999737739563, confidence:0.9958795309066772, loss:9.915493965148926
epoch12: step4500/4680
step 0: accuracy:0.10000000149011612, confidence:0.9985659718513489, loss:10.281027793884277
epoch13: step0/4680
step 6500: accuracy:0.1379999965429306, confidence:0.9115654826164246, loss:6.079809665679932
epoch13: step500/4680
step 13000: accuracy:0.05000000074505806, confidence:0.8048719167709351, loss:6.271063804626465
epoch13: step1000/4680
step 19500: accuracy:0.11900000274181366, confidence:0.9625850915908813, loss:9.52617359161377
epoch13: step1500/4680
step 26000: accuracy:0.13300000131130219, confidence:0.8858246803283691, loss:8.826844215393066
epoch13: step2000/4680
step 32500: accuracy:0.10100000351667404, confidence:0.997599720954895, loss:10.81617546081543
epoch13: step2500/4680
step 39000: accuracy:0.10400000214576721, confidence:0.9700504541397095, loss:9.106610298156738
epoch13: step3000/4680
step 45500: accuracy:0.10499999672174454, confidence:0.8895291686058044, loss:5.970555782318115
epoch13: step3500/4680
step 52000: accuracy:0.09000000357627869, confidence:0.7617849707603455, loss:5.377164840698242
epoch13: step4000/4680
step 58500: accuracy:0.10700000077486038, confidence:0.9998651146888733, loss:13.408424377441406
epoch13: step4500/4680
step 0: accuracy:0.11100000143051147, confidence:0.9996746778488159, loss:12.014802932739258
epoch14: step0/4680
step 7000: accuracy:0.12300000339746475, confidence:0.9292176961898804, loss:6.298732757568359
epoch14: step500/4680
step 14000: accuracy:0.04699999839067459, confidence:0.8328965306282043, loss:6.174743175506592
epoch14: step1000/4680
step 21000: accuracy:0.09700000286102295, confidence:0.9147506356239319, loss:8.729398727416992
epoch14: step1500/4680
step 28000: accuracy:0.10599999874830246, confidence:0.8391147255897522, loss:6.853930473327637
epoch14: step2000/4680
step 35000: accuracy:0.08900000154972076, confidence:0.9831293225288391, loss:11.797995567321777
epoch14: step2500/4680
step 42000: accuracy:0.11800000071525574, confidence:0.9907081127166748, loss:11.500276565551758
epoch14: step3000/4680
step 49000: accuracy:0.09600000083446503, confidence:0.9225601553916931, loss:6.600226879119873
epoch14: step3500/4680
step 56000: accuracy:0.08500000089406967, confidence:0.8329420685768127, loss:6.046458721160889
epoch14: step4000/4680
step 63000: accuracy:0.10300000011920929, confidence:0.999451756477356, loss:13.090230941772461
epoch14: step4500/4680
step 0: accuracy:0.10199999809265137, confidence:0.9994322061538696, loss:12.082274436950684
epoch15: step0/4680
step 7500: accuracy:0.11400000005960464, confidence:0.9261908531188965, loss:7.446535587310791
epoch15: step500/4680
step 15000: accuracy:0.11400000005960464, confidence:0.9574226140975952, loss:8.389719009399414
epoch15: step1000/4680
step 22500: accuracy:0.1589999943971634, confidence:0.8745461702346802, loss:7.714811325073242
epoch15: step1500/4680
step 30000: accuracy:0.10000000149011612, confidence:0.8945448994636536, loss:9.366703987121582
epoch15: step2000/4680
step 37500: accuracy:0.10499999672174454, confidence:0.9833272099494934, loss:11.17435359954834
epoch15: step2500/4680
step 45000: accuracy:0.0989999994635582, confidence:0.9704455733299255, loss:9.841249465942383
epoch15: step3000/4680
step 52500: accuracy:0.06800000369548798, confidence:0.8580524921417236, loss:6.57398796081543
epoch15: step3500/4680
step 60000: accuracy:0.09399999678134918, confidence:0.7828428745269775, loss:5.842381477355957
epoch15: step4000/4680
step 67500: accuracy:0.0989999994635582, confidence:0.9963557720184326, loss:13.8306245803833
epoch15: step4500/4680
step 0: accuracy:0.09300000220537186, confidence:0.9981019496917725, loss:12.926481246948242
epoch16: step0/4680
step 8000: accuracy:0.13899999856948853, confidence:0.9068865776062012, loss:6.611097812652588
epoch16: step500/4680
step 16000: accuracy:0.05999999865889549, confidence:0.8661224842071533, loss:6.719033241271973
epoch16: step1000/4680
step 24000: accuracy:0.09300000220537186, confidence:0.8771157264709473, loss:8.492841720581055
epoch16: step1500/4680
step 32000: accuracy:0.08399999886751175, confidence:0.9448227286338806, loss:7.488376140594482
epoch16: step2000/4680
step 40000: accuracy:0.10400000214576721, confidence:0.9914423823356628, loss:12.373198509216309
epoch16: step2500/4680
step 48000: accuracy:0.11599999666213989, confidence:0.9991082549095154, loss:13.123560905456543
epoch16: step3000/4680
step 56000: accuracy:0.11299999803304672, confidence:0.9159087538719177, loss:6.941242218017578
epoch16: step3500/4680
step 64000: accuracy:0.09099999815225601, confidence:0.8957896828651428, loss:6.661359786987305
epoch16: step4000/4680
step 72000: accuracy:0.08399999886751175, confidence:0.9961602687835693, loss:13.263664245605469
epoch16: step4500/4680
step 0: accuracy:0.10400000214576721, confidence:0.9950498938560486, loss:12.095658302307129
epoch17: step0/4680
step 8500: accuracy:0.12999999523162842, confidence:0.9445645809173584, loss:7.694578170776367
epoch17: step500/4680
step 17000: accuracy:0.06300000101327896, confidence:0.8029847145080566, loss:7.249889850616455
epoch17: step1000/4680
step 25500: accuracy:0.08100000023841858, confidence:0.9482729434967041, loss:10.290380477905273
epoch17: step1500/4680
step 34000: accuracy:0.0949999988079071, confidence:0.8675987124443054, loss:8.814013481140137
epoch17: step2000/4680
step 42500: accuracy:0.08699999749660492, confidence:0.9777469635009766, loss:10.92801570892334
epoch17: step2500/4680
step 51000: accuracy:0.10499999672174454, confidence:0.9909220933914185, loss:11.901227951049805
epoch17: step3000/4680
step 59500: accuracy:0.07699999958276749, confidence:0.9131407737731934, loss:7.522894859313965
epoch17: step3500/4680
step 68000: accuracy:0.08299999684095383, confidence:0.8303557634353638, loss:6.445582389831543
epoch17: step4000/4680
step 76500: accuracy:0.09200000017881393, confidence:0.9976680874824524, loss:14.143799781799316
epoch17: step4500/4680
step 0: accuracy:0.0949999988079071, confidence:0.9983147382736206, loss:13.215804100036621
epoch18: step0/4680
step 9000: accuracy:0.12800000607967377, confidence:0.910137414932251, loss:6.245474815368652
epoch18: step500/4680
step 18000: accuracy:0.07100000232458115, confidence:0.8179253935813904, loss:6.938652992248535
epoch18: step1000/4680
step 27000: accuracy:0.14499999582767487, confidence:0.8774293661117554, loss:7.758634567260742
epoch18: step1500/4680
step 36000: accuracy:0.0989999994635582, confidence:0.8725589513778687, loss:6.240785598754883
epoch18: step2000/4680
step 45000: accuracy:0.09600000083446503, confidence:0.991538941860199, loss:13.786649703979492
epoch18: step2500/4680
step 54000: accuracy:0.10700000077486038, confidence:0.9700073599815369, loss:11.520357131958008
epoch18: step3000/4680
step 63000: accuracy:0.09700000286102295, confidence:0.9300981760025024, loss:7.999602794647217
epoch18: step3500/4680
step 72000: accuracy:0.07500000298023224, confidence:0.8582256436347961, loss:6.764545917510986
epoch18: step4000/4680
step 81000: accuracy:0.08900000154972076, confidence:0.9821234345436096, loss:12.542938232421875
epoch18: step4500/4680
step 0: accuracy:0.09099999815225601, confidence:0.9839360117912292, loss:11.507654190063477
epoch19: step0/4680
step 9500: accuracy:0.12999999523162842, confidence:0.9431239366531372, loss:7.7664690017700195
epoch19: step500/4680
step 19000: accuracy:0.07199999690055847, confidence:0.8872944116592407, loss:6.932981014251709
epoch19: step1000/4680
step 28500: accuracy:0.09600000083446503, confidence:0.8988524079322815, loss:9.73984146118164
epoch19: step1500/4680
step 38000: accuracy:0.11699999868869781, confidence:0.9057009816169739, loss:10.648698806762695
epoch19: step2000/4680
step 47500: accuracy:0.09000000357627869, confidence:0.9936131238937378, loss:11.538016319274902
epoch19: step2500/4680
step 57000: accuracy:0.10700000077486038, confidence:0.9896681308746338, loss:11.147241592407227
epoch19: step3000/4680
step 66500: accuracy:0.10999999940395355, confidence:0.912993311882019, loss:7.283759593963623
epoch19: step3500/4680
step 76000: accuracy:0.12800000607967377, confidence:0.7685210108757019, loss:5.733752250671387
epoch19: step4000/4680
step 85500: accuracy:0.08799999952316284, confidence:0.9951249361038208, loss:12.40713882446289
epoch19: step4500/4680
step 0: accuracy:0.09399999678134918, confidence:0.9961933493614197, loss:11.474756240844727
epoch20: step0/4680
step 10000: accuracy:0.13099999725818634, confidence:0.8885910511016846, loss:6.511209964752197
epoch20: step500/4680
step 20000: accuracy:0.04600000008940697, confidence:0.8594915866851807, loss:6.989904403686523
epoch20: step1000/4680
step 30000: accuracy:0.13600000739097595, confidence:0.8856837749481201, loss:8.654098510742188
epoch20: step1500/4680
step 40000: accuracy:0.10100000351667404, confidence:0.8811346292495728, loss:6.476526737213135
epoch20: step2000/4680
step 50000: accuracy:0.10700000077486038, confidence:0.993733286857605, loss:14.429374694824219
epoch20: step2500/4680
step 60000: accuracy:0.09799999743700027, confidence:0.9906478524208069, loss:13.601301193237305
epoch20: step3000/4680
step 70000: accuracy:0.08699999749660492, confidence:0.9557291865348816, loss:9.020096778869629
epoch20: step3500/4680
step 80000: accuracy:0.10499999672174454, confidence:0.8780108094215393, loss:6.5598320960998535
epoch20: step4000/4680
step 90000: accuracy:0.10899999737739563, confidence:0.9770983457565308, loss:12.263537406921387
epoch20: step4500/4680
step 0: accuracy:0.09300000220537186, confidence:0.9842349886894226, loss:11.473724365234375
epoch21: step0/4680
step 10500: accuracy:0.10400000214576721, confidence:0.9587064981460571, loss:8.107531547546387
epoch21: step500/4680
step 21000: accuracy:0.050999999046325684, confidence:0.8278630375862122, loss:6.939430236816406
epoch21: step1000/4680
step 31500: accuracy:0.10199999809265137, confidence:0.976085364818573, loss:10.670208930969238
epoch21: step1500/4680
step 42000: accuracy:0.09700000286102295, confidence:0.8829553127288818, loss:10.004727363586426
epoch21: step2000/4680
step 52500: accuracy:0.09799999743700027, confidence:0.9476819038391113, loss:9.52611255645752
epoch21: step2500/4680
step 63000: accuracy:0.09600000083446503, confidence:0.9885752201080322, loss:12.865683555603027
epoch21: step3000/4680
step 73500: accuracy:0.10899999737739563, confidence:0.8531606197357178, loss:7.284346580505371
epoch21: step3500/4680
step 84000: accuracy:0.10100000351667404, confidence:0.8605510592460632, loss:7.372828006744385
epoch21: step4000/4680
step 94500: accuracy:0.09099999815225601, confidence:0.9829874038696289, loss:9.597077369689941
epoch21: step4500/4680
step 0: accuracy:0.07999999821186066, confidence:0.9942867755889893, loss:10.209615707397461
epoch22: step0/4680
step 11000: accuracy:0.12800000607967377, confidence:0.9024117588996887, loss:6.525237560272217
epoch22: step500/4680
step 22000: accuracy:0.08500000089406967, confidence:0.8381215929985046, loss:7.404728412628174
epoch22: step1000/4680
step 33000: accuracy:0.0860000029206276, confidence:0.8899681568145752, loss:8.933650970458984
epoch22: step1500/4680
step 44000: accuracy:0.0820000022649765, confidence:0.8568080067634583, loss:6.116506576538086
epoch22: step2000/4680
step 55000: accuracy:0.10199999809265137, confidence:0.9815629124641418, loss:11.543286323547363
epoch22: step2500/4680
step 66000: accuracy:0.10599999874830246, confidence:0.9859244227409363, loss:12.719453811645508
epoch22: step3000/4680
step 77000: accuracy:0.09000000357627869, confidence:0.9370691776275635, loss:8.463537216186523
epoch22: step3500/4680
step 88000: accuracy:0.08699999749660492, confidence:0.8502129316329956, loss:6.601189136505127
epoch22: step4000/4680
step 99000: accuracy:0.052000001072883606, confidence:0.9557892084121704, loss:10.541053771972656
epoch22: step4500/4680
step 0: accuracy:0.07000000029802322, confidence:0.9706705212593079, loss:10.614267349243164
epoch23: step0/4680
step 11500: accuracy:0.11100000143051147, confidence:0.9679691791534424, loss:7.315192222595215
epoch23: step500/4680
step 23000: accuracy:0.05400000140070915, confidence:0.8321581482887268, loss:7.208924770355225
epoch23: step1000/4680
step 34500: accuracy:0.10100000351667404, confidence:0.9533248543739319, loss:9.6803560256958
epoch23: step1500/4680
step 46000: accuracy:0.1120000034570694, confidence:0.8726754188537598, loss:8.203975677490234
epoch23: step2000/4680
step 57500: accuracy:0.09099999815225601, confidence:0.9615781307220459, loss:9.240830421447754
epoch23: step2500/4680
step 69000: accuracy:0.10499999672174454, confidence:0.9630650877952576, loss:9.84237289428711
epoch23: step3000/4680
step 80500: accuracy:0.10400000214576721, confidence:0.8543006777763367, loss:7.218782901763916
epoch23: step3500/4680
step 92000: accuracy:0.11900000274181366, confidence:0.8282198309898376, loss:6.927849769592285
epoch23: step4000/4680
step 103500: accuracy:0.0949999988079071, confidence:0.9899919629096985, loss:10.38439655303955
epoch23: step4500/4680
step 0: accuracy:0.09300000220537186, confidence:0.9969887733459473, loss:10.672711372375488
epoch24: step0/4680
step 12000: accuracy:0.14399999380111694, confidence:0.9120575189590454, loss:7.75214958190918
epoch24: step500/4680
step 24000: accuracy:0.0560000017285347, confidence:0.8514055609703064, loss:6.980382442474365
epoch24: step1000/4680
step 36000: accuracy:0.10700000077486038, confidence:0.8260555267333984, loss:7.222907543182373
epoch24: step1500/4680
step 48000: accuracy:0.09300000220537186, confidence:0.9653794765472412, loss:7.4386091232299805
epoch24: step2000/4680
step 60000: accuracy:0.09799999743700027, confidence:0.9835044145584106, loss:10.784103393554688
epoch24: step2500/4680
step 72000: accuracy:0.10100000351667404, confidence:0.9916985034942627, loss:13.720251083374023
epoch24: step3000/4680
step 84000: accuracy:0.1120000034570694, confidence:0.9477055072784424, loss:9.37142562866211
epoch24: step3500/4680
step 96000: accuracy:0.09200000017881393, confidence:0.8632906675338745, loss:7.050662994384766
epoch24: step4000/4680
step 108000: accuracy:0.06800000369548798, confidence:0.9580782651901245, loss:11.886662483215332
epoch24: step4500/4680
step 0: accuracy:0.07400000095367432, confidence:0.9725353717803955, loss:11.444075584411621
epoch25: step0/4680
step 12500: accuracy:0.11500000208616257, confidence:0.9828730821609497, loss:9.29776382446289
epoch25: step500/4680
step 25000: accuracy:0.029999999329447746, confidence:0.8502483367919922, loss:7.819483757019043
epoch25: step1000/4680
step 37500: accuracy:0.08900000154972076, confidence:0.9800967574119568, loss:13.566888809204102
epoch25: step1500/4680
step 50000: accuracy:0.10300000011920929, confidence:0.8934655785560608, loss:11.201751708984375
epoch25: step2000/4680
step 62500: accuracy:0.09000000357627869, confidence:0.9661266207695007, loss:9.542232513427734
epoch25: step2500/4680
step 75000: accuracy:0.10100000351667404, confidence:0.9663929343223572, loss:10.209504127502441
epoch25: step3000/4680
step 87500: accuracy:0.10599999874830246, confidence:0.8501404523849487, loss:7.2731242179870605
epoch25: step3500/4680
step 100000: accuracy:0.10199999809265137, confidence:0.8549181222915649, loss:7.516579627990723
epoch25: step4000/4680
step 112500: accuracy:0.09000000357627869, confidence:0.9916871786117554, loss:9.92426872253418
epoch25: step4500/4680
step 0: accuracy:0.09300000220537186, confidence:0.9969817996025085, loss:10.675838470458984
epoch26: step0/4680
step 13000: accuracy:0.14300000667572021, confidence:0.9202433228492737, loss:8.28352165222168
epoch26: step500/4680
step 26000: accuracy:0.09600000083446503, confidence:0.9400990009307861, loss:9.042001724243164
epoch26: step1000/4680
step 39000: accuracy:0.07100000232458115, confidence:0.8384127020835876, loss:7.393834590911865
epoch26: step1500/4680
step 52000: accuracy:0.10899999737739563, confidence:0.9381670951843262, loss:8.246402740478516
epoch26: step2000/4680
step 65000: accuracy:0.11400000005960464, confidence:0.9874334931373596, loss:11.559338569641113
epoch26: step2500/4680
step 78000: accuracy:0.11599999666213989, confidence:0.9867126941680908, loss:12.064374923706055
epoch26: step3000/4680
step 91000: accuracy:0.09600000083446503, confidence:0.9450349807739258, loss:8.951608657836914
epoch26: step3500/4680
step 104000: accuracy:0.10700000077486038, confidence:0.8674055337905884, loss:6.9464240074157715
epoch26: step4000/4680
step 117000: accuracy:0.07100000232458115, confidence:0.9565110802650452, loss:11.202603340148926
epoch26: step4500/4680
step 0: accuracy:0.07599999755620956, confidence:0.9695032835006714, loss:11.128701210021973
epoch27: step0/4680
step 13500: accuracy:0.0989999994635582, confidence:0.9865294694900513, loss:10.292762756347656
epoch27: step500/4680
step 27000: accuracy:0.03200000151991844, confidence:0.8928162455558777, loss:7.708474159240723
epoch27: step1000/4680
step 40500: accuracy:0.08900000154972076, confidence:0.9755879640579224, loss:13.120696067810059
epoch27: step1500/4680
step 54000: accuracy:0.09300000220537186, confidence:0.8836571574211121, loss:10.07439136505127
epoch27: step2000/4680
step 67500: accuracy:0.07599999755620956, confidence:0.9497566819190979, loss:10.136025428771973
epoch27: step2500/4680
step 81000: accuracy:0.1120000034570694, confidence:0.9884438514709473, loss:14.219945907592773
epoch27: step3000/4680
step 94500: accuracy:0.10400000214576721, confidence:0.8707408905029297, loss:7.866937637329102
epoch27: step3500/4680
step 108000: accuracy:0.09200000017881393, confidence:0.8759276866912842, loss:8.044196128845215
epoch27: step4000/4680
step 121500: accuracy:0.10000000149011612, confidence:0.9838319420814514, loss:9.429548263549805
epoch27: step4500/4680
step 0: accuracy:0.08799999952316284, confidence:0.9933411478996277, loss:10.384391784667969
epoch28: step0/4680
step 14000: accuracy:0.12399999797344208, confidence:0.9034896492958069, loss:7.444293975830078
epoch28: step500/4680
step 28000: accuracy:0.035999998450279236, confidence:0.847676157951355, loss:7.115925312042236
epoch28: step1000/4680
step 42000: accuracy:0.07800000160932541, confidence:0.8223751783370972, loss:7.209688186645508
epoch28: step1500/4680
step 56000: accuracy:0.10400000214576721, confidence:0.9380833506584167, loss:7.229512691497803
epoch28: step2000/4680
step 70000: accuracy:0.10499999672174454, confidence:0.9789019227027893, loss:10.563333511352539
epoch28: step2500/4680
step 84000: accuracy:0.10700000077486038, confidence:0.9847986102104187, loss:12.963578224182129
epoch28: step3000/4680
step 98000: accuracy:0.0860000029206276, confidence:0.9322406053543091, loss:9.195989608764648
epoch28: step3500/4680
step 112000: accuracy:0.08299999684095383, confidence:0.8583810925483704, loss:7.154556751251221
epoch28: step4000/4680
step 126000: accuracy:0.05700000002980232, confidence:0.9371665716171265, loss:9.478029251098633
epoch28: step4500/4680
step 0: accuracy:0.05999999865889549, confidence:0.9599283933639526, loss:10.28834342956543
epoch29: step0/4680
step 14500: accuracy:0.11400000005960464, confidence:0.9753856062889099, loss:8.826650619506836
epoch29: step500/4680
step 29000: accuracy:0.03700000047683716, confidence:0.8640583753585815, loss:7.445624351501465
epoch29: step1000/4680
step 43500: accuracy:0.08900000154972076, confidence:0.9440755844116211, loss:10.862557411193848
epoch29: step1500/4680
step 58000: accuracy:0.1080000028014183, confidence:0.900477945804596, loss:10.308526992797852
epoch29: step2000/4680
step 72500: accuracy:0.08399999886751175, confidence:0.9579964876174927, loss:10.134947776794434
epoch29: step2500/4680
step 87000: accuracy:0.0949999988079071, confidence:0.9552725553512573, loss:10.422794342041016
epoch29: step3000/4680
step 101500: accuracy:0.09300000220537186, confidence:0.8583583235740662, loss:7.7650346755981445
epoch29: step3500/4680
step 116000: accuracy:0.09300000220537186, confidence:0.8420385718345642, loss:7.351995944976807
epoch29: step4000/4680
step 130500: accuracy:0.08299999684095383, confidence:0.9823968410491943, loss:10.37686538696289
epoch29: step4500/4680
2018-06-15 19:52:49.362231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000a:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 19:52:49.362423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
step 0: accuracy:0.0949999988079071, confidence:0.9474576711654663, loss:9.634812355041504
epoch0: step0/4680
step 0: accuracy:0.08799999952316284, confidence:0.9999997615814209, loss:19.830400466918945
epoch0: step500/4680
step 0: accuracy:0.12300000339746475, confidence:1.0, loss:24.215837478637695
epoch0: step1000/4680
step 0: accuracy:0.10199999809265137, confidence:1.0, loss:26.34467124938965
epoch0: step1500/4680
step 0: accuracy:0.08299999684095383, confidence:0.9994498491287231, loss:10.17867660522461
epoch0: step2000/4680
step 0: accuracy:0.10199999809265137, confidence:0.9999430179595947, loss:11.872284889221191
epoch0: step2500/4680
step 0: accuracy:0.08900000154972076, confidence:0.9997352361679077, loss:10.31185531616211
epoch0: step3000/4680
step 0: accuracy:0.09600000083446503, confidence:0.99998939037323, loss:13.5275297164917
epoch0: step3500/4680
step 0: accuracy:0.09200000017881393, confidence:0.999897837638855, loss:11.715526580810547
epoch0: step4000/4680
step 0: accuracy:0.08900000154972076, confidence:0.9992161393165588, loss:9.901515007019043
epoch0: step4500/4680
step 0: accuracy:0.1120000034570694, confidence:0.9989373683929443, loss:9.349705696105957
epoch1: step0/4680
step 500: accuracy:0.10400000214576721, confidence:0.9877307415008545, loss:6.815344333648682
epoch1: step500/4680
step 1000: accuracy:0.09000000357627869, confidence:0.9990483522415161, loss:9.946586608886719
epoch1: step1000/4680
step 1500: accuracy:0.10700000077486038, confidence:0.9991579651832581, loss:10.07166576385498
epoch1: step1500/4680
step 2000: accuracy:0.10899999737739563, confidence:0.9999021291732788, loss:11.349587440490723
epoch1: step2000/4680
step 2500: accuracy:0.10599999874830246, confidence:0.999956488609314, loss:11.822077751159668
epoch1: step2500/4680
step 3000: accuracy:0.10700000077486038, confidence:0.9996398091316223, loss:10.234128952026367
epoch1: step3000/4680
step 3500: accuracy:0.11400000005960464, confidence:0.9998571872711182, loss:11.118037223815918
epoch1: step3500/4680
step 4000: accuracy:0.12200000137090683, confidence:0.9988213181495667, loss:9.241031646728516
epoch1: step4000/4680
step 4500: accuracy:0.08900000154972076, confidence:0.998351514339447, loss:9.477140426635742
epoch1: step4500/4680
step 0: accuracy:0.09000000357627869, confidence:0.9980621933937073, loss:9.155701637268066
epoch2: step0/4680
step 1000: accuracy:0.10499999672174454, confidence:0.9955424666404724, loss:8.071991920471191
epoch2: step500/4680
step 2000: accuracy:0.11500000208616257, confidence:0.9952781200408936, loss:7.842041969299316
epoch2: step1000/4680
step 3000: accuracy:0.12099999934434891, confidence:0.9983036518096924, loss:9.463081359863281
epoch2: step1500/4680
step 4000: accuracy:0.0949999988079071, confidence:0.9961723685264587, loss:8.751412391662598
epoch2: step2000/4680
step 5000: accuracy:0.11400000005960464, confidence:0.999456524848938, loss:10.137500762939453
epoch2: step2500/4680
step 6000: accuracy:0.10000000149011612, confidence:0.9794436693191528, loss:6.237229347229004
epoch2: step3000/4680
step 7000: accuracy:0.11100000143051147, confidence:0.9999381303787231, loss:12.509571075439453
epoch2: step3500/4680
step 8000: accuracy:0.0949999988079071, confidence:0.9951851963996887, loss:7.9617533683776855
epoch2: step4000/4680
step 9000: accuracy:0.09799999743700027, confidence:0.9918195009231567, loss:7.706811428070068
epoch2: step4500/4680
step 0: accuracy:0.10700000077486038, confidence:0.9919686317443848, loss:7.599340438842773
epoch3: step0/4680
step 1500: accuracy:0.10000000149011612, confidence:0.9937536716461182, loss:8.710832595825195
epoch3: step500/4680
step 3000: accuracy:0.1120000034570694, confidence:0.9143013954162598, loss:5.241140365600586
epoch3: step1000/4680
step 4500: accuracy:0.08799999952316284, confidence:0.9991869926452637, loss:11.583005905151367
epoch3: step1500/4680
step 6000: accuracy:0.0860000029206276, confidence:0.9975389242172241, loss:9.669353485107422
epoch3: step2000/4680
step 7500: accuracy:0.09799999743700027, confidence:0.9995160698890686, loss:11.001709938049316
epoch3: step2500/4680
step 9000: accuracy:0.1120000034570694, confidence:0.8919990658760071, loss:4.752999305725098
epoch3: step3000/4680
step 10500: accuracy:0.10499999672174454, confidence:0.9994465708732605, loss:10.518179893493652
epoch3: step3500/4680
step 12000: accuracy:0.1080000028014183, confidence:0.9493152499198914, loss:5.309409141540527
epoch3: step4000/4680
step 13500: accuracy:0.08799999952316284, confidence:0.9643181562423706, loss:7.262483596801758
epoch3: step4500/4680
step 0: accuracy:0.09799999743700027, confidence:0.9679842591285706, loss:7.0624680519104
epoch4: step0/4680
step 2000: accuracy:0.10999999940395355, confidence:0.9700011014938354, loss:6.7039361000061035
epoch4: step500/4680
step 4000: accuracy:0.09799999743700027, confidence:0.8554391860961914, loss:5.727962970733643
epoch4: step1000/4680
step 6000: accuracy:0.09600000083446503, confidence:0.9977747201919556, loss:10.867905616760254
epoch4: step1500/4680
step 8000: accuracy:0.09399999678134918, confidence:0.9997683763504028, loss:12.272494316101074
epoch4: step2000/4680
step 10000: accuracy:0.10599999874830246, confidence:0.9997225403785706, loss:11.461271286010742
epoch4: step2500/4680
step 12000: accuracy:0.0949999988079071, confidence:0.9330903887748718, loss:5.812219142913818
epoch4: step3000/4680
step 14000: accuracy:0.11900000274181366, confidence:0.9986045956611633, loss:9.957399368286133
epoch4: step3500/4680
step 16000: accuracy:0.1080000028014183, confidence:0.9338461756706238, loss:5.123547554016113
epoch4: step4000/4680
step 18000: accuracy:0.10000000149011612, confidence:0.9429085850715637, loss:6.958184719085693
epoch4: step4500/4680
step 0: accuracy:0.09700000286102295, confidence:0.9446945190429688, loss:6.9639387130737305
epoch5: step0/4680
step 2500: accuracy:0.09799999743700027, confidence:0.9867191314697266, loss:7.876821517944336
epoch5: step500/4680
step 5000: accuracy:0.09200000017881393, confidence:0.7938793897628784, loss:5.17906379699707
epoch5: step1000/4680
step 7500: accuracy:0.08399999886751175, confidence:0.9817851185798645, loss:9.73461627960205
epoch5: step1500/4680
step 10000: accuracy:0.09700000286102295, confidence:0.9998753070831299, loss:13.362471580505371
epoch5: step2000/4680
step 12500: accuracy:0.10499999672174454, confidence:0.9997797608375549, loss:12.213457107543945
epoch5: step2500/4680
step 15000: accuracy:0.09700000286102295, confidence:0.9333427548408508, loss:6.346179485321045
epoch5: step3000/4680
step 17500: accuracy:0.1080000028014183, confidence:0.998176097869873, loss:10.680706977844238
epoch5: step3500/4680
step 20000: accuracy:0.10199999809265137, confidence:0.8016072511672974, loss:4.301772117614746
epoch5: step4000/4680
step 22500: accuracy:0.08900000154972076, confidence:0.8835391998291016, loss:7.1132988929748535
epoch5: step4500/4680
step 0: accuracy:0.08500000089406967, confidence:0.9200868010520935, loss:7.132185459136963
epoch6: step0/4680
step 3000: accuracy:0.09399999678134918, confidence:0.9926004409790039, loss:9.185612678527832
epoch6: step500/4680
step 6000: accuracy:0.11299999803304672, confidence:0.824095606803894, loss:5.901163578033447
epoch6: step1000/4680
step 9000: accuracy:0.09300000220537186, confidence:0.9165065884590149, loss:7.032334804534912
epoch6: step1500/4680
step 12000: accuracy:0.0949999988079071, confidence:0.999740481376648, loss:12.735562324523926
epoch6: step2000/4680
step 15000: accuracy:0.11299999803304672, confidence:0.9998613595962524, loss:12.571394920349121
epoch6: step2500/4680
step 18000: accuracy:0.09799999743700027, confidence:0.9229466319084167, loss:6.482699394226074
epoch6: step3000/4680
step 21000: accuracy:0.11500000208616257, confidence:0.9992729425430298, loss:12.357527732849121
epoch6: step3500/4680
step 24000: accuracy:0.0989999994635582, confidence:0.8754512071609497, loss:4.871752738952637
epoch6: step4000/4680
step 27000: accuracy:0.08799999952316284, confidence:0.9134844541549683, loss:7.270064353942871
epoch6: step4500/4680
step 0: accuracy:0.09200000017881393, confidence:0.9413033127784729, loss:7.513071060180664
epoch7: step0/4680
step 3500: accuracy:0.10999999940395355, confidence:0.9798455238342285, loss:7.7415056228637695
epoch7: step500/4680
step 7000: accuracy:0.057999998331069946, confidence:0.7857193350791931, loss:5.856694221496582
epoch7: step1000/4680
step 10500: accuracy:0.09600000083446503, confidence:0.8888120055198669, loss:7.508395671844482
epoch7: step1500/4680
step 14000: accuracy:0.10700000077486038, confidence:0.9997233152389526, loss:13.681756019592285
epoch7: step2000/4680
step 17500: accuracy:0.08900000154972076, confidence:0.9995256066322327, loss:12.744296073913574
epoch7: step2500/4680
step 21000: accuracy:0.09300000220537186, confidence:0.9236683249473572, loss:7.426080226898193
epoch7: step3000/4680
step 24500: accuracy:0.09700000286102295, confidence:0.9997434020042419, loss:14.515626907348633
epoch7: step3500/4680
step 28000: accuracy:0.10400000214576721, confidence:0.938088059425354, loss:5.893949508666992
epoch7: step4000/4680
step 31500: accuracy:0.11599999666213989, confidence:0.927171528339386, loss:7.35541296005249
epoch7: step4500/4680
step 0: accuracy:0.10700000077486038, confidence:0.9458565711975098, loss:7.63683557510376
epoch8: step0/4680
step 4000: accuracy:0.09399999678134918, confidence:0.9714434146881104, loss:8.263566017150879
epoch8: step500/4680
step 8000: accuracy:0.052000001072883606, confidence:0.8105392456054688, loss:6.5102057456970215
epoch8: step1000/4680
step 12000: accuracy:0.09600000083446503, confidence:0.8580805659294128, loss:6.6001739501953125
epoch8: step1500/4680
step 16000: accuracy:0.09799999743700027, confidence:0.998915433883667, loss:12.71635913848877
epoch8: step2000/4680
step 20000: accuracy:0.11400000005960464, confidence:0.999536395072937, loss:12.493709564208984
epoch8: step2500/4680
step 24000: accuracy:0.08299999684095383, confidence:0.8898160457611084, loss:7.161964416503906
epoch8: step3000/4680
step 28000: accuracy:0.12200000137090683, confidence:0.9999173879623413, loss:15.441259384155273
epoch8: step3500/4680
step 32000: accuracy:0.10400000214576721, confidence:0.7989698052406311, loss:4.836474418640137
epoch8: step4000/4680
step 36000: accuracy:0.09300000220537186, confidence:0.8477658629417419, loss:7.5190348625183105
epoch8: step4500/4680
step 0: accuracy:0.10999999940395355, confidence:0.8836559057235718, loss:7.251805782318115
epoch9: step0/4680
step 4500: accuracy:0.09000000357627869, confidence:0.9721890687942505, loss:8.648881912231445
epoch9: step500/4680
step 9000: accuracy:0.10300000011920929, confidence:0.7927601337432861, loss:6.362092018127441
epoch9: step1000/4680
step 13500: accuracy:0.0820000022649765, confidence:0.7935708165168762, loss:6.250066757202148
epoch9: step1500/4680
step 18000: accuracy:0.10899999737739563, confidence:0.9962365627288818, loss:11.709607124328613
epoch9: step2000/4680
step 22500: accuracy:0.10899999737739563, confidence:0.9993378520011902, loss:13.451728820800781
epoch9: step2500/4680
step 27000: accuracy:0.09700000286102295, confidence:0.9537343382835388, loss:9.176779747009277
epoch9: step3000/4680
step 31500: accuracy:0.10899999737739563, confidence:0.9999223351478577, loss:15.649566650390625
epoch9: step3500/4680
step 36000: accuracy:0.10000000149011612, confidence:0.8693805932998657, loss:5.678310871124268
epoch9: step4000/4680
step 40500: accuracy:0.11900000274181366, confidence:0.8138678073883057, loss:7.446671962738037
epoch9: step4500/4680
step 0: accuracy:0.125, confidence:0.8665940165519714, loss:7.477227687835693
epoch10: step0/4680
step 5000: accuracy:0.07400000095367432, confidence:0.9258770942687988, loss:7.541408061981201
epoch10: step500/4680
step 10000: accuracy:0.06499999761581421, confidence:0.8231258988380432, loss:7.171878814697266
epoch10: step1000/4680
step 15000: accuracy:0.09000000357627869, confidence:0.8608594536781311, loss:7.189285755157471
epoch10: step1500/4680
step 20000: accuracy:0.09300000220537186, confidence:0.9979730248451233, loss:13.133795738220215
epoch10: step2000/4680
step 25000: accuracy:0.09799999743700027, confidence:0.9988939762115479, loss:13.048937797546387
epoch10: step2500/4680
step 30000: accuracy:0.10599999874830246, confidence:0.8293030858039856, loss:7.941422939300537
epoch10: step3000/4680
step 35000: accuracy:0.1080000028014183, confidence:0.9983464479446411, loss:18.310842514038086
epoch10: step3500/4680
step 40000: accuracy:0.0949999988079071, confidence:0.9848356246948242, loss:7.445908069610596
epoch10: step4000/4680
step 45000: accuracy:0.10300000011920929, confidence:0.9249334335327148, loss:7.6723713874816895
epoch10: step4500/4680
step 0: accuracy:0.10899999737739563, confidence:0.9155595898628235, loss:7.4092535972595215
epoch11: step0/4680
step 5500: accuracy:0.08500000089406967, confidence:0.8965106010437012, loss:6.932352066040039
epoch11: step500/4680
step 11000: accuracy:0.03999999910593033, confidence:0.8330165147781372, loss:6.92230224609375
epoch11: step1000/4680
step 16500: accuracy:0.07699999958276749, confidence:0.7930247187614441, loss:6.285627841949463
epoch11: step1500/4680
step 22000: accuracy:0.0860000029206276, confidence:0.9865835309028625, loss:11.953455924987793
epoch11: step2000/4680
step 27500: accuracy:0.11500000208616257, confidence:0.9988827109336853, loss:13.194586753845215
epoch11: step2500/4680
step 33000: accuracy:0.08299999684095383, confidence:0.8530867695808411, loss:8.537590026855469
epoch11: step3000/4680
step 38500: accuracy:0.10100000351667404, confidence:0.9951342344284058, loss:16.77394676208496
epoch11: step3500/4680
step 44000: accuracy:0.10700000077486038, confidence:0.9069569706916809, loss:6.451301097869873
epoch11: step4000/4680
step 49500: accuracy:0.13500000536441803, confidence:0.8475331664085388, loss:8.267477035522461
epoch11: step4500/4680
step 0: accuracy:0.10999999940395355, confidence:0.8999208807945251, loss:8.049907684326172
epoch12: step0/4680
step 6000: accuracy:0.09799999743700027, confidence:0.9565476179122925, loss:8.20021915435791
epoch12: step500/4680
step 12000: accuracy:0.09300000220537186, confidence:0.6526452898979187, loss:5.4847187995910645
epoch12: step1000/4680
step 18000: accuracy:0.032999999821186066, confidence:0.6453766822814941, loss:5.539292335510254
epoch12: step1500/4680
step 24000: accuracy:0.11100000143051147, confidence:0.985983669757843, loss:12.806971549987793
epoch12: step2000/4680
step 30000: accuracy:0.08799999952316284, confidence:0.999203085899353, loss:14.393071174621582
epoch12: step2500/4680
step 36000: accuracy:0.08699999749660492, confidence:0.9042351841926575, loss:10.414520263671875
epoch12: step3000/4680
step 42000: accuracy:0.0989999994635582, confidence:0.9981981515884399, loss:18.481523513793945
epoch12: step3500/4680
step 48000: accuracy:0.07999999821186066, confidence:0.7838492393493652, loss:6.157984733581543
epoch12: step4000/4680
step 54000: accuracy:0.1120000034570694, confidence:0.8670717477798462, loss:9.95905876159668
epoch12: step4500/4680
step 0: accuracy:0.14300000667572021, confidence:0.8939962387084961, loss:8.507783889770508
epoch13: step0/4680
step 6500: accuracy:0.09700000286102295, confidence:0.9282786250114441, loss:7.642605304718018
epoch13: step500/4680
step 13000: accuracy:0.04500000178813934, confidence:0.7895765900611877, loss:6.684062957763672
epoch13: step1000/4680
step 19500: accuracy:0.04600000008940697, confidence:0.6554325222969055, loss:5.3507080078125
epoch13: step1500/4680
step 26000: accuracy:0.09799999743700027, confidence:0.9678345322608948, loss:11.788415908813477
epoch13: step2000/4680
step 32500: accuracy:0.10100000351667404, confidence:0.9994134902954102, loss:14.370912551879883
epoch13: step2500/4680
step 39000: accuracy:0.06300000101327896, confidence:0.8311008214950562, loss:8.796120643615723
epoch13: step3000/4680
step 45500: accuracy:0.12999999523162842, confidence:0.9934676885604858, loss:19.551557540893555
epoch13: step3500/4680
step 52000: accuracy:0.08500000089406967, confidence:0.7449536323547363, loss:6.266739368438721
epoch13: step4000/4680
step 58500: accuracy:0.1289999932050705, confidence:0.8490058779716492, loss:9.166791915893555
epoch13: step4500/4680
step 0: accuracy:0.10999999940395355, confidence:0.8550827503204346, loss:8.302596092224121
epoch14: step0/4680
step 7000: accuracy:0.09099999815225601, confidence:0.9452093243598938, loss:8.209961891174316
epoch14: step500/4680
step 14000: accuracy:0.14800000190734863, confidence:0.6984643340110779, loss:6.096450328826904
epoch14: step1000/4680
step 21000: accuracy:0.04399999976158142, confidence:0.6903795599937439, loss:6.331462860107422
epoch14: step1500/4680
step 28000: accuracy:0.10999999940395355, confidence:0.969019889831543, loss:12.4054536819458
epoch14: step2000/4680
step 35000: accuracy:0.08900000154972076, confidence:0.998705267906189, loss:14.368721961975098
epoch14: step2500/4680
step 42000: accuracy:0.08299999684095383, confidence:0.9002935290336609, loss:11.34997844696045
epoch14: step3000/4680
step 49000: accuracy:0.11400000005960464, confidence:0.999390721321106, loss:20.70291519165039
epoch14: step3500/4680
step 56000: accuracy:0.08699999749660492, confidence:0.6938026547431946, loss:6.888936996459961
epoch14: step4000/4680
step 63000: accuracy:0.1289999932050705, confidence:0.8657258152961731, loss:10.453179359436035
epoch14: step4500/4680
step 0: accuracy:0.11900000274181366, confidence:0.890985906124115, loss:9.539916038513184
epoch15: step0/4680
step 7500: accuracy:0.10100000351667404, confidence:0.9540337920188904, loss:9.429160118103027
epoch15: step500/4680
step 15000: accuracy:0.06400000303983688, confidence:0.7132709622383118, loss:6.516859531402588
epoch15: step1000/4680
step 22500: accuracy:0.06499999761581421, confidence:0.7907840013504028, loss:7.223711967468262
epoch15: step1500/4680
step 30000: accuracy:0.08299999684095383, confidence:0.9740908145904541, loss:12.694548606872559
epoch15: step2000/4680
step 37500: accuracy:0.10499999672174454, confidence:0.996481716632843, loss:14.244043350219727
epoch15: step2500/4680
step 45000: accuracy:0.023000000044703484, confidence:0.8378123044967651, loss:9.27902603149414
epoch15: step3000/4680
step 52500: accuracy:0.12999999523162842, confidence:0.9995343685150146, loss:21.42627716064453
epoch15: step3500/4680
step 60000: accuracy:0.08699999749660492, confidence:0.7523285150527954, loss:6.229053020477295
epoch15: step4000/4680
step 67500: accuracy:0.1340000033378601, confidence:0.8674414157867432, loss:10.561216354370117
epoch15: step4500/4680
step 0: accuracy:0.10599999874830246, confidence:0.8663503527641296, loss:9.649991035461426
epoch16: step0/4680
step 8000: accuracy:0.07599999755620956, confidence:0.9205751419067383, loss:9.720664024353027
epoch16: step500/4680
step 16000: accuracy:0.06199999898672104, confidence:0.7389624118804932, loss:7.278310775756836
epoch16: step1000/4680
step 24000: accuracy:0.057999998331069946, confidence:0.7496682405471802, loss:7.1944684982299805
epoch16: step1500/4680
step 32000: accuracy:0.11100000143051147, confidence:0.9406633377075195, loss:12.518189430236816
epoch16: step2000/4680
step 40000: accuracy:0.10400000214576721, confidence:0.9992409348487854, loss:15.355169296264648
epoch16: step2500/4680
step 48000: accuracy:0.06700000166893005, confidence:0.8747431635856628, loss:10.933834075927734
epoch16: step3000/4680
step 56000: accuracy:0.10000000149011612, confidence:0.9838750958442688, loss:14.319657325744629
epoch16: step3500/4680
step 64000: accuracy:0.11299999803304672, confidence:0.6180919408798218, loss:6.258082866668701
epoch16: step4000/4680
step 72000: accuracy:0.1289999932050705, confidence:0.8970776796340942, loss:10.104753494262695
epoch16: step4500/4680
step 0: accuracy:0.09300000220537186, confidence:0.9251042008399963, loss:10.171883583068848
epoch17: step0/4680
step 8500: accuracy:0.1080000028014183, confidence:0.9655666947364807, loss:10.440967559814453
epoch17: step500/4680
step 17000: accuracy:0.050999999046325684, confidence:0.724609375, loss:7.116184234619141
epoch17: step1000/4680
step 25500: accuracy:0.0560000017285347, confidence:0.6870706081390381, loss:6.737958908081055
epoch17: step1500/4680
step 34000: accuracy:0.11500000208616257, confidence:0.9554128646850586, loss:16.514549255371094
epoch17: step2000/4680
step 42500: accuracy:0.08699999749660492, confidence:0.999406099319458, loss:18.254602432250977
epoch17: step2500/4680
step 51000: accuracy:0.08699999749660492, confidence:0.910537838935852, loss:10.848609924316406
epoch17: step3000/4680
step 59500: accuracy:0.11400000005960464, confidence:0.9276793599128723, loss:16.329498291015625
epoch17: step3500/4680
step 68000: accuracy:0.07699999958276749, confidence:0.6716129183769226, loss:6.138750076293945
epoch17: step4000/4680
step 76500: accuracy:0.12300000339746475, confidence:0.8742126226425171, loss:10.68576431274414
epoch17: step4500/4680
step 0: accuracy:0.10599999874830246, confidence:0.8950868248939514, loss:9.715439796447754
epoch18: step0/4680
step 9000: accuracy:0.08699999749660492, confidence:0.928000271320343, loss:11.111912727355957
epoch18: step500/4680
step 18000: accuracy:0.04699999839067459, confidence:0.8171582221984863, loss:8.38541316986084
epoch18: step1000/4680
step 27000: accuracy:0.0430000014603138, confidence:0.7049763202667236, loss:6.693897247314453
epoch18: step1500/4680
step 36000: accuracy:0.09300000220537186, confidence:0.921909749507904, loss:14.211771011352539
epoch18: step2000/4680
step 45000: accuracy:0.09600000083446503, confidence:0.9994518756866455, loss:17.647066116333008
epoch18: step2500/4680
step 54000: accuracy:0.03099999949336052, confidence:0.8468206524848938, loss:9.417274475097656
epoch18: step3000/4680
step 63000: accuracy:0.1379999965429306, confidence:0.9206925630569458, loss:17.508342742919922
epoch18: step3500/4680
step 72000: accuracy:0.06300000101327896, confidence:0.6850296854972839, loss:6.470769882202148
epoch18: step4000/4680
step 81000: accuracy:0.125, confidence:0.8823708295822144, loss:10.91618824005127
epoch18: step4500/4680
step 0: accuracy:0.11100000143051147, confidence:0.8901225924491882, loss:9.531058311462402
epoch19: step0/4680
step 9500: accuracy:0.10999999940395355, confidence:0.9297307133674622, loss:9.597555160522461
epoch19: step500/4680
step 19000: accuracy:0.09399999678134918, confidence:0.7144807577133179, loss:6.674968242645264
epoch19: step1000/4680
step 28500: accuracy:0.08500000089406967, confidence:0.7403678297996521, loss:6.751114368438721
epoch19: step1500/4680
step 38000: accuracy:0.09200000017881393, confidence:0.93366539478302, loss:12.95193099975586
epoch19: step2000/4680
step 47500: accuracy:0.09000000357627869, confidence:0.9983153939247131, loss:16.214656829833984
epoch19: step2500/4680
step 57000: accuracy:0.032999999821186066, confidence:0.8802034258842468, loss:10.890625953674316
epoch19: step3000/4680
step 66500: accuracy:0.12200000137090683, confidence:0.9409244060516357, loss:19.870132446289062
epoch19: step3500/4680
step 76000: accuracy:0.07100000232458115, confidence:0.7159339189529419, loss:6.584130764007568
epoch19: step4000/4680
step 85500: accuracy:0.11900000274181366, confidence:0.873799741268158, loss:12.782928466796875
epoch19: step4500/4680
step 0: accuracy:0.11299999803304672, confidence:0.8653120994567871, loss:10.618987083435059
epoch20: step0/4680
step 10000: accuracy:0.08299999684095383, confidence:0.9150307774543762, loss:9.595098495483398
epoch20: step500/4680
step 20000: accuracy:0.039000000804662704, confidence:0.8066790699958801, loss:7.298053741455078
epoch20: step1000/4680
step 30000: accuracy:0.04500000178813934, confidence:0.7516112327575684, loss:7.400809288024902
epoch20: step1500/4680
step 40000: accuracy:0.10400000214576721, confidence:0.967129647731781, loss:12.929421424865723
epoch20: step2000/4680
step 50000: accuracy:0.10700000077486038, confidence:0.9969926476478577, loss:15.275753021240234
epoch20: step2500/4680
step 60000: accuracy:0.02800000086426735, confidence:0.8516140580177307, loss:9.823101997375488
epoch20: step3000/4680
step 70000: accuracy:0.09700000286102295, confidence:0.9231916666030884, loss:20.100048065185547
epoch20: step3500/4680
step 80000: accuracy:0.09099999815225601, confidence:0.68388432264328, loss:7.128798007965088
epoch20: step4000/4680
step 90000: accuracy:0.11800000071525574, confidence:0.8871380686759949, loss:12.004044532775879
epoch20: step4500/4680
step 0: accuracy:0.12300000339746475, confidence:0.8809096217155457, loss:10.386798858642578
epoch21: step0/4680
step 10500: accuracy:0.11900000274181366, confidence:0.8746693134307861, loss:8.543011665344238
epoch21: step500/4680
step 21000: accuracy:0.1420000046491623, confidence:0.7167447805404663, loss:6.372927665710449
epoch21: step1000/4680
step 31500: accuracy:0.08500000089406967, confidence:0.7657649517059326, loss:6.7026214599609375
epoch21: step1500/4680
step 42000: accuracy:0.08900000154972076, confidence:0.9435855150222778, loss:13.989990234375
epoch21: step2000/4680
step 52500: accuracy:0.10100000351667404, confidence:0.9971444606781006, loss:15.294149398803711
epoch21: step2500/4680
step 63000: accuracy:0.07599999755620956, confidence:0.8947511315345764, loss:11.978606224060059
epoch21: step3000/4680
step 73500: accuracy:0.11100000143051147, confidence:0.9335210919380188, loss:18.963075637817383
epoch21: step3500/4680
step 84000: accuracy:0.07000000029802322, confidence:0.7177556753158569, loss:7.486545562744141
epoch21: step4000/4680
step 94500: accuracy:0.13899999856948853, confidence:0.8891409039497375, loss:10.74499797821045
epoch21: step4500/4680
step 0: accuracy:0.09600000083446503, confidence:0.9182332754135132, loss:10.253368377685547
epoch22: step0/4680
step 11000: accuracy:0.08500000089406967, confidence:0.9216758012771606, loss:9.495323181152344
epoch22: step500/4680
step 22000: accuracy:0.014000000432133675, confidence:0.8390181660652161, loss:7.6876397132873535
epoch22: step1000/4680
step 33000: accuracy:0.041999999433755875, confidence:0.7115205526351929, loss:6.360860347747803
epoch22: step1500/4680
step 44000: accuracy:0.11299999803304672, confidence:0.9562495350837708, loss:13.286114692687988
epoch22: step2000/4680
step 55000: accuracy:0.10199999809265137, confidence:0.9980649352073669, loss:16.389127731323242
epoch22: step2500/4680
step 66000: accuracy:0.057999998331069946, confidence:0.8755611777305603, loss:9.770759582519531
epoch22: step3000/4680
step 77000: accuracy:0.09000000357627869, confidence:0.9885644912719727, loss:20.277013778686523
epoch22: step3500/4680
step 88000: accuracy:0.10000000149011612, confidence:0.690077543258667, loss:6.830854415893555
epoch22: step4000/4680
step 99000: accuracy:0.12300000339746475, confidence:0.8643060922622681, loss:11.352324485778809
epoch22: step4500/4680
step 0: accuracy:0.11900000274181366, confidence:0.8551549315452576, loss:10.172051429748535
epoch23: step0/4680
step 11500: accuracy:0.07500000298023224, confidence:0.8797946572303772, loss:9.529651641845703
epoch23: step500/4680
step 23000: accuracy:0.14000000059604645, confidence:0.7172904014587402, loss:6.606603622436523
epoch23: step1000/4680
step 34500: accuracy:0.09200000017881393, confidence:0.7491667866706848, loss:6.607845783233643
epoch23: step1500/4680
step 46000: accuracy:0.10599999874830246, confidence:0.9242745637893677, loss:12.075593948364258
epoch23: step2000/4680
step 57500: accuracy:0.09200000017881393, confidence:0.9979267716407776, loss:15.796112060546875
epoch23: step2500/4680
step 69000: accuracy:0.041999999433755875, confidence:0.8560428023338318, loss:10.997239112854004
epoch23: step3000/4680
step 80500: accuracy:0.10000000149011612, confidence:0.9667244553565979, loss:20.240583419799805
epoch23: step3500/4680
step 92000: accuracy:0.08399999886751175, confidence:0.7402361631393433, loss:7.345857620239258
epoch23: step4000/4680
step 103500: accuracy:0.12600000202655792, confidence:0.852328896522522, loss:11.595625877380371
epoch23: step4500/4680
step 0: accuracy:0.10700000077486038, confidence:0.8529381155967712, loss:10.382331848144531
epoch24: step0/4680
step 12000: accuracy:0.04800000041723251, confidence:0.858253538608551, loss:8.986594200134277
epoch24: step500/4680
step 24000: accuracy:0.006000000052154064, confidence:0.9160509705543518, loss:8.869438171386719
epoch24: step1000/4680
step 36000: accuracy:0.050999999046325684, confidence:0.7954622507095337, loss:7.103693962097168
epoch24: step1500/4680
step 48000: accuracy:0.08900000154972076, confidence:0.9487895369529724, loss:12.34404182434082
epoch24: step2000/4680
step 60000: accuracy:0.09799999743700027, confidence:0.994503915309906, loss:15.325000762939453
epoch24: step2500/4680
step 72000: accuracy:0.07800000160932541, confidence:0.9034428596496582, loss:11.120595932006836
epoch24: step3000/4680
step 84000: accuracy:0.13500000536441803, confidence:0.9154388308525085, loss:18.79149627685547
epoch24: step3500/4680
step 96000: accuracy:0.125, confidence:0.7064350843429565, loss:7.141636848449707
epoch24: step4000/4680
step 108000: accuracy:0.1120000034570694, confidence:0.8866167664527893, loss:9.517709732055664
epoch24: step4500/4680
step 0: accuracy:0.10599999874830246, confidence:0.8743236660957336, loss:9.078865051269531
epoch25: step0/4680
step 12500: accuracy:0.10300000011920929, confidence:0.8505326509475708, loss:7.9572248458862305
epoch25: step500/4680
step 25000: accuracy:0.11599999666213989, confidence:0.7133658528327942, loss:6.655729293823242
epoch25: step1000/4680
step 37500: accuracy:0.06499999761581421, confidence:0.7543776631355286, loss:7.0341715812683105
epoch25: step1500/4680
step 50000: accuracy:0.09600000083446503, confidence:0.9126822352409363, loss:12.161300659179688
epoch25: step2000/4680
step 62500: accuracy:0.09000000357627869, confidence:0.9931383728981018, loss:14.708499908447266
epoch25: step2500/4680
step 75000: accuracy:0.039000000804662704, confidence:0.8217898011207581, loss:9.576838493347168
epoch25: step3000/4680
step 87500: accuracy:0.09799999743700027, confidence:0.921643078327179, loss:21.64607810974121
epoch25: step3500/4680
step 100000: accuracy:0.057999998331069946, confidence:0.7337842583656311, loss:6.828657627105713
epoch25: step4000/4680
step 112500: accuracy:0.13300000131130219, confidence:0.8625661730766296, loss:11.118067741394043
epoch25: step4500/4680
step 0: accuracy:0.10700000077486038, confidence:0.8572817444801331, loss:10.352774620056152
epoch26: step0/4680
step 13000: accuracy:0.07800000160932541, confidence:0.8297246694564819, loss:8.388880729675293
epoch26: step500/4680
step 26000: accuracy:0.023000000044703484, confidence:0.8398959636688232, loss:7.064842700958252
epoch26: step1000/4680
step 39000: accuracy:0.04100000113248825, confidence:0.7619906663894653, loss:6.432774543762207
epoch26: step1500/4680
step 52000: accuracy:0.08500000089406967, confidence:0.9365347027778625, loss:11.730208396911621
epoch26: step2000/4680
step 65000: accuracy:0.11400000005960464, confidence:0.995235025882721, loss:14.928609848022461
epoch26: step2500/4680
step 78000: accuracy:0.03700000047683716, confidence:0.854869544506073, loss:10.295907974243164
epoch26: step3000/4680
step 91000: accuracy:0.10199999809265137, confidence:0.9107812643051147, loss:18.309553146362305
epoch26: step3500/4680
step 104000: accuracy:0.1120000034570694, confidence:0.6998921036720276, loss:7.623176574707031
epoch26: step4000/4680
step 117000: accuracy:0.12399999797344208, confidence:0.8782384395599365, loss:11.917535781860352
epoch26: step4500/4680
step 0: accuracy:0.11800000071525574, confidence:0.864669680595398, loss:10.280412673950195
epoch27: step0/4680
step 13500: accuracy:0.09000000357627869, confidence:0.7883426547050476, loss:8.004366874694824
epoch27: step500/4680
step 27000: accuracy:0.12099999934434891, confidence:0.7207970023155212, loss:6.645263671875
epoch27: step1000/4680
step 40500: accuracy:0.09700000286102295, confidence:0.7814893126487732, loss:6.774868488311768
epoch27: step1500/4680
step 54000: accuracy:0.09700000286102295, confidence:0.9211301207542419, loss:11.905879020690918
epoch27: step2000/4680
step 67500: accuracy:0.08399999886751175, confidence:0.9948289394378662, loss:14.868671417236328
epoch27: step2500/4680
step 81000: accuracy:0.0430000014603138, confidence:0.8568697571754456, loss:9.905957221984863
epoch27: step3000/4680
step 94500: accuracy:0.09399999678134918, confidence:0.9861883521080017, loss:21.026081085205078
epoch27: step3500/4680
step 108000: accuracy:0.10499999672174454, confidence:0.7094298601150513, loss:7.529552936553955
epoch27: step4000/4680
step 121500: accuracy:0.11500000208616257, confidence:0.8583630323410034, loss:10.599440574645996
epoch27: step4500/4680
step 0: accuracy:0.13600000739097595, confidence:0.8553322553634644, loss:9.526557922363281
epoch28: step0/4680
step 14000: accuracy:0.08799999952316284, confidence:0.9070678949356079, loss:9.553930282592773
epoch28: step500/4680
step 28000: accuracy:0.11800000071525574, confidence:0.6918838620185852, loss:6.226174831390381
epoch28: step1000/4680
step 42000: accuracy:0.04699999839067459, confidence:0.6926215291023254, loss:6.163941860198975
epoch28: step1500/4680
step 56000: accuracy:0.10199999809265137, confidence:0.9531804919242859, loss:13.411795616149902
epoch28: step2000/4680
step 70000: accuracy:0.10499999672174454, confidence:0.9928882122039795, loss:15.995984077453613
epoch28: step2500/4680
step 84000: accuracy:0.08500000089406967, confidence:0.8950998783111572, loss:10.120953559875488
epoch28: step3000/4680
step 98000: accuracy:0.10199999809265137, confidence:0.9017106294631958, loss:18.09303855895996
epoch28: step3500/4680
step 112000: accuracy:0.1120000034570694, confidence:0.6864171624183655, loss:6.764526844024658
epoch28: step4000/4680
step 126000: accuracy:0.10599999874830246, confidence:0.8746402859687805, loss:10.748730659484863
epoch28: step4500/4680
step 0: accuracy:0.10199999809265137, confidence:0.8597725629806519, loss:9.998146057128906
epoch29: step0/4680
step 14500: accuracy:0.04600000008940697, confidence:0.8314406275749207, loss:8.242630004882812
epoch29: step500/4680
step 29000: accuracy:0.017000000923871994, confidence:0.8888742923736572, loss:8.812553405761719
epoch29: step1000/4680
step 43500: accuracy:0.03999999910593033, confidence:0.7622921466827393, loss:7.14732027053833
epoch29: step1500/4680
step 58000: accuracy:0.09099999815225601, confidence:0.939590334892273, loss:12.58051872253418
epoch29: step2000/4680
step 72500: accuracy:0.08799999952316284, confidence:0.9950058460235596, loss:15.167551040649414
epoch29: step2500/4680
step 87000: accuracy:0.05999999865889549, confidence:0.8435462117195129, loss:8.909278869628906
epoch29: step3000/4680
step 101500: accuracy:0.11299999803304672, confidence:0.9488738179206848, loss:19.15496063232422
epoch29: step3500/4680
step 116000: accuracy:0.0989999994635582, confidence:0.7786263227462769, loss:7.7061767578125
epoch29: step4000/4680
step 130500: accuracy:0.11900000274181366, confidence:0.8825283646583557, loss:11.6150484085083
epoch29: step4500/4680
2018-06-15 20:01:19.433774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000a:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 20:01:19.433958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
step 0: accuracy:0.1080000028014183, confidence:0.9990571737289429, loss:12.527783393859863
epoch0: step0/4680
step 0: accuracy:0.08900000154972076, confidence:1.0, loss:30.28561019897461
epoch0: step500/4680
step 0: accuracy:0.10999999940395355, confidence:1.0, loss:36.43738555908203
epoch0: step1000/4680
step 0: accuracy:0.10400000214576721, confidence:0.999997079372406, loss:16.525169372558594
epoch0: step1500/4680
step 0: accuracy:0.08699999749660492, confidence:0.9997598528862, loss:10.647130966186523
epoch0: step2000/4680
step 0: accuracy:0.10199999809265137, confidence:0.9999488592147827, loss:11.218831062316895
epoch0: step2500/4680
step 0: accuracy:0.07900000363588333, confidence:0.9994696378707886, loss:9.856199264526367
epoch0: step3000/4680
step 0: accuracy:0.1080000028014183, confidence:0.9952510595321655, loss:7.849954128265381
epoch0: step3500/4680
step 0: accuracy:0.09200000017881393, confidence:0.9972030520439148, loss:7.783123016357422
epoch0: step4000/4680
step 0: accuracy:0.11999999731779099, confidence:0.9995332956314087, loss:9.819746017456055
epoch0: step4500/4680
step 0: accuracy:0.11500000208616257, confidence:0.9995866417884827, loss:9.861444473266602
epoch1: step0/4680
step 500: accuracy:0.08500000089406967, confidence:0.999965250492096, loss:12.377578735351562
epoch1: step500/4680
step 1000: accuracy:0.09700000286102295, confidence:0.9823912978172302, loss:6.276671886444092
epoch1: step1000/4680
step 1500: accuracy:0.0860000029206276, confidence:0.9958022832870483, loss:8.428152084350586
epoch1: step1500/4680
step 2000: accuracy:0.0949999988079071, confidence:0.9990226030349731, loss:9.407811164855957
epoch1: step2000/4680
step 2500: accuracy:0.10599999874830246, confidence:0.9992590546607971, loss:9.106040954589844
epoch1: step2500/4680
step 3000: accuracy:0.10400000214576721, confidence:0.9995526075363159, loss:9.462850570678711
epoch1: step3000/4680
step 3500: accuracy:0.10499999672174454, confidence:0.999171257019043, loss:9.257213592529297
epoch1: step3500/4680
step 4000: accuracy:0.12200000137090683, confidence:0.9927994012832642, loss:6.7180399894714355
epoch1: step4000/4680
step 4500: accuracy:0.1289999932050705, confidence:0.9989146590232849, loss:8.447044372558594
epoch1: step4500/4680
step 0: accuracy:0.11500000208616257, confidence:0.9991648197174072, loss:8.790201187133789
epoch2: step0/4680
step 1000: accuracy:0.09200000017881393, confidence:0.9998471736907959, loss:10.701533317565918
epoch2: step500/4680
step 2000: accuracy:0.10100000351667404, confidence:0.9960182905197144, loss:7.853907108306885
epoch2: step1000/4680
step 3000: accuracy:0.09799999743700027, confidence:0.9832913875579834, loss:6.715994358062744
epoch2: step1500/4680
step 4000: accuracy:0.0949999988079071, confidence:0.9991424679756165, loss:9.595037460327148
epoch2: step2000/4680
step 5000: accuracy:0.11400000005960464, confidence:0.9998026490211487, loss:10.2089262008667
epoch2: step2500/4680
step 6000: accuracy:0.0949999988079071, confidence:0.9996055960655212, loss:9.62965202331543
epoch2: step3000/4680
step 7000: accuracy:0.09399999678134918, confidence:0.999426007270813, loss:10.183732986450195
epoch2: step3500/4680
step 8000: accuracy:0.0949999988079071, confidence:0.9912334084510803, loss:6.708583354949951
epoch2: step4000/4680
step 9000: accuracy:0.11400000005960464, confidence:0.9975734949111938, loss:7.899168491363525
epoch2: step4500/4680
step 0: accuracy:0.10599999874830246, confidence:0.9986469745635986, loss:8.458891868591309
epoch3: step0/4680
step 1500: accuracy:0.08399999886751175, confidence:0.998235285282135, loss:8.254931449890137
epoch3: step500/4680
step 3000: accuracy:0.08299999684095383, confidence:0.7534672021865845, loss:3.9048430919647217
epoch3: step1000/4680
step 4500: accuracy:0.11500000208616257, confidence:0.9993017315864563, loss:9.276657104492188
epoch3: step1500/4680
step 6000: accuracy:0.10400000214576721, confidence:0.9951393604278564, loss:7.812228202819824
epoch3: step2000/4680
step 7500: accuracy:0.09799999743700027, confidence:0.9963802099227905, loss:8.057456970214844
epoch3: step2500/4680
step 9000: accuracy:0.0989999994635582, confidence:0.9951195120811462, loss:7.560458183288574
epoch3: step3000/4680
step 10500: accuracy:0.08699999749660492, confidence:0.9592621922492981, loss:6.116738319396973
epoch3: step3500/4680
step 12000: accuracy:0.1080000028014183, confidence:0.9890623688697815, loss:6.7523369789123535
epoch3: step4000/4680
step 13500: accuracy:0.10300000011920929, confidence:0.9779433608055115, loss:6.059995174407959
epoch3: step4500/4680
step 0: accuracy:0.11400000005960464, confidence:0.9877964854240417, loss:6.5169830322265625
epoch4: step0/4680
step 2000: accuracy:0.09600000083446503, confidence:0.9980184435844421, loss:8.22191047668457
epoch4: step500/4680
step 4000: accuracy:0.10599999874830246, confidence:0.5864758491516113, loss:3.3960819244384766
epoch4: step1000/4680
step 6000: accuracy:0.10100000351667404, confidence:0.9983554482460022, loss:9.3608980178833
epoch4: step1500/4680
step 8000: accuracy:0.09700000286102295, confidence:0.9498494267463684, loss:5.511011600494385
epoch4: step2000/4680
step 10000: accuracy:0.10599999874830246, confidence:0.9962274432182312, loss:8.339604377746582
epoch4: step2500/4680
step 12000: accuracy:0.09600000083446503, confidence:0.9839711785316467, loss:7.0231146812438965
epoch4: step3000/4680
step 14000: accuracy:0.08900000154972076, confidence:0.924363911151886, loss:6.089062690734863
epoch4: step3500/4680
step 16000: accuracy:0.1080000028014183, confidence:0.9763222336769104, loss:6.492473125457764
epoch4: step4000/4680
step 18000: accuracy:0.12099999934434891, confidence:0.9737187623977661, loss:6.021954536437988
epoch4: step4500/4680
step 0: accuracy:0.1289999932050705, confidence:0.9943510890007019, loss:7.35930871963501
epoch5: step0/4680
step 2500: accuracy:0.10199999809265137, confidence:0.9867491722106934, loss:6.9836812019348145
epoch5: step500/4680
step 5000: accuracy:0.09000000357627869, confidence:0.626948893070221, loss:3.5065813064575195
epoch5: step1000/4680
step 7500: accuracy:0.0949999988079071, confidence:0.9848883152008057, loss:8.047154426574707
epoch5: step1500/4680
step 10000: accuracy:0.09799999743700027, confidence:0.922761082649231, loss:5.300220489501953
epoch5: step2000/4680
step 12500: accuracy:0.10499999672174454, confidence:0.9894111156463623, loss:8.663446426391602
epoch5: step2500/4680
step 15000: accuracy:0.0860000029206276, confidence:0.9578045606613159, loss:6.82028341293335
epoch5: step3000/4680
step 17500: accuracy:0.12099999934434891, confidence:0.6286548972129822, loss:4.175290107727051
epoch5: step3500/4680
step 20000: accuracy:0.10199999809265137, confidence:0.9668551087379456, loss:6.979522228240967
epoch5: step4000/4680
step 22500: accuracy:0.1120000034570694, confidence:0.9618198871612549, loss:6.131315231323242
epoch5: step4500/4680
step 0: accuracy:0.09600000083446503, confidence:0.9877299070358276, loss:7.61712646484375
epoch6: step0/4680
step 3000: accuracy:0.07800000160932541, confidence:0.9917853474617004, loss:8.20409107208252
epoch6: step500/4680
step 6000: accuracy:0.10400000214576721, confidence:0.5869729518890381, loss:3.627743721008301
epoch6: step1000/4680
step 9000: accuracy:0.09300000220537186, confidence:0.9883542656898499, loss:8.324172019958496
epoch6: step1500/4680
step 12000: accuracy:0.10499999672174454, confidence:0.9448288083076477, loss:6.350020408630371
epoch6: step2000/4680
step 15000: accuracy:0.11299999803304672, confidence:0.9859912395477295, loss:9.685888290405273
epoch6: step2500/4680
step 18000: accuracy:0.09399999678134918, confidence:0.9346405267715454, loss:7.211047649383545
epoch6: step3000/4680
step 21000: accuracy:0.11800000071525574, confidence:0.5938314199447632, loss:4.065349578857422
epoch6: step3500/4680
step 24000: accuracy:0.0989999994635582, confidence:0.9807719588279724, loss:8.370868682861328
epoch6: step4000/4680
step 27000: accuracy:0.12700000405311584, confidence:0.9559891819953918, loss:6.291804313659668
epoch6: step4500/4680
step 0: accuracy:0.10300000011920929, confidence:0.9772025942802429, loss:7.342891216278076
epoch7: step0/4680
step 3500: accuracy:0.0989999994635582, confidence:0.9989860653877258, loss:10.852991104125977
epoch7: step500/4680
step 7000: accuracy:0.12399999797344208, confidence:0.7060912251472473, loss:4.214544773101807
epoch7: step1000/4680
step 10500: accuracy:0.10000000149011612, confidence:0.9928435683250427, loss:9.534018516540527
epoch7: step1500/4680
step 14000: accuracy:0.10300000011920929, confidence:0.9280927777290344, loss:6.126888751983643
epoch7: step2000/4680
step 17500: accuracy:0.08900000154972076, confidence:0.9427829384803772, loss:8.680119514465332
epoch7: step2500/4680
step 21000: accuracy:0.1080000028014183, confidence:0.927888810634613, loss:7.675853729248047
epoch7: step3000/4680
step 24500: accuracy:0.11800000071525574, confidence:0.5946275591850281, loss:4.119178771972656
epoch7: step3500/4680
step 28000: accuracy:0.10499999672174454, confidence:0.9269744753837585, loss:6.970956802368164
epoch7: step4000/4680
step 31500: accuracy:0.12099999934434891, confidence:0.9537501931190491, loss:6.654361724853516
epoch7: step4500/4680
step 0: accuracy:0.13699999451637268, confidence:0.9561744332313538, loss:6.513574600219727
epoch8: step0/4680
step 4000: accuracy:0.10000000149011612, confidence:0.9972953200340271, loss:10.379572868347168
epoch8: step500/4680
step 8000: accuracy:0.1080000028014183, confidence:0.7043824195861816, loss:4.546988487243652
epoch8: step1000/4680
step 12000: accuracy:0.11800000071525574, confidence:0.9081418514251709, loss:6.7112040519714355
epoch8: step1500/4680
step 16000: accuracy:0.07800000160932541, confidence:0.8137637972831726, loss:5.5382537841796875
epoch8: step2000/4680
step 20000: accuracy:0.11400000005960464, confidence:0.968370795249939, loss:10.047056198120117
epoch8: step2500/4680
step 24000: accuracy:0.08299999684095383, confidence:0.9212262034416199, loss:7.770662307739258
epoch8: step3000/4680
step 28000: accuracy:0.12399999797344208, confidence:0.5929189920425415, loss:4.043248653411865
epoch8: step3500/4680
step 32000: accuracy:0.10599999874830246, confidence:0.960942268371582, loss:8.248514175415039
epoch8: step4000/4680
step 36000: accuracy:0.09600000083446503, confidence:0.9615106582641602, loss:7.306381702423096
epoch8: step4500/4680
step 0: accuracy:0.11800000071525574, confidence:0.9650365114212036, loss:7.253046989440918
epoch9: step0/4680
step 4500: accuracy:0.1080000028014183, confidence:0.997139573097229, loss:10.411857604980469
epoch9: step500/4680
step 9000: accuracy:0.08500000089406967, confidence:0.8901886343955994, loss:5.911632537841797
epoch9: step1000/4680
step 13500: accuracy:0.08699999749660492, confidence:0.988032341003418, loss:8.951263427734375
epoch9: step1500/4680
step 18000: accuracy:0.07000000029802322, confidence:0.8762907981872559, loss:5.875707626342773
epoch9: step2000/4680
step 22500: accuracy:0.10400000214576721, confidence:0.9357947707176208, loss:8.819535255432129
epoch9: step2500/4680
step 27000: accuracy:0.09000000357627869, confidence:0.9022990465164185, loss:7.528998851776123
epoch9: step3000/4680
step 31500: accuracy:0.11400000005960464, confidence:0.6366699934005737, loss:4.426191806793213
epoch9: step3500/4680
step 36000: accuracy:0.10000000149011612, confidence:0.9542515873908997, loss:8.537277221679688
epoch9: step4000/4680
step 40500: accuracy:0.09799999743700027, confidence:0.9419566988945007, loss:6.618821620941162
epoch9: step4500/4680
step 0: accuracy:0.09799999743700027, confidence:0.9622348546981812, loss:7.343198299407959
epoch10: step0/4680
step 5000: accuracy:0.08500000089406967, confidence:0.9834120273590088, loss:9.817850112915039
epoch10: step500/4680
step 10000: accuracy:0.08399999886751175, confidence:0.9283446669578552, loss:7.137046813964844
epoch10: step1000/4680
step 15000: accuracy:0.10499999672174454, confidence:0.9856650233268738, loss:9.370819091796875
epoch10: step1500/4680
step 20000: accuracy:0.07000000029802322, confidence:0.8231433033943176, loss:5.848813056945801
epoch10: step2000/4680
step 25000: accuracy:0.09799999743700027, confidence:0.9453156590461731, loss:9.553791999816895
epoch10: step2500/4680
step 30000: accuracy:0.11400000005960464, confidence:0.9261726140975952, loss:7.8612470626831055
epoch10: step3000/4680
step 35000: accuracy:0.10000000149011612, confidence:0.6439294815063477, loss:4.550183296203613
epoch10: step3500/4680
step 40000: accuracy:0.10000000149011612, confidence:0.8773607015609741, loss:7.448812961578369
epoch10: step4000/4680
step 45000: accuracy:0.1120000034570694, confidence:0.991820752620697, loss:10.227923393249512
epoch10: step4500/4680
step 0: accuracy:0.09099999815225601, confidence:0.9879571795463562, loss:9.795639038085938
epoch11: step0/4680
step 5500: accuracy:0.09099999815225601, confidence:0.9891521334648132, loss:9.341486930847168
epoch11: step500/4680
step 11000: accuracy:0.07599999755620956, confidence:0.9921148419380188, loss:9.651470184326172
epoch11: step1000/4680
step 16500: accuracy:0.10599999874830246, confidence:0.996548593044281, loss:10.114350318908691
epoch11: step1500/4680
step 22000: accuracy:0.09700000286102295, confidence:0.8825874924659729, loss:6.420192241668701
epoch11: step2000/4680
step 27500: accuracy:0.1120000034570694, confidence:0.9248464107513428, loss:8.498748779296875
epoch11: step2500/4680
step 33000: accuracy:0.0989999994635582, confidence:0.9013956189155579, loss:7.345256805419922
epoch11: step3000/4680
step 38500: accuracy:0.10999999940395355, confidence:0.6269824504852295, loss:4.676840305328369
epoch11: step3500/4680
step 44000: accuracy:0.10999999940395355, confidence:0.9178664088249207, loss:7.851860523223877
epoch11: step4000/4680
step 49500: accuracy:0.11999999731779099, confidence:0.9359803795814514, loss:6.511688709259033
epoch11: step4500/4680
step 0: accuracy:0.11500000208616257, confidence:0.9512075185775757, loss:7.309787273406982
epoch12: step0/4680
step 6000: accuracy:0.08699999749660492, confidence:0.9995025396347046, loss:12.168810844421387
epoch12: step500/4680
step 12000: accuracy:0.10400000214576721, confidence:0.8108879327774048, loss:5.420980453491211
epoch12: step1000/4680
step 18000: accuracy:0.08799999952316284, confidence:0.9556481242179871, loss:9.122222900390625
epoch12: step1500/4680
step 24000: accuracy:0.061000000685453415, confidence:0.8139035701751709, loss:6.210214138031006
epoch12: step2000/4680
step 30000: accuracy:0.08799999952316284, confidence:0.9525713324546814, loss:10.833539962768555
epoch12: step2500/4680
step 36000: accuracy:0.11599999666213989, confidence:0.9275408983230591, loss:8.326144218444824
epoch12: step3000/4680
step 42000: accuracy:0.14300000667572021, confidence:0.6590709090232849, loss:4.776755332946777
epoch12: step3500/4680
step 48000: accuracy:0.0989999994635582, confidence:0.8829716444015503, loss:7.407986164093018
epoch12: step4000/4680
step 54000: accuracy:0.10999999940395355, confidence:0.9797921180725098, loss:9.354243278503418
epoch12: step4500/4680
step 0: accuracy:0.10899999737739563, confidence:0.9743306636810303, loss:8.97412395477295
epoch13: step0/4680
step 6500: accuracy:0.10499999672174454, confidence:0.991020143032074, loss:9.897393226623535
epoch13: step500/4680
step 13000: accuracy:0.08699999749660492, confidence:0.9817460775375366, loss:10.328920364379883
epoch13: step1000/4680
step 19500: accuracy:0.09200000017881393, confidence:0.9957942366600037, loss:10.625215530395508
epoch13: step1500/4680
step 26000: accuracy:0.11299999803304672, confidence:0.9118862152099609, loss:7.387485504150391
epoch13: step2000/4680
step 32500: accuracy:0.10400000214576721, confidence:0.9177441000938416, loss:9.777200698852539
epoch13: step2500/4680
step 39000: accuracy:0.12200000137090683, confidence:0.8888596892356873, loss:7.9467267990112305
epoch13: step3000/4680
step 45500: accuracy:0.09799999743700027, confidence:0.7250916957855225, loss:5.352804183959961
epoch13: step3500/4680
step 52000: accuracy:0.10499999672174454, confidence:0.9315643906593323, loss:8.807042121887207
epoch13: step4000/4680
step 58500: accuracy:0.10300000011920929, confidence:0.9388294219970703, loss:7.535147666931152
epoch13: step4500/4680
step 0: accuracy:0.10999999940395355, confidence:0.9605982303619385, loss:8.20368480682373
epoch14: step0/4680
step 7000: accuracy:0.09300000220537186, confidence:0.9987753629684448, loss:13.44981861114502
epoch14: step500/4680
step 14000: accuracy:0.09099999815225601, confidence:0.8598465323448181, loss:6.510677337646484
epoch14: step1000/4680
step 21000: accuracy:0.09600000083446503, confidence:0.9704393148422241, loss:9.3351469039917
epoch14: step1500/4680
step 28000: accuracy:0.06400000303983688, confidence:0.8222857117652893, loss:6.761297225952148
epoch14: step2000/4680
step 35000: accuracy:0.08900000154972076, confidence:0.9566748738288879, loss:11.110198974609375
epoch14: step2500/4680
step 42000: accuracy:0.12200000137090683, confidence:0.9269355535507202, loss:8.411507606506348
epoch14: step3000/4680
step 49000: accuracy:0.10199999809265137, confidence:0.6376031637191772, loss:4.736999034881592
epoch14: step3500/4680
step 56000: accuracy:0.07699999958276749, confidence:0.9050366282463074, loss:8.568123817443848
epoch14: step4000/4680
step 63000: accuracy:0.11400000005960464, confidence:0.9758561253547668, loss:9.8681058883667
epoch14: step4500/4680
step 0: accuracy:0.10199999809265137, confidence:0.9680182337760925, loss:9.616817474365234
epoch15: step0/4680
step 7500: accuracy:0.09000000357627869, confidence:0.988632321357727, loss:10.419276237487793
epoch15: step500/4680
step 15000: accuracy:0.09300000220537186, confidence:0.9757902026176453, loss:10.499860763549805
epoch15: step1000/4680
step 22500: accuracy:0.0949999988079071, confidence:0.9650168418884277, loss:9.391797065734863
epoch15: step1500/4680
step 30000: accuracy:0.08399999886751175, confidence:0.865902841091156, loss:6.722570419311523
epoch15: step2000/4680
step 37500: accuracy:0.10499999672174454, confidence:0.9485524892807007, loss:10.159757614135742
epoch15: step2500/4680
step 45000: accuracy:0.08399999886751175, confidence:0.8974844813346863, loss:9.301094055175781
epoch15: step3000/4680
step 52500: accuracy:0.11999999731779099, confidence:0.6824228763580322, loss:5.241742134094238
epoch15: step3500/4680
step 60000: accuracy:0.0949999988079071, confidence:0.9144363403320312, loss:8.374144554138184
epoch15: step4000/4680
step 67500: accuracy:0.0989999994635582, confidence:0.9282178282737732, loss:7.785645484924316
epoch15: step4500/4680
step 0: accuracy:0.1120000034570694, confidence:0.9418919086456299, loss:8.000044822692871
epoch16: step0/4680
step 8000: accuracy:0.07999999821186066, confidence:0.9937971830368042, loss:11.228416442871094
epoch16: step500/4680
step 16000: accuracy:0.08799999952316284, confidence:0.8922073841094971, loss:7.116346836090088
epoch16: step1000/4680
step 24000: accuracy:0.09099999815225601, confidence:0.9328796863555908, loss:8.887170791625977
epoch16: step1500/4680
step 32000: accuracy:0.04699999839067459, confidence:0.8140095472335815, loss:6.847230911254883
epoch16: step2000/4680
step 40000: accuracy:0.10400000214576721, confidence:0.9587706923484802, loss:11.106070518493652
epoch16: step2500/4680
step 48000: accuracy:0.11100000143051147, confidence:0.900459885597229, loss:8.168766975402832
epoch16: step3000/4680
step 56000: accuracy:0.0989999994635582, confidence:0.6575093865394592, loss:4.896089553833008
epoch16: step3500/4680
step 64000: accuracy:0.09600000083446503, confidence:0.8747086524963379, loss:7.941513538360596
epoch16: step4000/4680
step 72000: accuracy:0.11299999803304672, confidence:0.9852592945098877, loss:10.672813415527344
epoch16: step4500/4680
step 0: accuracy:0.12999999523162842, confidence:0.9750797152519226, loss:9.823315620422363
epoch17: step0/4680
step 8500: accuracy:0.08799999952316284, confidence:0.9842577576637268, loss:9.862215042114258
epoch17: step500/4680
step 17000: accuracy:0.09700000286102295, confidence:0.986312747001648, loss:11.064635276794434
epoch17: step1000/4680
step 25500: accuracy:0.11500000208616257, confidence:0.980137050151825, loss:9.790940284729004
epoch17: step1500/4680
step 34000: accuracy:0.09300000220537186, confidence:0.8955111503601074, loss:7.791884899139404
epoch17: step2000/4680
step 42500: accuracy:0.08699999749660492, confidence:0.9579287171363831, loss:11.40738582611084
epoch17: step2500/4680
step 51000: accuracy:0.08500000089406967, confidence:0.8890145421028137, loss:8.993489265441895
epoch17: step3000/4680
step 59500: accuracy:0.0949999988079071, confidence:0.727726936340332, loss:5.686800479888916
epoch17: step3500/4680
step 68000: accuracy:0.10499999672174454, confidence:0.8955337405204773, loss:8.168317794799805
epoch17: step4000/4680
step 76500: accuracy:0.10999999940395355, confidence:0.9295436143875122, loss:7.719546318054199
epoch17: step4500/4680
step 0: accuracy:0.10700000077486038, confidence:0.9450367093086243, loss:8.215350151062012
epoch18: step0/4680
step 9000: accuracy:0.1080000028014183, confidence:0.9997028112411499, loss:13.465895652770996
epoch18: step500/4680
step 18000: accuracy:0.07900000363588333, confidence:0.8401517271995544, loss:6.6369524002075195
epoch18: step1000/4680
step 27000: accuracy:0.09700000286102295, confidence:0.9313792586326599, loss:9.670909881591797
epoch18: step1500/4680
step 36000: accuracy:0.061000000685453415, confidence:0.8077835440635681, loss:7.304415702819824
epoch18: step2000/4680
step 45000: accuracy:0.0949999988079071, confidence:0.9296789765357971, loss:10.531264305114746
epoch18: step2500/4680
step 54000: accuracy:0.08399999886751175, confidence:0.9230116009712219, loss:9.410534858703613
epoch18: step3000/4680
step 63000: accuracy:0.11699999868869781, confidence:0.691188633441925, loss:5.1355509757995605
epoch18: step3500/4680
step 72000: accuracy:0.10499999672174454, confidence:0.877820611000061, loss:8.194337844848633
epoch18: step4000/4680
step 81000: accuracy:0.0989999994635582, confidence:0.9683940410614014, loss:10.574356079101562
epoch18: step4500/4680
step 0: accuracy:0.12700000405311584, confidence:0.9651337265968323, loss:9.901570320129395
epoch19: step0/4680
step 9500: accuracy:0.08900000154972076, confidence:0.9901270866394043, loss:10.855341911315918
epoch19: step500/4680
step 19000: accuracy:0.09099999815225601, confidence:0.9823900461196899, loss:11.664563179016113
epoch19: step1000/4680
step 28500: accuracy:0.11599999666213989, confidence:0.9773429036140442, loss:9.892457008361816
epoch19: step1500/4680
step 38000: accuracy:0.08900000154972076, confidence:0.8647223114967346, loss:7.3313984870910645
epoch19: step2000/4680
step 47500: accuracy:0.09000000357627869, confidence:0.9870018362998962, loss:12.61388874053955
epoch19: step2500/4680
step 57000: accuracy:0.10100000351667404, confidence:0.8768215179443359, loss:8.782933235168457
epoch19: step3000/4680
step 66500: accuracy:0.09099999815225601, confidence:0.708901584148407, loss:5.799628734588623
epoch19: step3500/4680
step 76000: accuracy:0.09600000083446503, confidence:0.8971036672592163, loss:8.06203556060791
epoch19: step4000/4680
step 85500: accuracy:0.09600000083446503, confidence:0.9218377470970154, loss:7.930807590484619
epoch19: step4500/4680
step 0: accuracy:0.11800000071525574, confidence:0.929834246635437, loss:8.274503707885742
epoch20: step0/4680
step 10000: accuracy:0.0860000029206276, confidence:0.9909344911575317, loss:11.815116882324219
epoch20: step500/4680
step 20000: accuracy:0.09300000220537186, confidence:0.9574375152587891, loss:8.925274848937988
epoch20: step1000/4680
step 30000: accuracy:0.10199999809265137, confidence:0.8581846952438354, loss:8.305270195007324
epoch20: step1500/4680
step 40000: accuracy:0.04800000041723251, confidence:0.7756673693656921, loss:7.113434791564941
epoch20: step2000/4680
step 50000: accuracy:0.10700000077486038, confidence:0.9487362504005432, loss:10.71028995513916
epoch20: step2500/4680
step 60000: accuracy:0.11500000208616257, confidence:0.925028920173645, loss:9.300755500793457
epoch20: step3000/4680
step 70000: accuracy:0.11999999731779099, confidence:0.6768994331359863, loss:5.339280128479004
epoch20: step3500/4680
step 80000: accuracy:0.10300000011920929, confidence:0.8490990400314331, loss:7.728538513183594
epoch20: step4000/4680
step 90000: accuracy:0.11599999666213989, confidence:0.9725460410118103, loss:10.819856643676758
epoch20: step4500/4680
step 0: accuracy:0.1120000034570694, confidence:0.9747692942619324, loss:10.112247467041016
epoch21: step0/4680
step 10500: accuracy:0.08500000089406967, confidence:0.972949743270874, loss:9.811264991760254
epoch21: step500/4680
step 21000: accuracy:0.09300000220537186, confidence:0.9766141176223755, loss:11.748169898986816
epoch21: step1000/4680
step 31500: accuracy:0.12200000137090683, confidence:0.9676004648208618, loss:9.654294967651367
epoch21: step1500/4680
step 42000: accuracy:0.08900000154972076, confidence:0.86473548412323, loss:7.592843532562256
epoch21: step2000/4680
step 52500: accuracy:0.10100000351667404, confidence:0.9315525889396667, loss:10.663519859313965
epoch21: step2500/4680
step 63000: accuracy:0.08399999886751175, confidence:0.8943098187446594, loss:9.157472610473633
epoch21: step3000/4680
step 73500: accuracy:0.08900000154972076, confidence:0.7740416526794434, loss:6.197431564331055
epoch21: step3500/4680
step 84000: accuracy:0.10499999672174454, confidence:0.8963284492492676, loss:8.019257545471191
epoch21: step4000/4680
step 94500: accuracy:0.11500000208616257, confidence:0.9258179068565369, loss:7.699397087097168
epoch21: step4500/4680
step 0: accuracy:0.12300000339746475, confidence:0.9334464073181152, loss:7.800151348114014
epoch22: step0/4680
step 11000: accuracy:0.0949999988079071, confidence:0.9935428500175476, loss:11.09227180480957
epoch22: step500/4680
step 22000: accuracy:0.0860000029206276, confidence:0.965485692024231, loss:8.907914161682129
epoch22: step1000/4680
step 33000: accuracy:0.09099999815225601, confidence:0.883907675743103, loss:9.0652437210083
epoch22: step1500/4680
step 44000: accuracy:0.03999999910593033, confidence:0.80217444896698, loss:7.809169292449951
epoch22: step2000/4680
step 55000: accuracy:0.10199999809265137, confidence:0.9185245037078857, loss:11.095138549804688
epoch22: step2500/4680
step 66000: accuracy:0.10700000077486038, confidence:0.8966814279556274, loss:8.877680778503418
epoch22: step3000/4680
step 77000: accuracy:0.11400000005960464, confidence:0.6750083565711975, loss:5.223502159118652
epoch22: step3500/4680
step 88000: accuracy:0.1080000028014183, confidence:0.8778021335601807, loss:8.61979866027832
epoch22: step4000/4680
step 99000: accuracy:0.11400000005960464, confidence:0.9545462727546692, loss:10.820938110351562
epoch22: step4500/4680
step 0: accuracy:0.11400000005960464, confidence:0.955001711845398, loss:10.203252792358398
epoch23: step0/4680
step 11500: accuracy:0.10000000149011612, confidence:0.9582966566085815, loss:9.52824878692627
epoch23: step500/4680
step 23000: accuracy:0.07199999690055847, confidence:0.9676669239997864, loss:12.157089233398438
epoch23: step1000/4680
step 34500: accuracy:0.07699999958276749, confidence:0.89053875207901, loss:8.662870407104492
epoch23: step1500/4680
step 46000: accuracy:0.09300000220537186, confidence:0.880195677280426, loss:8.212331771850586
epoch23: step2000/4680
step 57500: accuracy:0.09200000017881393, confidence:0.9303233027458191, loss:11.122644424438477
epoch23: step2500/4680
step 69000: accuracy:0.08799999952316284, confidence:0.8962796926498413, loss:9.606745719909668
epoch23: step3000/4680
step 80500: accuracy:0.08900000154972076, confidence:0.7127058506011963, loss:6.178322792053223
epoch23: step3500/4680
step 92000: accuracy:0.08399999886751175, confidence:0.8944528698921204, loss:8.801756858825684
epoch23: step4000/4680
step 103500: accuracy:0.09799999743700027, confidence:0.9284161925315857, loss:8.779791831970215
epoch23: step4500/4680
step 0: accuracy:0.11999999731779099, confidence:0.9360837340354919, loss:9.0420560836792
epoch24: step0/4680
step 12000: accuracy:0.10100000351667404, confidence:0.993862509727478, loss:12.218198776245117
epoch24: step500/4680
step 24000: accuracy:0.09300000220537186, confidence:0.9293027520179749, loss:8.683296203613281
epoch24: step1000/4680
step 36000: accuracy:0.09300000220537186, confidence:0.8321110606193542, loss:8.257550239562988
epoch24: step1500/4680
step 48000: accuracy:0.03400000184774399, confidence:0.7770183086395264, loss:7.4557976722717285
epoch24: step2000/4680
step 60000: accuracy:0.09799999743700027, confidence:0.9929414987564087, loss:14.423449516296387
epoch24: step2500/4680
step 72000: accuracy:0.10700000077486038, confidence:0.9272663593292236, loss:10.013179779052734
epoch24: step3000/4680
step 84000: accuracy:0.10700000077486038, confidence:0.6985841989517212, loss:5.540637969970703
epoch24: step3500/4680
step 96000: accuracy:0.11999999731779099, confidence:0.876042902469635, loss:8.07388973236084
epoch24: step4000/4680
step 108000: accuracy:0.11900000274181366, confidence:0.956615149974823, loss:9.601922988891602
epoch24: step4500/4680
step 0: accuracy:0.11400000005960464, confidence:0.9534797072410583, loss:9.532225608825684
epoch25: step0/4680
step 12500: accuracy:0.09300000220537186, confidence:0.9600722789764404, loss:10.741941452026367
epoch25: step500/4680
step 25000: accuracy:0.07000000029802322, confidence:0.9911976456642151, loss:11.908989906311035
epoch25: step1000/4680
step 37500: accuracy:0.10400000214576721, confidence:0.8968172669410706, loss:9.171320915222168
epoch25: step1500/4680
step 50000: accuracy:0.07100000232458115, confidence:0.8508603572845459, loss:7.171207904815674
epoch25: step2000/4680
step 62500: accuracy:0.10599999874830246, confidence:0.8319774270057678, loss:8.483671188354492
epoch25: step2500/4680
step 75000: accuracy:0.08799999952316284, confidence:0.8991947770118713, loss:9.939896583557129
epoch25: step3000/4680
step 87500: accuracy:0.09300000220537186, confidence:0.7563582062721252, loss:6.416855335235596
epoch25: step3500/4680
step 100000: accuracy:0.07599999755620956, confidence:0.9318138360977173, loss:10.007364273071289
epoch25: step4000/4680
step 112500: accuracy:0.11800000071525574, confidence:0.9254516959190369, loss:8.229752540588379
epoch25: step4500/4680
step 0: accuracy:0.11400000005960464, confidence:0.9448056221008301, loss:8.905332565307617
epoch26: step0/4680
step 13000: accuracy:0.09000000357627869, confidence:0.9848426580429077, loss:11.03934097290039
epoch26: step500/4680
step 26000: accuracy:0.0949999988079071, confidence:0.9790982604026794, loss:9.960089683532715
epoch26: step1000/4680
step 39000: accuracy:0.07900000363588333, confidence:0.8591544032096863, loss:8.578940391540527
epoch26: step1500/4680
step 52000: accuracy:0.06300000101327896, confidence:0.7807274460792542, loss:7.504176616668701
epoch26: step2000/4680
step 65000: accuracy:0.11400000005960464, confidence:0.9766915440559387, loss:13.157033920288086
epoch26: step2500/4680
step 78000: accuracy:0.10400000214576721, confidence:0.9135957956314087, loss:9.837846755981445
epoch26: step3000/4680
step 91000: accuracy:0.09700000286102295, confidence:0.7037569284439087, loss:5.78907585144043
epoch26: step3500/4680
step 104000: accuracy:0.10999999940395355, confidence:0.865193247795105, loss:8.125495910644531
epoch26: step4000/4680
step 117000: accuracy:0.12700000405311584, confidence:0.9696682691574097, loss:11.013605117797852
epoch26: step4500/4680
step 0: accuracy:0.12600000202655792, confidence:0.9661952257156372, loss:10.551761627197266
epoch27: step0/4680
step 13500: accuracy:0.12300000339746475, confidence:0.843415379524231, loss:8.706274032592773
epoch27: step500/4680
step 27000: accuracy:0.08900000154972076, confidence:0.9743558168411255, loss:11.962173461914062
epoch27: step1000/4680
step 40500: accuracy:0.050999999046325684, confidence:0.8252225518226624, loss:9.197422981262207
epoch27: step1500/4680
step 54000: accuracy:0.07100000232458115, confidence:0.7962459325790405, loss:7.232778549194336
epoch27: step2000/4680
step 67500: accuracy:0.08399999886751175, confidence:0.9410316348075867, loss:11.556068420410156
epoch27: step2500/4680
step 81000: accuracy:0.0949999988079071, confidence:0.9097210168838501, loss:10.960031509399414
epoch27: step3000/4680
step 94500: accuracy:0.0949999988079071, confidence:0.7287857532501221, loss:6.269132614135742
epoch27: step3500/4680
step 108000: accuracy:0.0989999994635582, confidence:0.9149346947669983, loss:9.817989349365234
epoch27: step4000/4680
step 121500: accuracy:0.1080000028014183, confidence:0.9147274494171143, loss:8.515009880065918
epoch27: step4500/4680
step 0: accuracy:0.11699999868869781, confidence:0.9131820797920227, loss:8.617072105407715
epoch28: step0/4680
step 14000: accuracy:0.09399999678134918, confidence:0.9426232576370239, loss:11.24893569946289
epoch28: step500/4680
step 28000: accuracy:0.08900000154972076, confidence:0.9666590094566345, loss:10.897451400756836
epoch28: step1000/4680
step 42000: accuracy:0.08399999886751175, confidence:0.8765302896499634, loss:9.132330894470215
epoch28: step1500/4680
step 56000: accuracy:0.0560000017285347, confidence:0.7766265869140625, loss:7.290275573730469
epoch28: step2000/4680
step 70000: accuracy:0.10499999672174454, confidence:0.9663731455802917, loss:12.751910209655762
epoch28: step2500/4680
step 84000: accuracy:0.0820000022649765, confidence:0.9034150838851929, loss:9.348724365234375
epoch28: step3000/4680
step 98000: accuracy:0.10599999874830246, confidence:0.7454609274864197, loss:6.2171311378479
epoch28: step3500/4680
step 112000: accuracy:0.10499999672174454, confidence:0.8946732878684998, loss:8.517339706420898
epoch28: step4000/4680
step 126000: accuracy:0.10599999874830246, confidence:0.964108943939209, loss:10.504732131958008
epoch28: step4500/4680
step 0: accuracy:0.10400000214576721, confidence:0.9564538598060608, loss:9.981145858764648
epoch29: step0/4680
step 14500: accuracy:0.08900000154972076, confidence:0.9558405876159668, loss:10.05302619934082
epoch29: step500/4680
step 29000: accuracy:0.09700000286102295, confidence:0.9758123755455017, loss:10.998470306396484
epoch29: step1000/4680
step 43500: accuracy:0.1120000034570694, confidence:0.9847043752670288, loss:10.997701644897461
epoch29: step1500/4680
step 58000: accuracy:0.07699999958276749, confidence:0.8071166276931763, loss:7.4000773429870605
epoch29: step2000/4680
step 72500: accuracy:0.09099999815225601, confidence:0.8861908316612244, loss:10.460248947143555
epoch29: step2500/4680
step 87000: accuracy:0.07500000298023224, confidence:0.8762668371200562, loss:8.976226806640625
epoch29: step3000/4680
step 101500: accuracy:0.10100000351667404, confidence:0.7563037872314453, loss:6.469176769256592
epoch29: step3500/4680
step 116000: accuracy:0.08799999952316284, confidence:0.896940290927887, loss:9.310807228088379
epoch29: step4000/4680
step 130500: accuracy:0.09799999743700027, confidence:0.8964811563491821, loss:8.39402961730957
epoch29: step4500/4680
2018-06-15 20:09:48.887742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000a:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 20:09:48.887916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
step 0: accuracy:0.08900000154972076, confidence:0.9202845096588135, loss:8.710836410522461
epoch0: step0/4680
step 0: accuracy:0.09600000083446503, confidence:1.0, loss:50.47256851196289
epoch0: step500/4680
step 0: accuracy:0.10700000077486038, confidence:1.0, loss:42.80167007446289
epoch0: step1000/4680
step 0: accuracy:0.10199999809265137, confidence:0.9999976754188538, loss:16.109251022338867
epoch0: step1500/4680
step 0: accuracy:0.10599999874830246, confidence:0.9999982714653015, loss:15.033796310424805
epoch0: step2000/4680
step 0: accuracy:0.12399999797344208, confidence:0.9997293949127197, loss:10.010703086853027
epoch0: step2500/4680
step 0: accuracy:0.09200000017881393, confidence:0.9999443292617798, loss:11.96431827545166
epoch0: step3000/4680
step 0: accuracy:0.07900000363588333, confidence:0.9997693300247192, loss:11.777885437011719
epoch0: step3500/4680
step 0: accuracy:0.0949999988079071, confidence:0.9997013807296753, loss:10.963321685791016
epoch0: step4000/4680
step 0: accuracy:0.10499999672174454, confidence:0.9997711777687073, loss:11.544852256774902
epoch0: step4500/4680
step 0: accuracy:0.1080000028014183, confidence:0.9997653365135193, loss:11.136846542358398
epoch1: step0/4680
step 500: accuracy:0.1340000033378601, confidence:0.9948601126670837, loss:7.868447780609131
epoch1: step500/4680
step 1000: accuracy:0.10700000077486038, confidence:0.9880697727203369, loss:7.808082580566406
epoch1: step1000/4680
step 1500: accuracy:0.10499999672174454, confidence:0.998744010925293, loss:10.395959854125977
epoch1: step1500/4680
step 2000: accuracy:0.05999999865889549, confidence:0.999186098575592, loss:9.6510009765625
epoch1: step2000/4680
step 2500: accuracy:0.1120000034570694, confidence:0.9996525049209595, loss:9.669078826904297
epoch1: step2500/4680
step 3000: accuracy:0.09799999743700027, confidence:0.999703049659729, loss:10.156005859375
epoch1: step3000/4680
step 3500: accuracy:0.11400000005960464, confidence:0.9997319579124451, loss:10.744542121887207
epoch1: step3500/4680
step 4000: accuracy:0.08799999952316284, confidence:0.9992002248764038, loss:9.405538558959961
epoch1: step4000/4680
step 4500: accuracy:0.08699999749660492, confidence:0.9999718070030212, loss:12.680523872375488
epoch1: step4500/4680
step 0: accuracy:0.09600000083446503, confidence:0.9999334812164307, loss:11.591400146484375
epoch2: step0/4680
step 1000: accuracy:0.11599999666213989, confidence:0.9710046052932739, loss:5.968024253845215
epoch2: step500/4680
step 2000: accuracy:0.09700000286102295, confidence:0.9847104549407959, loss:7.344148635864258
epoch2: step1000/4680
step 3000: accuracy:0.09399999678134918, confidence:0.9857285618782043, loss:7.131458759307861
epoch2: step1500/4680
step 4000: accuracy:0.09700000286102295, confidence:0.9998886585235596, loss:11.587294578552246
epoch2: step2000/4680
step 5000: accuracy:0.12700000405311584, confidence:0.9996294379234314, loss:9.553536415100098
epoch2: step2500/4680
step 6000: accuracy:0.11599999666213989, confidence:0.9997853636741638, loss:10.180222511291504
epoch2: step3000/4680
step 7000: accuracy:0.1080000028014183, confidence:0.99625563621521, loss:7.991775035858154
epoch2: step3500/4680
step 8000: accuracy:0.1080000028014183, confidence:0.997995913028717, loss:8.432117462158203
epoch2: step4000/4680
step 9000: accuracy:0.10599999874830246, confidence:0.9997178912162781, loss:10.307025909423828
epoch2: step4500/4680
step 0: accuracy:0.09600000083446503, confidence:0.9995141625404358, loss:9.754999160766602
epoch3: step0/4680
step 1500: accuracy:0.11800000071525574, confidence:0.9918515682220459, loss:6.553832530975342
epoch3: step500/4680
step 3000: accuracy:0.10300000011920929, confidence:0.9851483702659607, loss:6.7723259925842285
epoch3: step1000/4680
step 4500: accuracy:0.11699999868869781, confidence:0.9275407791137695, loss:5.344446659088135
epoch3: step1500/4680
step 6000: accuracy:0.0949999988079071, confidence:0.9981672763824463, loss:8.724624633789062
epoch3: step2000/4680
step 7500: accuracy:0.11999999731779099, confidence:0.9985904693603516, loss:8.609465599060059
epoch3: step2500/4680
step 9000: accuracy:0.10000000149011612, confidence:0.9983797073364258, loss:8.38537883758545
epoch3: step3000/4680
step 10500: accuracy:0.11500000208616257, confidence:0.9970658421516418, loss:8.117474555969238
epoch3: step3500/4680
step 12000: accuracy:0.10499999672174454, confidence:0.9993264079093933, loss:9.322795867919922
epoch3: step4000/4680
step 13500: accuracy:0.1080000028014183, confidence:0.9985806345939636, loss:8.687009811401367
epoch3: step4500/4680
step 0: accuracy:0.10100000351667404, confidence:0.9985086917877197, loss:8.556873321533203
epoch4: step0/4680
step 2000: accuracy:0.09399999678134918, confidence:0.9495837688446045, loss:5.138482093811035
epoch4: step500/4680
step 4000: accuracy:0.10199999809265137, confidence:0.9738641977310181, loss:5.809735298156738
epoch4: step1000/4680
step 6000: accuracy:0.0989999994635582, confidence:0.9465919137001038, loss:5.679022312164307
epoch4: step1500/4680
step 8000: accuracy:0.10599999874830246, confidence:0.9985036253929138, loss:9.15433120727539
epoch4: step2000/4680
step 10000: accuracy:0.11599999666213989, confidence:0.9708770513534546, loss:5.597168445587158
epoch4: step2500/4680
step 12000: accuracy:0.10999999940395355, confidence:0.9954410195350647, loss:7.4756388664245605
epoch4: step3000/4680
step 14000: accuracy:0.10999999940395355, confidence:0.9684619903564453, loss:5.379358768463135
epoch4: step3500/4680
step 16000: accuracy:0.10599999874830246, confidence:0.9962987303733826, loss:7.357177257537842
epoch4: step4000/4680
step 18000: accuracy:0.10599999874830246, confidence:0.995579719543457, loss:7.562981605529785
epoch4: step4500/4680
step 0: accuracy:0.10499999672174454, confidence:0.9948055744171143, loss:7.267763614654541
epoch5: step0/4680
step 2500: accuracy:0.09099999815225601, confidence:0.8560330271720886, loss:3.990861654281616
epoch5: step500/4680
step 5000: accuracy:0.10400000214576721, confidence:0.9870970845222473, loss:6.52287483215332
epoch5: step1000/4680
step 7500: accuracy:0.09300000220537186, confidence:0.9603833556175232, loss:5.851611137390137
epoch5: step1500/4680
step 10000: accuracy:0.10499999672174454, confidence:0.997356653213501, loss:8.291189193725586
epoch5: step2000/4680
step 12500: accuracy:0.11900000274181366, confidence:0.9646117091178894, loss:5.443542957305908
epoch5: step2500/4680
step 15000: accuracy:0.09799999743700027, confidence:0.9901986122131348, loss:6.825356483459473
epoch5: step3000/4680
step 17500: accuracy:0.09799999743700027, confidence:0.9232040643692017, loss:4.798641204833984
epoch5: step3500/4680
step 20000: accuracy:0.09700000286102295, confidence:0.9813975691795349, loss:6.097278594970703
epoch5: step4000/4680
step 22500: accuracy:0.09399999678134918, confidence:0.991063117980957, loss:7.01504373550415
epoch5: step4500/4680
step 0: accuracy:0.10599999874830246, confidence:0.9901016354560852, loss:6.719420433044434
epoch6: step0/4680
step 3000: accuracy:0.09300000220537186, confidence:0.9106518626213074, loss:4.381944179534912
epoch6: step500/4680
step 6000: accuracy:0.09799999743700027, confidence:0.9895401000976562, loss:6.608729362487793
epoch6: step1000/4680
step 9000: accuracy:0.08299999684095383, confidence:0.9400003552436829, loss:5.824116230010986
epoch6: step1500/4680
step 12000: accuracy:0.10499999672174454, confidence:0.9984797239303589, loss:9.000746726989746
epoch6: step2000/4680
step 15000: accuracy:0.08900000154972076, confidence:0.9638187885284424, loss:5.587118148803711
epoch6: step2500/4680
step 18000: accuracy:0.10499999672174454, confidence:0.9990893602371216, loss:9.028059959411621
epoch6: step3000/4680
step 21000: accuracy:0.10599999874830246, confidence:0.9073796272277832, loss:4.513176918029785
epoch6: step3500/4680
step 24000: accuracy:0.10899999737739563, confidence:0.9913145899772644, loss:6.676350116729736
epoch6: step4000/4680
step 27000: accuracy:0.09700000286102295, confidence:0.9855856895446777, loss:6.762727737426758
epoch6: step4500/4680
step 0: accuracy:0.11299999803304672, confidence:0.9854130744934082, loss:6.617146968841553
epoch7: step0/4680
step 3500: accuracy:0.10000000149011612, confidence:0.9089489579200745, loss:4.49241828918457
epoch7: step500/4680
step 7000: accuracy:0.08699999749660492, confidence:0.994236171245575, loss:7.253007888793945
epoch7: step1000/4680
step 10500: accuracy:0.09799999743700027, confidence:0.9236409068107605, loss:5.188565731048584
epoch7: step1500/4680
step 14000: accuracy:0.10199999809265137, confidence:0.9989912509918213, loss:9.603386878967285
epoch7: step2000/4680
step 17500: accuracy:0.1289999932050705, confidence:0.9320158362388611, loss:4.820637226104736
epoch7: step2500/4680
step 21000: accuracy:0.10300000011920929, confidence:0.9954713582992554, loss:9.141456604003906
epoch7: step3000/4680
step 24500: accuracy:0.11100000143051147, confidence:0.8056227564811707, loss:3.7826731204986572
epoch7: step3500/4680
step 28000: accuracy:0.0989999994635582, confidence:0.9695137739181519, loss:5.621267795562744
epoch7: step4000/4680
step 31500: accuracy:0.10100000351667404, confidence:0.9750581979751587, loss:6.213358402252197
epoch7: step4500/4680
step 0: accuracy:0.09200000017881393, confidence:0.9714750051498413, loss:5.997422695159912
epoch8: step0/4680
step 4000: accuracy:0.09600000083446503, confidence:0.9247837662696838, loss:4.7040886878967285
epoch8: step500/4680
step 8000: accuracy:0.09300000220537186, confidence:0.9971314072608948, loss:8.265802383422852
epoch8: step1000/4680
step 12000: accuracy:0.10300000011920929, confidence:0.9645946025848389, loss:5.982847690582275
epoch8: step1500/4680
step 16000: accuracy:0.1120000034570694, confidence:0.9992567896842957, loss:9.535238265991211
epoch8: step2000/4680
step 20000: accuracy:0.11500000208616257, confidence:0.8113221526145935, loss:3.8834660053253174
epoch8: step2500/4680
step 24000: accuracy:0.10499999672174454, confidence:0.937808096408844, loss:7.362649440765381
epoch8: step3000/4680
step 28000: accuracy:0.09000000357627869, confidence:0.7746558785438538, loss:3.694688320159912
epoch8: step3500/4680
step 32000: accuracy:0.09700000286102295, confidence:0.966595470905304, loss:5.494359016418457
epoch8: step4000/4680
step 36000: accuracy:0.0949999988079071, confidence:0.8621515035629272, loss:4.685144901275635
epoch8: step4500/4680
step 0: accuracy:0.09799999743700027, confidence:0.8449897170066833, loss:4.380621910095215
epoch9: step0/4680
step 4500: accuracy:0.11800000071525574, confidence:0.8908636569976807, loss:4.362311363220215
epoch9: step500/4680
step 9000: accuracy:0.08500000089406967, confidence:0.9992891550064087, loss:9.32054615020752
epoch9: step1000/4680
step 13500: accuracy:0.10999999940395355, confidence:0.971984326839447, loss:6.577085494995117
epoch9: step1500/4680
step 18000: accuracy:0.10000000149011612, confidence:0.9992338418960571, loss:9.957286834716797
epoch9: step2000/4680
step 22500: accuracy:0.08900000154972076, confidence:0.8725406527519226, loss:4.4401044845581055
epoch9: step2500/4680
step 27000: accuracy:0.11500000208616257, confidence:0.9565443396568298, loss:8.212286949157715
epoch9: step3000/4680
step 31500: accuracy:0.10499999672174454, confidence:0.8336204290390015, loss:3.9030978679656982
epoch9: step3500/4680
step 36000: accuracy:0.11100000143051147, confidence:0.9899886846542358, loss:6.522490501403809
epoch9: step4000/4680
step 40500: accuracy:0.11299999803304672, confidence:0.9639298319816589, loss:5.7783966064453125
epoch9: step4500/4680
step 0: accuracy:0.10100000351667404, confidence:0.963013231754303, loss:5.7889814376831055
epoch10: step0/4680
step 5000: accuracy:0.0860000029206276, confidence:0.8659001588821411, loss:4.345158576965332
epoch10: step500/4680
step 10000: accuracy:0.09300000220537186, confidence:0.998942494392395, loss:8.930846214294434
epoch10: step1000/4680
step 15000: accuracy:0.10400000214576721, confidence:0.9382316470146179, loss:5.692414283752441
epoch10: step1500/4680
step 20000: accuracy:0.09700000286102295, confidence:0.9995992183685303, loss:11.251187324523926
epoch10: step2000/4680
step 25000: accuracy:0.13600000739097595, confidence:0.9031789302825928, loss:4.6598734855651855
epoch10: step2500/4680
step 30000: accuracy:0.11500000208616257, confidence:0.980961263179779, loss:8.978371620178223
epoch10: step3000/4680
step 35000: accuracy:0.09799999743700027, confidence:0.8526623249053955, loss:4.110432147979736
epoch10: step3500/4680
step 40000: accuracy:0.10000000149011612, confidence:0.9792104959487915, loss:5.912405014038086
epoch10: step4000/4680
step 45000: accuracy:0.09200000017881393, confidence:0.9231469631195068, loss:5.120815753936768
epoch10: step4500/4680
step 0: accuracy:0.09700000286102295, confidence:0.9222166538238525, loss:5.085697650909424
epoch11: step0/4680
step 5500: accuracy:0.09099999815225601, confidence:0.8793060183525085, loss:4.676467418670654
epoch11: step500/4680
step 11000: accuracy:0.0860000029206276, confidence:0.9972452521324158, loss:8.496978759765625
epoch11: step1000/4680
step 16500: accuracy:0.10400000214576721, confidence:0.9432685971260071, loss:5.765138149261475
epoch11: step1500/4680
step 22000: accuracy:0.08900000154972076, confidence:0.9999073147773743, loss:12.685707092285156
epoch11: step2000/4680
step 27500: accuracy:0.13099999725818634, confidence:0.8382248282432556, loss:4.210869312286377
epoch11: step2500/4680
step 33000: accuracy:0.1850000023841858, confidence:0.7317959666252136, loss:6.356621742248535
epoch11: step3000/4680
step 38500: accuracy:0.10499999672174454, confidence:0.6120888590812683, loss:3.3868987560272217
epoch11: step3500/4680
step 44000: accuracy:0.0949999988079071, confidence:0.8856241106987, loss:4.530037879943848
epoch11: step4000/4680
step 49500: accuracy:0.10199999809265137, confidence:0.7627727389335632, loss:4.0265069007873535
epoch11: step4500/4680
step 0: accuracy:0.1120000034570694, confidence:0.7523245215415955, loss:3.8979873657226562
epoch12: step0/4680
step 6000: accuracy:0.09799999743700027, confidence:0.8629912734031677, loss:4.810439109802246
epoch12: step500/4680
step 12000: accuracy:0.10199999809265137, confidence:0.9990950226783752, loss:9.497284889221191
epoch12: step1000/4680
step 18000: accuracy:0.09300000220537186, confidence:0.963401734828949, loss:6.4610066413879395
epoch12: step1500/4680
step 24000: accuracy:0.10899999737739563, confidence:0.9998586177825928, loss:12.555375099182129
epoch12: step2000/4680
step 30000: accuracy:0.10400000214576721, confidence:0.8181769251823425, loss:4.194546699523926
epoch12: step2500/4680
step 36000: accuracy:0.12800000607967377, confidence:0.7606377601623535, loss:7.601879119873047
epoch12: step3000/4680
step 42000: accuracy:0.08900000154972076, confidence:0.7441800832748413, loss:3.7135684490203857
epoch12: step3500/4680
step 48000: accuracy:0.09600000083446503, confidence:0.956041693687439, loss:5.289119243621826
epoch12: step4000/4680
step 54000: accuracy:0.11299999803304672, confidence:0.871589183807373, loss:4.682582855224609
epoch12: step4500/4680
step 0: accuracy:0.09300000220537186, confidence:0.8643893003463745, loss:4.673659801483154
epoch13: step0/4680
step 6500: accuracy:0.07400000095367432, confidence:0.8188391327857971, loss:4.507870674133301
epoch13: step500/4680
step 13000: accuracy:0.09799999743700027, confidence:0.9958661794662476, loss:7.884840488433838
epoch13: step1000/4680
step 19500: accuracy:0.0989999994635582, confidence:0.9790993332862854, loss:6.97953987121582
epoch13: step1500/4680
step 26000: accuracy:0.08399999886751175, confidence:0.9999229907989502, loss:13.54908561706543
epoch13: step2000/4680
step 32500: accuracy:0.10199999809265137, confidence:0.8780983090400696, loss:4.722729682922363
epoch13: step2500/4680
step 39000: accuracy:0.17299999296665192, confidence:0.7381952404975891, loss:7.639591693878174
epoch13: step3000/4680
step 45500: accuracy:0.11500000208616257, confidence:0.667400062084198, loss:3.465879201889038
epoch13: step3500/4680
step 52000: accuracy:0.09000000357627869, confidence:0.881432294845581, loss:4.645828723907471
epoch13: step4000/4680
step 58500: accuracy:0.10400000214576721, confidence:0.6767987012863159, loss:3.764387607574463
epoch13: step4500/4680
step 0: accuracy:0.0989999994635582, confidence:0.6674365401268005, loss:3.67059326171875
epoch14: step0/4680
step 7000: accuracy:0.10700000077486038, confidence:0.8747634291648865, loss:4.800275802612305
epoch14: step500/4680
step 14000: accuracy:0.09000000357627869, confidence:0.9978671073913574, loss:8.722728729248047
epoch14: step1000/4680
step 21000: accuracy:0.08299999684095383, confidence:0.9465861320495605, loss:6.2343831062316895
epoch14: step1500/4680
step 28000: accuracy:0.09099999815225601, confidence:0.9995086789131165, loss:12.44215202331543
epoch14: step2000/4680
step 35000: accuracy:0.11100000143051147, confidence:0.916333794593811, loss:5.227049827575684
epoch14: step2500/4680
step 42000: accuracy:0.18000000715255737, confidence:0.74559485912323, loss:7.114349842071533
epoch14: step3000/4680
step 49000: accuracy:0.11100000143051147, confidence:0.7857096195220947, loss:3.869079828262329
epoch14: step3500/4680
step 56000: accuracy:0.0989999994635582, confidence:0.9719180464744568, loss:5.850400447845459
epoch14: step4000/4680
step 63000: accuracy:0.09799999743700027, confidence:0.8496221899986267, loss:4.752566814422607
epoch14: step4500/4680
step 0: accuracy:0.10700000077486038, confidence:0.8403165936470032, loss:4.594339370727539
epoch15: step0/4680
step 7500: accuracy:0.07599999755620956, confidence:0.7795776128768921, loss:4.533012866973877
epoch15: step500/4680
step 15000: accuracy:0.11999999731779099, confidence:0.9961666464805603, loss:8.056123733520508
epoch15: step1000/4680
step 22500: accuracy:0.10000000149011612, confidence:0.9455170631408691, loss:6.199462890625
epoch15: step1500/4680
step 30000: accuracy:0.10999999940395355, confidence:0.9994655251502991, loss:12.650081634521484
epoch15: step2000/4680
step 37500: accuracy:0.1120000034570694, confidence:0.8844984769821167, loss:5.1343817710876465
epoch15: step2500/4680
step 45000: accuracy:0.19599999487400055, confidence:0.7786315679550171, loss:7.327831268310547
epoch15: step3000/4680
step 52500: accuracy:0.13300000131130219, confidence:0.6000785231590271, loss:3.5328309535980225
epoch15: step3500/4680
step 60000: accuracy:0.10599999874830246, confidence:0.7975881099700928, loss:4.223077774047852
epoch15: step4000/4680
step 67500: accuracy:0.14100000262260437, confidence:0.6529890298843384, loss:4.0086350440979
epoch15: step4500/4680
step 0: accuracy:0.12600000202655792, confidence:0.6308895349502563, loss:3.87080454826355
epoch16: step0/4680
step 8000: accuracy:0.05400000140070915, confidence:0.6920204758644104, loss:4.23915433883667
epoch16: step500/4680
step 16000: accuracy:0.1080000028014183, confidence:0.997199296951294, loss:8.57938289642334
epoch16: step1000/4680
step 24000: accuracy:0.10599999874830246, confidence:0.9889740347862244, loss:7.7710137367248535
epoch16: step1500/4680
step 32000: accuracy:0.09399999678134918, confidence:0.9995560050010681, loss:11.213693618774414
epoch16: step2000/4680
step 40000: accuracy:0.10000000149011612, confidence:0.8234148025512695, loss:4.549618244171143
epoch16: step2500/4680
step 48000: accuracy:0.17299999296665192, confidence:0.7559504508972168, loss:7.484677314758301
epoch16: step3000/4680
step 56000: accuracy:0.10199999809265137, confidence:0.6686865091323853, loss:3.6747772693634033
epoch16: step3500/4680
step 64000: accuracy:0.08799999952316284, confidence:0.8430691361427307, loss:4.478196144104004
epoch16: step4000/4680
step 72000: accuracy:0.10999999940395355, confidence:0.6573348045349121, loss:3.9311890602111816
epoch16: step4500/4680
step 0: accuracy:0.11699999868869781, confidence:0.6380560398101807, loss:3.8257083892822266
epoch17: step0/4680
step 8500: accuracy:0.0560000017285347, confidence:0.6897364258766174, loss:4.0988545417785645
epoch17: step500/4680
step 17000: accuracy:0.0820000022649765, confidence:0.9970459342002869, loss:8.920165061950684
epoch17: step1000/4680
step 25500: accuracy:0.10599999874830246, confidence:0.9791578650474548, loss:7.398617267608643
epoch17: step1500/4680
step 34000: accuracy:0.09399999678134918, confidence:0.9995099306106567, loss:11.160560607910156
epoch17: step2000/4680
step 42500: accuracy:0.11299999803304672, confidence:0.8209323883056641, loss:4.548966884613037
epoch17: step2500/4680
step 51000: accuracy:0.18400000035762787, confidence:0.7575981020927429, loss:7.184787750244141
epoch17: step3000/4680
step 59500: accuracy:0.09600000083446503, confidence:0.6418851613998413, loss:3.7220263481140137
epoch17: step3500/4680
step 68000: accuracy:0.0989999994635582, confidence:0.8044226765632629, loss:4.358399868011475
epoch17: step4000/4680
step 76500: accuracy:0.11800000071525574, confidence:0.5612186789512634, loss:3.654350757598877
epoch17: step4500/4680
step 0: accuracy:0.1340000033378601, confidence:0.5612974762916565, loss:3.5400595664978027
epoch18: step0/4680
step 9000: accuracy:0.04500000178813934, confidence:0.7055132985115051, loss:4.449445724487305
epoch18: step500/4680
step 18000: accuracy:0.10300000011920929, confidence:0.9967058897018433, loss:8.748930931091309
epoch18: step1000/4680
step 27000: accuracy:0.08900000154972076, confidence:0.9820309281349182, loss:7.775887489318848
epoch18: step1500/4680
step 36000: accuracy:0.09799999743700027, confidence:0.9997183084487915, loss:12.049737930297852
epoch18: step2000/4680
step 45000: accuracy:0.11299999803304672, confidence:0.8443427681922913, loss:4.817472457885742
epoch18: step2500/4680
step 54000: accuracy:0.16699999570846558, confidence:0.7473801374435425, loss:7.126981735229492
epoch18: step3000/4680
step 63000: accuracy:0.10100000351667404, confidence:0.5907230377197266, loss:3.623432159423828
epoch18: step3500/4680
step 72000: accuracy:0.08900000154972076, confidence:0.9546530246734619, loss:5.673699378967285
epoch18: step4000/4680
step 81000: accuracy:0.08299999684095383, confidence:0.9485048651695251, loss:6.215178966522217
epoch18: step4500/4680
step 0: accuracy:0.10000000149011612, confidence:0.9352552890777588, loss:5.645675182342529
epoch19: step0/4680
step 9500: accuracy:0.03799999877810478, confidence:0.7320464253425598, loss:4.978044033050537
epoch19: step500/4680
step 19000: accuracy:0.09700000286102295, confidence:0.992123544216156, loss:8.160524368286133
epoch19: step1000/4680
step 28500: accuracy:0.10700000077486038, confidence:0.9764236211776733, loss:7.785642147064209
epoch19: step1500/4680
step 38000: accuracy:0.09600000083446503, confidence:0.9995260834693909, loss:12.826947212219238
epoch19: step2000/4680
step 47500: accuracy:0.11800000071525574, confidence:0.910354733467102, loss:6.1684956550598145
epoch19: step2500/4680
step 57000: accuracy:0.17299999296665192, confidence:0.7582910656929016, loss:8.814682006835938
epoch19: step3000/4680
step 66500: accuracy:0.10199999809265137, confidence:0.7886855006217957, loss:4.208765029907227
epoch19: step3500/4680
step 76000: accuracy:0.12300000339746475, confidence:0.913460373878479, loss:4.9864726066589355
epoch19: step4000/4680
step 85500: accuracy:0.09300000220537186, confidence:0.7083653807640076, loss:4.184736251831055
epoch19: step4500/4680
step 0: accuracy:0.08399999886751175, confidence:0.6694296002388, loss:4.011892795562744
epoch20: step0/4680
step 10000: accuracy:0.03099999949336052, confidence:0.7251874208450317, loss:4.912188529968262
epoch20: step500/4680
step 20000: accuracy:0.10100000351667404, confidence:0.9978342056274414, loss:9.782126426696777
epoch20: step1000/4680
step 30000: accuracy:0.08100000023841858, confidence:0.9685441851615906, loss:7.833428382873535
epoch20: step1500/4680
step 40000: accuracy:0.0949999988079071, confidence:0.9996143579483032, loss:14.022554397583008
epoch20: step2000/4680
step 50000: accuracy:0.11999999731779099, confidence:0.9045386910438538, loss:6.171037197113037
epoch20: step2500/4680
step 60000: accuracy:0.11900000274181366, confidence:0.8432672023773193, loss:8.856651306152344
epoch20: step3000/4680
step 70000: accuracy:0.11699999868869781, confidence:0.7297340035438538, loss:4.034101963043213
epoch20: step3500/4680
step 80000: accuracy:0.09600000083446503, confidence:0.8733713626861572, loss:4.996645450592041
epoch20: step4000/4680
step 90000: accuracy:0.13899999856948853, confidence:0.5672502517700195, loss:3.6557931900024414
epoch20: step4500/4680
step 0: accuracy:0.14300000667572021, confidence:0.5391834378242493, loss:3.616453170776367
epoch21: step0/4680
step 10500: accuracy:0.035999998450279236, confidence:0.7310924530029297, loss:4.880920886993408
epoch21: step500/4680
step 21000: accuracy:0.10400000214576721, confidence:0.9966406226158142, loss:9.61727523803711
epoch21: step1000/4680
step 31500: accuracy:0.09700000286102295, confidence:0.9762755036354065, loss:8.1155366897583
epoch21: step1500/4680
step 42000: accuracy:0.10199999809265137, confidence:0.998759388923645, loss:14.474515914916992
epoch21: step2000/4680
step 52500: accuracy:0.12300000339746475, confidence:0.9119641780853271, loss:6.46359920501709
epoch21: step2500/4680
step 63000: accuracy:0.16099999845027924, confidence:0.8043546676635742, loss:9.338245391845703
epoch21: step3000/4680
step 73500: accuracy:0.11100000143051147, confidence:0.8426641821861267, loss:4.421607494354248
epoch21: step3500/4680
step 84000: accuracy:0.1080000028014183, confidence:0.9312356114387512, loss:5.205327987670898
epoch21: step4000/4680
step 94500: accuracy:0.08900000154972076, confidence:0.585673451423645, loss:3.967353343963623
epoch21: step4500/4680
step 0: accuracy:0.11900000274181366, confidence:0.5595923066139221, loss:3.6679182052612305
epoch22: step0/4680
step 11000: accuracy:0.01600000075995922, confidence:0.6916594505310059, loss:4.764374256134033
epoch22: step500/4680
step 22000: accuracy:0.09399999678134918, confidence:0.9938551187515259, loss:8.80029582977295
epoch22: step1000/4680
step 33000: accuracy:0.10199999809265137, confidence:0.9829773306846619, loss:8.37735366821289
epoch22: step1500/4680
step 44000: accuracy:0.08399999886751175, confidence:0.9996468424797058, loss:15.428831100463867
epoch22: step2000/4680
step 55000: accuracy:0.125, confidence:0.9333916306495667, loss:6.862305641174316
epoch22: step2500/4680
step 66000: accuracy:0.13300000131130219, confidence:0.8523520827293396, loss:9.035989761352539
epoch22: step3000/4680
step 77000: accuracy:0.10300000011920929, confidence:0.9613256454467773, loss:5.853331565856934
epoch22: step3500/4680
step 88000: accuracy:0.08500000089406967, confidence:0.9875491261482239, loss:7.001246929168701
epoch22: step4000/4680
step 99000: accuracy:0.10999999940395355, confidence:0.8032699823379517, loss:4.443338394165039
epoch22: step4500/4680
step 0: accuracy:0.10199999809265137, confidence:0.7784842252731323, loss:4.226475238800049
epoch23: step0/4680
step 11500: accuracy:0.01899999938905239, confidence:0.6711482405662537, loss:4.6029372215271
epoch23: step500/4680
step 23000: accuracy:0.11599999666213989, confidence:0.9813917875289917, loss:7.6598100662231445
epoch23: step1000/4680
step 34500: accuracy:0.11100000143051147, confidence:0.9916603565216064, loss:8.973491668701172
epoch23: step1500/4680
step 46000: accuracy:0.1120000034570694, confidence:0.9996184706687927, loss:13.473772048950195
epoch23: step2000/4680
step 57500: accuracy:0.11999999731779099, confidence:0.888900876045227, loss:6.191347599029541
epoch23: step2500/4680
step 69000: accuracy:0.1379999965429306, confidence:0.8420314788818359, loss:8.701333999633789
epoch23: step3000/4680
step 80500: accuracy:0.12200000137090683, confidence:0.6803017258644104, loss:4.329549789428711
epoch23: step3500/4680
step 92000: accuracy:0.1120000034570694, confidence:0.821223795413971, loss:4.825827598571777
epoch23: step4000/4680
step 103500: accuracy:0.09399999678134918, confidence:0.6449698805809021, loss:4.0446624755859375
epoch23: step4500/4680
step 0: accuracy:0.10499999672174454, confidence:0.6013167500495911, loss:3.796267509460449
epoch24: step0/4680
step 12000: accuracy:0.014999999664723873, confidence:0.6843578219413757, loss:4.648688316345215
epoch24: step500/4680
step 24000: accuracy:0.10400000214576721, confidence:0.9633073806762695, loss:7.1852922439575195
epoch24: step1000/4680
step 36000: accuracy:0.1120000034570694, confidence:0.9752931594848633, loss:8.338949203491211
epoch24: step1500/4680
step 48000: accuracy:0.10499999672174454, confidence:0.9997040629386902, loss:15.089800834655762
epoch24: step2000/4680
step 60000: accuracy:0.12099999934434891, confidence:0.9295361042022705, loss:6.963097095489502
epoch24: step2500/4680
step 72000: accuracy:0.11500000208616257, confidence:0.8946040272712708, loss:9.05608081817627
epoch24: step3000/4680
step 84000: accuracy:0.10499999672174454, confidence:0.8443700075149536, loss:4.947510719299316
epoch24: step3500/4680
step 96000: accuracy:0.10199999809265137, confidence:0.9100328087806702, loss:5.544068336486816
epoch24: step4000/4680
step 108000: accuracy:0.07599999755620956, confidence:0.593915581703186, loss:4.00324010848999
epoch24: step4500/4680
step 0: accuracy:0.07699999958276749, confidence:0.5854033827781677, loss:3.9278690814971924
epoch25: step0/4680
step 12500: accuracy:0.019999999552965164, confidence:0.7312535047531128, loss:5.5926899909973145
epoch25: step500/4680
step 25000: accuracy:0.11299999803304672, confidence:0.9921278357505798, loss:8.787150382995605
epoch25: step1000/4680
step 37500: accuracy:0.09099999815225601, confidence:0.9690815210342407, loss:8.228012084960938
epoch25: step1500/4680
step 50000: accuracy:0.09799999743700027, confidence:0.9998617768287659, loss:17.306140899658203
epoch25: step2000/4680
step 62500: accuracy:0.125, confidence:0.9778510332107544, loss:8.769493103027344
epoch25: step2500/4680
step 75000: accuracy:0.1379999965429306, confidence:0.8880833983421326, loss:9.710786819458008
epoch25: step3000/4680
step 87500: accuracy:0.09799999743700027, confidence:0.7955467700958252, loss:4.724262714385986
epoch25: step3500/4680
step 100000: accuracy:0.11699999868869781, confidence:0.9079827666282654, loss:5.595919609069824
epoch25: step4000/4680
step 112500: accuracy:0.08500000089406967, confidence:0.6688528656959534, loss:4.274806499481201
epoch25: step4500/4680
step 0: accuracy:0.07000000029802322, confidence:0.6459273099899292, loss:4.185694694519043
epoch26: step0/4680
step 13000: accuracy:0.019999999552965164, confidence:0.7837305068969727, loss:6.327460289001465
epoch26: step500/4680
step 26000: accuracy:0.10599999874830246, confidence:0.9964331984519958, loss:9.719283103942871
epoch26: step1000/4680
step 39000: accuracy:0.09799999743700027, confidence:0.9793197512626648, loss:8.525679588317871
epoch26: step1500/4680
step 52000: accuracy:0.11400000005960464, confidence:0.999372661113739, loss:15.616454124450684
epoch26: step2000/4680
step 65000: accuracy:0.10499999672174454, confidence:0.9418374300003052, loss:8.059894561767578
epoch26: step2500/4680
step 78000: accuracy:0.11699999868869781, confidence:0.9015445113182068, loss:10.0025634765625
epoch26: step3000/4680
step 91000: accuracy:0.0989999994635582, confidence:0.8857964873313904, loss:5.652842998504639
epoch26: step3500/4680
step 104000: accuracy:0.10499999672174454, confidence:0.9333927035331726, loss:6.102768898010254
epoch26: step4000/4680
step 117000: accuracy:0.0989999994635582, confidence:0.7629199028015137, loss:4.817531585693359
epoch26: step4500/4680
step 0: accuracy:0.09399999678134918, confidence:0.7237241864204407, loss:4.526043891906738
epoch27: step0/4680
step 13500: accuracy:0.029999999329447746, confidence:0.8157389163970947, loss:6.022780895233154
epoch27: step500/4680
step 27000: accuracy:0.09300000220537186, confidence:0.9857652187347412, loss:8.768172264099121
epoch27: step1000/4680
step 40500: accuracy:0.1080000028014183, confidence:0.9968495965003967, loss:10.842577934265137
epoch27: step1500/4680
step 54000: accuracy:0.0989999994635582, confidence:0.9992302060127258, loss:15.343358993530273
epoch27: step2000/4680
step 67500: accuracy:0.10400000214576721, confidence:0.9361072778701782, loss:7.7159624099731445
epoch27: step2500/4680
step 81000: accuracy:0.12700000405311584, confidence:0.8737061023712158, loss:9.998756408691406
epoch27: step3000/4680
step 94500: accuracy:0.12800000607967377, confidence:0.7081230282783508, loss:4.8672614097595215
epoch27: step3500/4680
step 108000: accuracy:0.11800000071525574, confidence:0.7988409399986267, loss:5.367294788360596
epoch27: step4000/4680
step 121500: accuracy:0.11800000071525574, confidence:0.8300566673278809, loss:5.107616424560547
epoch27: step4500/4680
step 0: accuracy:0.11100000143051147, confidence:0.7560341358184814, loss:4.563699245452881
epoch28: step0/4680
step 14000: accuracy:0.039000000804662704, confidence:0.8309417366981506, loss:6.236087799072266
epoch28: step500/4680
step 28000: accuracy:0.10100000351667404, confidence:0.9877432584762573, loss:8.827031135559082
epoch28: step1000/4680
step 42000: accuracy:0.10100000351667404, confidence:0.8823292255401611, loss:6.905623912811279
epoch28: step1500/4680
step 56000: accuracy:0.11500000208616257, confidence:0.9992738962173462, loss:15.338090896606445
epoch28: step2000/4680
step 70000: accuracy:0.10499999672174454, confidence:0.9453309178352356, loss:8.274736404418945
epoch28: step2500/4680
step 84000: accuracy:0.14000000059604645, confidence:0.8634436726570129, loss:10.255488395690918
epoch28: step3000/4680
step 98000: accuracy:0.1080000028014183, confidence:0.826349675655365, loss:5.139169692993164
epoch28: step3500/4680
step 112000: accuracy:0.10899999737739563, confidence:0.8971174359321594, loss:5.9708571434021
epoch28: step4000/4680
step 126000: accuracy:0.10000000149011612, confidence:0.8286446928977966, loss:5.598380088806152
epoch28: step4500/4680
step 0: accuracy:0.1080000028014183, confidence:0.7358065247535706, loss:4.971048355102539
epoch29: step0/4680
step 14500: accuracy:0.04600000008940697, confidence:0.8227023482322693, loss:6.0741400718688965
epoch29: step500/4680
step 29000: accuracy:0.08799999952316284, confidence:0.9674776196479797, loss:8.144039154052734
epoch29: step1000/4680
step 43500: accuracy:0.08799999952316284, confidence:0.9684714078903198, loss:8.734718322753906
epoch29: step1500/4680
step 58000: accuracy:0.09200000017881393, confidence:0.9998617768287659, loss:17.648927688598633
epoch29: step2000/4680
step 72500: accuracy:0.11100000143051147, confidence:0.9839256405830383, loss:10.408761024475098
epoch29: step2500/4680
step 87000: accuracy:0.15399999916553497, confidence:0.8662303686141968, loss:10.47745418548584
epoch29: step3000/4680
step 101500: accuracy:0.11100000143051147, confidence:0.7745540738105774, loss:5.4808669090271
epoch29: step3500/4680
step 116000: accuracy:0.0949999988079071, confidence:0.8373902440071106, loss:5.910730838775635
epoch29: step4000/4680
step 130500: accuracy:0.09200000017881393, confidence:0.8842443227767944, loss:6.380273342132568
epoch29: step4500/4680
