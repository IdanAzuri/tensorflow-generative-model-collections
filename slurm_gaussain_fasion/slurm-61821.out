2018-06-15 17:27:23.075200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000a:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 17:27:23.075377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
2018-06-15 17:27:24.475244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
CHEKPOINT DIR: uniform
model has been loaded from fashion-mnist_classifier.pkl
---------
Variables: name (type shape) [size]
---------
Variable:0 (float32_ref 5x5x1x32) [800, bytes: 3200]
Variable_1:0 (float32_ref 32) [32, bytes: 128]
Variable_2:0 (float32_ref 5x5x32x64) [51200, bytes: 204800]
Variable_3:0 (float32_ref 64) [64, bytes: 256]
Variable_4:0 (float32_ref 3136x1024) [3211264, bytes: 12845056]
Variable_5:0 (float32_ref 1024) [1024, bytes: 4096]
Variable_6:0 (float32_ref 1024x10) [10240, bytes: 40960]
Variable_7:0 (float32_ref 10) [10, bytes: 40]
discriminator/d_conv1/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]
discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]
discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]
discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]
generator/g_fc1/Matrix:0 (float32_ref 74x1024) [75776, bytes: 303104]
generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]
generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]
generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]
classifier/c_fc1/Matrix:0 (float32_ref 1024x64) [65536, bytes: 262144]
classifier/c_fc1/bias:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/beta:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/gamma:0 (float32_ref 64) [64, bytes: 256]
classifier/c_fc2/Matrix:0 (float32_ref 64x12) [768, bytes: 3072]
classifier/c_fc2/bias:0 (float32_ref 12) [12, bytes: 48]
Total size of variables: 16552792
Total bytes of variables: 66211168
 [*] Reading checkpoints...
 [*] Success to read MultiModalInfoGAN.model-43721
 [*] Load SUCCESS


SAMPLES SIZE=4680,LABELS=299520


SAVED TRAINING SET generated_training_set_fashion-mnist_UniformSample_mu_0_sigma_0.15_czcc_czrc_rzcc_rzrc
 [*] Training finished!
 [*] Testing finished!
2018-06-15 17:28:23.410689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000a:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 17:28:23.410850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
model has been loaded from fashion-mnist_classifier.pkl
argmax:[3 3 3 ..., 6 6 6]
step 0: accuracy:0.0, confidence:0.9714910387992859, loss:14.669201850891113
Assinging:4
[ 625    0    0 8862    0    0  513]
argmax:[1 1 1 ..., 1 1 1]
step 0: accuracy:0.0, confidence:0.9989244341850281, loss:16.06884002685547
Assinging:2
[    0 10000]
argmax:[7 7 7 ..., 7 7 7]
step 0: accuracy:0.14090000092983246, confidence:0.734927773475647, loss:1.4632506370544434
Assinging:8
[   0    0    0    0    0  200    0 8391    0 1409]
argmax:[0 0 0 ..., 0 0 0]
step 0: accuracy:0.0, confidence:0.9953765869140625, loss:15.222267150878906
Assinging:1
[9920    0    0    0    0    0   80]
argmax:[4 4 4 ..., 2 2 2]
step 0: accuracy:0.0, confidence:0.9291709065437317, loss:11.873727798461914
Assinging:5
[   0    0 1066    0 8463    0  471]
argmax:[9 9 9 ..., 5 5 5]
step 0: accuracy:0.8774999976158142, confidence:0.9917004108428955, loss:1.5468921661376953
Assinging:10
[   0    0    0    0    0 1225    0    0    0 8775]
argmax:[2 2 2 ..., 2 2 2]
step 0: accuracy:0.0, confidence:0.6685035228729248, loss:13.229225158691406
Assinging:3
[   0    0 9220    0  468    0  312]
argmax:[7 7 7 ..., 5 5 5]
step 0: accuracy:0.0, confidence:0.9782641530036926, loss:9.14956283569336
Assinging:8
[   0    0    0    0    0 1576    0 8424]
argmax:[2 2 2 ..., 6 6 2]
step 0: accuracy:0.0, confidence:0.6487152576446533, loss:6.259286880493164
Assinging:3
[   0    0 7606  199 1644    0  551]
argmax:[8 8 8 ..., 8 8 8]
step 0: accuracy:0.0, confidence:0.9997271299362183, loss:16.723506927490234
Assinging:9
[    0     0     0     0     0     0     0     0 10000]
2018-06-15 17:28:54.193082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000a:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 17:28:54.193140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
step 0: accuracy:0.09700000286102295, confidence:0.5759191513061523, loss:5.958256244659424
epoch0: step0/4680
step 0: accuracy:0.11500000208616257, confidence:1.0, loss:30.031391143798828
epoch0: step500/4680
step 0: accuracy:0.08799999952316284, confidence:1.0, loss:46.055519104003906
epoch0: step1000/4680
step 0: accuracy:0.10400000214576721, confidence:0.9999837875366211, loss:14.827714920043945
epoch0: step1500/4680
step 0: accuracy:0.1120000034570694, confidence:0.9975042343139648, loss:8.109794616699219
epoch0: step2000/4680
step 0: accuracy:0.10400000214576721, confidence:0.9991380572319031, loss:9.278286933898926
epoch0: step2500/4680
step 0: accuracy:0.08900000154972076, confidence:0.9999872446060181, loss:13.221179008483887
epoch0: step3000/4680
step 0: accuracy:0.1080000028014183, confidence:0.9997147917747498, loss:10.057872772216797
epoch0: step3500/4680
step 0: accuracy:0.09099999815225601, confidence:0.9999901652336121, loss:15.038675308227539
epoch0: step4000/4680
step 0: accuracy:0.09700000286102295, confidence:0.9822388887405396, loss:7.040152549743652
epoch0: step4500/4680
step 0: accuracy:0.0860000029206276, confidence:0.9869500994682312, loss:7.6265974044799805
epoch1: step0/4680
step 500: accuracy:0.09000000357627869, confidence:0.9909383654594421, loss:7.287121295928955
epoch1: step500/4680
step 1000: accuracy:0.1120000034570694, confidence:0.996550977230072, loss:7.932424545288086
epoch1: step1000/4680
step 1500: accuracy:0.0860000029206276, confidence:0.9996777772903442, loss:10.218137741088867
epoch1: step1500/4680
step 2000: accuracy:0.10100000351667404, confidence:0.9960944056510925, loss:7.864502429962158
epoch1: step2000/4680
step 2500: accuracy:0.10999999940395355, confidence:0.9963592886924744, loss:8.552577018737793
epoch1: step2500/4680
step 3000: accuracy:0.10700000077486038, confidence:0.9839454293251038, loss:6.4836273193359375
epoch1: step3000/4680
step 3500: accuracy:0.10499999672174454, confidence:0.9996874928474426, loss:9.903860092163086
epoch1: step3500/4680
step 4000: accuracy:0.09200000017881393, confidence:0.9997764825820923, loss:10.24515151977539
epoch1: step4000/4680
step 4500: accuracy:0.09200000017881393, confidence:0.9641293287277222, loss:6.156005859375
epoch1: step4500/4680
step 0: accuracy:0.10899999737739563, confidence:0.9755710363388062, loss:6.481197834014893
epoch2: step0/4680
step 1000: accuracy:0.10700000077486038, confidence:0.9938738346099854, loss:7.276788711547852
epoch2: step500/4680
step 2000: accuracy:0.11500000208616257, confidence:0.9835097789764404, loss:7.681983470916748
epoch2: step1000/4680
step 3000: accuracy:0.10400000214576721, confidence:0.8256884813308716, loss:7.022322654724121
epoch2: step1500/4680
step 4000: accuracy:0.09000000357627869, confidence:0.999938428401947, loss:15.772589683532715
epoch2: step2000/4680
step 5000: accuracy:0.09200000017881393, confidence:0.9965993762016296, loss:8.296611785888672
epoch2: step2500/4680
step 6000: accuracy:0.10000000149011612, confidence:0.9790722131729126, loss:6.046638488769531
epoch2: step3000/4680
step 7000: accuracy:0.09399999678134918, confidence:0.9995325803756714, loss:9.737988471984863
epoch2: step3500/4680
step 8000: accuracy:0.09399999678134918, confidence:0.9989485144615173, loss:8.692564010620117
epoch2: step4000/4680
step 9000: accuracy:0.13199999928474426, confidence:0.7894893884658813, loss:5.778090000152588
epoch2: step4500/4680
step 0: accuracy:0.10400000214576721, confidence:0.8707543015480042, loss:6.950636863708496
epoch3: step0/4680
step 1500: accuracy:0.08699999749660492, confidence:0.9977776408195496, loss:8.805338859558105
epoch3: step500/4680
step 3000: accuracy:0.09200000017881393, confidence:0.9962378144264221, loss:8.843235969543457
epoch3: step1000/4680
step 4500: accuracy:0.11500000208616257, confidence:0.9964020252227783, loss:8.496463775634766
epoch3: step1500/4680
step 6000: accuracy:0.10100000351667404, confidence:0.9623951315879822, loss:8.452953338623047
epoch3: step2000/4680
step 7500: accuracy:0.10199999809265137, confidence:0.9890012145042419, loss:6.988708019256592
epoch3: step2500/4680
step 9000: accuracy:0.1120000034570694, confidence:0.9685112237930298, loss:5.608539581298828
epoch3: step3000/4680
step 10500: accuracy:0.08699999749660492, confidence:0.9930272102355957, loss:8.389825820922852
epoch3: step3500/4680
step 12000: accuracy:0.10599999874830246, confidence:0.9993888735771179, loss:9.638599395751953
epoch3: step4000/4680
step 13500: accuracy:0.125, confidence:0.8581554293632507, loss:6.294698238372803
epoch3: step4500/4680
step 0: accuracy:0.10599999874830246, confidence:0.8175344467163086, loss:6.699287414550781
epoch4: step0/4680
step 2000: accuracy:0.10400000214576721, confidence:0.9916573762893677, loss:7.859928607940674
epoch4: step500/4680
step 4000: accuracy:0.12099999934434891, confidence:0.9946179986000061, loss:8.146319389343262
epoch4: step1000/4680
step 6000: accuracy:0.10100000351667404, confidence:0.9935418963432312, loss:8.591050148010254
epoch4: step1500/4680
step 8000: accuracy:0.10499999672174454, confidence:0.9856840372085571, loss:9.637828826904297
epoch4: step2000/4680
step 10000: accuracy:0.10199999809265137, confidence:0.9878421425819397, loss:7.541012763977051
epoch4: step2500/4680
step 12000: accuracy:0.0949999988079071, confidence:0.9643445611000061, loss:5.9514360427856445
epoch4: step3000/4680
step 14000: accuracy:0.08900000154972076, confidence:0.9937288761138916, loss:9.02247142791748
epoch4: step3500/4680
step 16000: accuracy:0.10999999940395355, confidence:0.9990734457969666, loss:9.741604804992676
epoch4: step4000/4680
step 18000: accuracy:0.11299999803304672, confidence:0.9304172396659851, loss:7.388218879699707
epoch4: step4500/4680
step 0: accuracy:0.10199999809265137, confidence:0.9097851514816284, loss:7.4454569816589355
epoch5: step0/4680
step 2500: accuracy:0.09700000286102295, confidence:0.9960067272186279, loss:10.839045524597168
epoch5: step500/4680
step 5000: accuracy:0.12600000202655792, confidence:0.9799669981002808, loss:7.8867363929748535
epoch5: step1000/4680
step 7500: accuracy:0.09200000017881393, confidence:0.8504233360290527, loss:8.188633918762207
epoch5: step1500/4680
step 10000: accuracy:0.10199999809265137, confidence:0.994321882724762, loss:15.702392578125
epoch5: step2000/4680
step 12500: accuracy:0.11100000143051147, confidence:0.9437458515167236, loss:6.810145854949951
epoch5: step2500/4680
step 15000: accuracy:0.09799999743700027, confidence:0.9455883502960205, loss:5.879735469818115
epoch5: step3000/4680
step 17500: accuracy:0.11999999731779099, confidence:0.9781090021133423, loss:8.365389823913574
epoch5: step3500/4680
step 20000: accuracy:0.12399999797344208, confidence:0.9983177781105042, loss:9.853160858154297
epoch5: step4000/4680
step 22500: accuracy:0.0989999994635582, confidence:0.8719522953033447, loss:6.200963497161865
epoch5: step4500/4680
step 0: accuracy:0.12399999797344208, confidence:0.8619946241378784, loss:6.060569763183594
epoch6: step0/4680
step 3000: accuracy:0.10499999672174454, confidence:0.9811574220657349, loss:8.41415023803711
epoch6: step500/4680
step 6000: accuracy:0.11699999868869781, confidence:0.880707859992981, loss:5.223579406738281
epoch6: step1000/4680
step 9000: accuracy:0.07199999690055847, confidence:0.8033822178840637, loss:7.064752101898193
epoch6: step1500/4680
step 12000: accuracy:0.10300000011920929, confidence:0.9710876941680908, loss:12.329264640808105
epoch6: step2000/4680
step 15000: accuracy:0.11299999803304672, confidence:0.9337194561958313, loss:6.365386486053467
epoch6: step2500/4680
step 18000: accuracy:0.10499999672174454, confidence:0.9181358814239502, loss:5.498050689697266
epoch6: step3000/4680
step 21000: accuracy:0.09000000357627869, confidence:0.9663430452346802, loss:8.344143867492676
epoch6: step3500/4680
step 24000: accuracy:0.08900000154972076, confidence:0.9968532919883728, loss:10.887578010559082
epoch6: step4000/4680
step 27000: accuracy:0.09600000083446503, confidence:0.9147347807884216, loss:7.638642311096191
epoch6: step4500/4680
step 0: accuracy:0.1340000033378601, confidence:0.8878006339073181, loss:7.076300621032715
epoch7: step0/4680
step 3500: accuracy:0.09799999743700027, confidence:0.9900203347206116, loss:9.567411422729492
epoch7: step500/4680
step 7000: accuracy:0.11100000143051147, confidence:0.94596266746521, loss:7.124586582183838
epoch7: step1000/4680
step 10500: accuracy:0.0860000029206276, confidence:0.8296704888343811, loss:7.118849754333496
epoch7: step1500/4680
step 14000: accuracy:0.10400000214576721, confidence:0.9585216641426086, loss:14.424713134765625
epoch7: step2000/4680
step 17500: accuracy:0.09600000083446503, confidence:0.9453520774841309, loss:7.287787437438965
epoch7: step2500/4680
step 21000: accuracy:0.08900000154972076, confidence:0.8543986678123474, loss:5.6191935539245605
epoch7: step3000/4680
step 24500: accuracy:0.15399999916553497, confidence:0.8181530833244324, loss:7.345136642456055
epoch7: step3500/4680
step 28000: accuracy:0.10599999874830246, confidence:0.9922072887420654, loss:11.053983688354492
epoch7: step4000/4680
step 31500: accuracy:0.10999999940395355, confidence:0.9305045008659363, loss:8.413268089294434
epoch7: step4500/4680
step 0: accuracy:0.09600000083446503, confidence:0.9235422611236572, loss:8.188190460205078
epoch8: step0/4680
step 4000: accuracy:0.10000000149011612, confidence:0.9860652685165405, loss:9.306300163269043
epoch8: step500/4680
step 8000: accuracy:0.0989999994635582, confidence:0.9382864236831665, loss:7.531246185302734
epoch8: step1000/4680
step 12000: accuracy:0.08399999886751175, confidence:0.8812295198440552, loss:7.907935619354248
epoch8: step1500/4680
step 16000: accuracy:0.08900000154972076, confidence:0.9017215967178345, loss:11.980870246887207
epoch8: step2000/4680
step 20000: accuracy:0.08699999749660492, confidence:0.7291195392608643, loss:6.068418025970459
epoch8: step2500/4680
step 24000: accuracy:0.07800000160932541, confidence:0.8568140864372253, loss:6.1356706619262695
epoch8: step3000/4680
step 28000: accuracy:0.15399999916553497, confidence:0.8323997855186462, loss:8.030221939086914
epoch8: step3500/4680
step 32000: accuracy:0.0989999994635582, confidence:0.9967126250267029, loss:11.569051742553711
epoch8: step4000/4680
step 36000: accuracy:0.0989999994635582, confidence:0.9369800686836243, loss:8.373331069946289
epoch8: step4500/4680
step 0: accuracy:0.10199999809265137, confidence:0.9366627931594849, loss:8.14537525177002
epoch9: step0/4680
step 4500: accuracy:0.09200000017881393, confidence:0.9868927001953125, loss:10.185165405273438
epoch9: step500/4680
step 9000: accuracy:0.11500000208616257, confidence:0.9183297753334045, loss:7.418204307556152
epoch9: step1000/4680
step 13500: accuracy:0.1080000028014183, confidence:0.86693274974823, loss:8.148456573486328
epoch9: step1500/4680
step 18000: accuracy:0.10499999672174454, confidence:0.9990500807762146, loss:15.191983222961426
epoch9: step2000/4680
step 22500: accuracy:0.1080000028014183, confidence:0.9844032526016235, loss:9.453740119934082
epoch9: step2500/4680
step 27000: accuracy:0.09600000083446503, confidence:0.9676464796066284, loss:7.953319549560547
epoch9: step3000/4680
step 31500: accuracy:0.13899999856948853, confidence:0.7789487838745117, loss:6.689986228942871
epoch9: step3500/4680
step 36000: accuracy:0.08699999749660492, confidence:0.9841088652610779, loss:12.245235443115234
epoch9: step4000/4680
step 40500: accuracy:0.11299999803304672, confidence:0.9535781741142273, loss:9.638901710510254
epoch9: step4500/4680
step 0: accuracy:0.11400000005960464, confidence:0.9221628904342651, loss:9.38669490814209
epoch10: step0/4680
step 5000: accuracy:0.0949999988079071, confidence:0.9987414479255676, loss:11.68895435333252
epoch10: step500/4680
step 10000: accuracy:0.11800000071525574, confidence:0.9708126187324524, loss:10.178605079650879
epoch10: step1000/4680
step 15000: accuracy:0.09200000017881393, confidence:0.8658323287963867, loss:9.039932250976562
epoch10: step1500/4680
step 20000: accuracy:0.09099999815225601, confidence:0.8691620826721191, loss:12.3320894241333
epoch10: step2000/4680
step 25000: accuracy:0.13600000739097595, confidence:0.7138108611106873, loss:6.287172317504883
epoch10: step2500/4680
step 30000: accuracy:0.12099999934434891, confidence:0.8440499305725098, loss:8.454949378967285
epoch10: step3000/4680
step 35000: accuracy:0.16699999570846558, confidence:0.787530779838562, loss:7.96795129776001
epoch10: step3500/4680
step 40000: accuracy:0.11400000005960464, confidence:0.9735703468322754, loss:10.259974479675293
epoch10: step4000/4680
step 45000: accuracy:0.10300000011920929, confidence:0.9074178338050842, loss:8.51097297668457
epoch10: step4500/4680
step 0: accuracy:0.09700000286102295, confidence:0.8921323418617249, loss:8.554790496826172
epoch11: step0/4680
step 5500: accuracy:0.10499999672174454, confidence:0.9894548654556274, loss:12.541923522949219
epoch11: step500/4680
step 11000: accuracy:0.1080000028014183, confidence:0.9705827832221985, loss:10.81611156463623
epoch11: step1000/4680
step 16500: accuracy:0.10599999874830246, confidence:0.9088720679283142, loss:9.307003021240234
epoch11: step1500/4680
step 22000: accuracy:0.08699999749660492, confidence:0.8639212846755981, loss:10.933448791503906
epoch11: step2000/4680
step 27500: accuracy:0.08399999886751175, confidence:0.7395237684249878, loss:8.09744930267334
epoch11: step2500/4680
step 33000: accuracy:0.04600000008940697, confidence:0.7686945796012878, loss:6.3836870193481445
epoch11: step3000/4680
step 38500: accuracy:0.16099999845027924, confidence:0.8082109689712524, loss:6.624716281890869
epoch11: step3500/4680
step 44000: accuracy:0.09600000083446503, confidence:0.9834756851196289, loss:11.19316291809082
epoch11: step4000/4680
step 49500: accuracy:0.11400000005960464, confidence:0.9042463898658752, loss:8.555477142333984
epoch11: step4500/4680
step 0: accuracy:0.11599999666213989, confidence:0.8887931704521179, loss:8.44386100769043
epoch12: step0/4680
step 6000: accuracy:0.10100000351667404, confidence:0.9633861184120178, loss:9.633806228637695
epoch12: step500/4680
step 12000: accuracy:0.125, confidence:0.8920946717262268, loss:7.146061420440674
epoch12: step1000/4680
step 18000: accuracy:0.08500000089406967, confidence:0.8601730465888977, loss:8.499601364135742
epoch12: step1500/4680
step 24000: accuracy:0.0989999994635582, confidence:0.899124264717102, loss:11.571412086486816
epoch12: step2000/4680
step 30000: accuracy:0.12200000137090683, confidence:0.7433118224143982, loss:7.034995079040527
epoch12: step2500/4680
step 36000: accuracy:0.06499999761581421, confidence:0.6671668887138367, loss:7.040750980377197
epoch12: step3000/4680
step 42000: accuracy:0.15399999916553497, confidence:0.8323913812637329, loss:6.966994285583496
epoch12: step3500/4680
step 48000: accuracy:0.10700000077486038, confidence:0.9492012858390808, loss:9.84665298461914
epoch12: step4000/4680
step 54000: accuracy:0.11900000274181366, confidence:0.896690309047699, loss:8.512372970581055
epoch12: step4500/4680
step 0: accuracy:0.10499999672174454, confidence:0.8683885931968689, loss:8.594208717346191
epoch13: step0/4680
step 6500: accuracy:0.10599999874830246, confidence:0.9731895923614502, loss:10.013642311096191
epoch13: step500/4680
step 13000: accuracy:0.125, confidence:0.8990591168403625, loss:8.09585189819336
epoch13: step1000/4680
step 19500: accuracy:0.10300000011920929, confidence:0.8670172095298767, loss:8.937352180480957
epoch13: step1500/4680
step 26000: accuracy:0.10199999809265137, confidence:0.9016695022583008, loss:11.925374031066895
epoch13: step2000/4680
step 32500: accuracy:0.10199999809265137, confidence:0.7383568286895752, loss:7.919379711151123
epoch13: step2500/4680
step 39000: accuracy:0.0560000017285347, confidence:0.6992207169532776, loss:6.293172836303711
epoch13: step3000/4680
step 45500: accuracy:0.20399999618530273, confidence:0.8181158304214478, loss:6.58650016784668
epoch13: step3500/4680
step 52000: accuracy:0.10899999737739563, confidence:0.9798387885093689, loss:10.784936904907227
epoch13: step4000/4680
step 58500: accuracy:0.12300000339746475, confidence:0.9015987515449524, loss:8.241717338562012
epoch13: step4500/4680
step 0: accuracy:0.09200000017881393, confidence:0.8971375823020935, loss:8.41907024383545
epoch14: step0/4680
step 7000: accuracy:0.09799999743700027, confidence:0.9818404316902161, loss:11.212946891784668
epoch14: step500/4680
step 14000: accuracy:0.11299999803304672, confidence:0.9209479689598083, loss:8.760464668273926
epoch14: step1000/4680
step 21000: accuracy:0.0989999994635582, confidence:0.8878574371337891, loss:9.068473815917969
epoch14: step1500/4680
step 28000: accuracy:0.0989999994635582, confidence:0.7766618728637695, loss:10.121337890625
epoch14: step2000/4680
step 35000: accuracy:0.09099999815225601, confidence:0.743501603603363, loss:10.041031837463379
epoch14: step2500/4680
step 42000: accuracy:0.05400000140070915, confidence:0.7873291969299316, loss:7.220333099365234
epoch14: step3000/4680
step 49000: accuracy:0.1770000010728836, confidence:0.8278558254241943, loss:6.383452892303467
epoch14: step3500/4680
step 56000: accuracy:0.10400000214576721, confidence:0.960893452167511, loss:9.762238502502441
epoch14: step4000/4680
step 63000: accuracy:0.08799999952316284, confidence:0.8973873257637024, loss:8.147574424743652
epoch14: step4500/4680
step 0: accuracy:0.10300000011920929, confidence:0.8869779706001282, loss:7.726924896240234
epoch15: step0/4680
step 7500: accuracy:0.09600000083446503, confidence:0.9208425283432007, loss:8.380167961120605
epoch15: step500/4680
step 15000: accuracy:0.13699999451637268, confidence:0.8590574860572815, loss:7.259701728820801
epoch15: step1000/4680
step 22500: accuracy:0.08100000023841858, confidence:0.8439163565635681, loss:9.00349235534668
epoch15: step1500/4680
step 30000: accuracy:0.10000000149011612, confidence:0.8524231314659119, loss:11.072578430175781
epoch15: step2000/4680
step 37500: accuracy:0.1550000011920929, confidence:0.7645657062530518, loss:7.448600769042969
epoch15: step2500/4680
step 45000: accuracy:0.07100000232458115, confidence:0.6832894086837769, loss:8.089263916015625
epoch15: step3000/4680
step 52500: accuracy:0.1940000057220459, confidence:0.8122183680534363, loss:6.412633895874023
epoch15: step3500/4680
step 60000: accuracy:0.0860000029206276, confidence:0.9653432965278625, loss:10.930242538452148
epoch15: step4000/4680
step 67500: accuracy:0.11999999731779099, confidence:0.8849790692329407, loss:8.363215446472168
epoch15: step4500/4680
step 0: accuracy:0.11900000274181366, confidence:0.8795990943908691, loss:8.272764205932617
epoch16: step0/4680
step 8000: accuracy:0.11100000143051147, confidence:0.9310292601585388, loss:9.15113639831543
epoch16: step500/4680
step 16000: accuracy:0.1080000028014183, confidence:0.9086888432502747, loss:8.572891235351562
epoch16: step1000/4680
step 24000: accuracy:0.08900000154972076, confidence:0.8801128268241882, loss:9.340835571289062
epoch16: step1500/4680
step 32000: accuracy:0.10000000149011612, confidence:0.786168098449707, loss:11.210322380065918
epoch16: step2000/4680
step 40000: accuracy:0.11999999731779099, confidence:0.7840246558189392, loss:8.575255393981934
epoch16: step2500/4680
step 48000: accuracy:0.08699999749660492, confidence:0.7368423938751221, loss:6.857110023498535
epoch16: step3000/4680
step 56000: accuracy:0.16699999570846558, confidence:0.8459312915802002, loss:6.865133285522461
epoch16: step3500/4680
step 64000: accuracy:0.10599999874830246, confidence:0.9712935090065002, loss:11.154913902282715
epoch16: step4000/4680
step 72000: accuracy:0.09600000083446503, confidence:0.9036575555801392, loss:8.948119163513184
epoch16: step4500/4680
step 0: accuracy:0.10899999737739563, confidence:0.9019699692726135, loss:8.383097648620605
epoch17: step0/4680
step 8500: accuracy:0.0989999994635582, confidence:0.9680149555206299, loss:10.337034225463867
epoch17: step500/4680
step 17000: accuracy:0.12800000607967377, confidence:0.9080979824066162, loss:8.076155662536621
epoch17: step1000/4680
step 25500: accuracy:0.11500000208616257, confidence:0.886971116065979, loss:8.87087631225586
epoch17: step1500/4680
step 34000: accuracy:0.09000000357627869, confidence:0.7595328092575073, loss:10.079129219055176
epoch17: step2000/4680
step 42500: accuracy:0.1420000046491623, confidence:0.7835091352462769, loss:7.741389751434326
epoch17: step2500/4680
step 51000: accuracy:0.08699999749660492, confidence:0.7442666292190552, loss:7.926258563995361
epoch17: step3000/4680
step 59500: accuracy:0.17100000381469727, confidence:0.8441962003707886, loss:7.175655841827393
epoch17: step3500/4680
step 68000: accuracy:0.09600000083446503, confidence:0.9705870151519775, loss:11.089743614196777
epoch17: step4000/4680
step 76500: accuracy:0.11100000143051147, confidence:0.9001169204711914, loss:8.537960052490234
epoch17: step4500/4680
step 0: accuracy:0.10199999809265137, confidence:0.8861861228942871, loss:8.401446342468262
epoch18: step0/4680
step 9000: accuracy:0.11800000071525574, confidence:0.9341148734092712, loss:9.570439338684082
epoch18: step500/4680
step 18000: accuracy:0.11800000071525574, confidence:0.8739367723464966, loss:8.125540733337402
epoch18: step1000/4680
step 27000: accuracy:0.0860000029206276, confidence:0.8551552891731262, loss:9.279799461364746
epoch18: step1500/4680
step 36000: accuracy:0.12300000339746475, confidence:0.789508044719696, loss:11.492826461791992
epoch18: step2000/4680
step 45000: accuracy:0.12700000405311584, confidence:0.7584807872772217, loss:7.366793155670166
epoch18: step2500/4680
step 54000: accuracy:0.08799999952316284, confidence:0.7550202012062073, loss:7.446939945220947
epoch18: step3000/4680
step 63000: accuracy:0.19300000369548798, confidence:0.8179363012313843, loss:6.676619052886963
epoch18: step3500/4680
step 72000: accuracy:0.0860000029206276, confidence:0.9751477241516113, loss:11.822528839111328
epoch18: step4000/4680
step 81000: accuracy:0.08699999749660492, confidence:0.9073888659477234, loss:9.103257179260254
epoch18: step4500/4680
step 0: accuracy:0.08900000154972076, confidence:0.8983734846115112, loss:8.72411060333252
epoch19: step0/4680
step 9500: accuracy:0.11800000071525574, confidence:0.9426099061965942, loss:9.065934181213379
epoch19: step500/4680
step 19000: accuracy:0.12700000405311584, confidence:0.8946483731269836, loss:7.92109489440918
epoch19: step1000/4680
step 28500: accuracy:0.08799999952316284, confidence:0.8662588000297546, loss:9.039901733398438
epoch19: step1500/4680
step 38000: accuracy:0.11699999868869781, confidence:0.7759938836097717, loss:9.501687049865723
epoch19: step2000/4680
step 47500: accuracy:0.16300000250339508, confidence:0.812650740146637, loss:7.8925395011901855
epoch19: step2500/4680
step 57000: accuracy:0.10300000011920929, confidence:0.7307034730911255, loss:7.466055870056152
epoch19: step3000/4680
step 66500: accuracy:0.17599999904632568, confidence:0.8350731730461121, loss:7.298452377319336
epoch19: step3500/4680
step 76000: accuracy:0.1080000028014183, confidence:0.946928083896637, loss:10.825057983398438
epoch19: step4000/4680
step 85500: accuracy:0.0989999994635582, confidence:0.8926606178283691, loss:9.211698532104492
epoch19: step4500/4680
step 0: accuracy:0.09399999678134918, confidence:0.8838518857955933, loss:9.11372184753418
epoch20: step0/4680
step 10000: accuracy:0.09099999815225601, confidence:0.9456294178962708, loss:10.205802917480469
epoch20: step500/4680
step 20000: accuracy:0.12300000339746475, confidence:0.866037130355835, loss:7.625134468078613
epoch20: step1000/4680
step 30000: accuracy:0.08900000154972076, confidence:0.8705772757530212, loss:8.908607482910156
epoch20: step1500/4680
step 40000: accuracy:0.09300000220537186, confidence:0.775739312171936, loss:9.646993637084961
epoch20: step2000/4680
step 50000: accuracy:0.14900000393390656, confidence:0.8308027386665344, loss:6.925188064575195
epoch20: step2500/4680
step 60000: accuracy:0.07500000298023224, confidence:0.7683432102203369, loss:7.597219467163086
epoch20: step3000/4680
step 70000: accuracy:0.164000004529953, confidence:0.8373789191246033, loss:7.841287136077881
epoch20: step3500/4680
step 80000: accuracy:0.10999999940395355, confidence:0.9827873706817627, loss:12.988919258117676
epoch20: step4000/4680
step 90000: accuracy:0.11599999666213989, confidence:0.8986076712608337, loss:9.217138290405273
epoch20: step4500/4680
step 0: accuracy:0.11599999666213989, confidence:0.8916037678718567, loss:8.938590049743652
epoch21: step0/4680
step 10500: accuracy:0.09799999743700027, confidence:0.9588428735733032, loss:11.865335464477539
epoch21: step500/4680
step 21000: accuracy:0.11500000208616257, confidence:0.9051904678344727, loss:8.650876998901367
epoch21: step1000/4680
step 31500: accuracy:0.06800000369548798, confidence:0.8498196005821228, loss:9.06167221069336
epoch21: step1500/4680
step 42000: accuracy:0.13300000131130219, confidence:0.7769937515258789, loss:9.867376327514648
epoch21: step2000/4680
step 52500: accuracy:0.15199999511241913, confidence:0.8074464797973633, loss:7.478555202484131
epoch21: step2500/4680
step 63000: accuracy:0.08299999684095383, confidence:0.7909178137779236, loss:7.6102824211120605
epoch21: step3000/4680
step 73500: accuracy:0.18700000643730164, confidence:0.8434557318687439, loss:7.7261247634887695
epoch21: step3500/4680
step 84000: accuracy:0.12200000137090683, confidence:0.9805476665496826, loss:12.893575668334961
epoch21: step4000/4680
step 94500: accuracy:0.08500000089406967, confidence:0.9187154173851013, loss:10.70156192779541
epoch21: step4500/4680
step 0: accuracy:0.11599999666213989, confidence:0.9230504035949707, loss:9.714384078979492
epoch22: step0/4680
step 11000: accuracy:0.11299999803304672, confidence:0.9130102396011353, loss:9.688051223754883
epoch22: step500/4680
step 22000: accuracy:0.11500000208616257, confidence:0.8546189069747925, loss:8.137874603271484
epoch22: step1000/4680
step 33000: accuracy:0.05900000035762787, confidence:0.8445689678192139, loss:9.119887351989746
epoch22: step1500/4680
step 44000: accuracy:0.1589999943971634, confidence:0.799494206905365, loss:10.685792922973633
epoch22: step2000/4680
step 55000: accuracy:0.14800000190734863, confidence:0.8326234221458435, loss:6.700448036193848
epoch22: step2500/4680
step 66000: accuracy:0.08299999684095383, confidence:0.7882027626037598, loss:7.164900302886963
epoch22: step3000/4680
step 77000: accuracy:0.16500000655651093, confidence:0.851676881313324, loss:7.749635219573975
epoch22: step3500/4680
step 88000: accuracy:0.10199999809265137, confidence:0.9769213199615479, loss:13.121794700622559
epoch22: step4000/4680
step 99000: accuracy:0.09399999678134918, confidence:0.9122747778892517, loss:9.797085762023926
epoch22: step4500/4680
step 0: accuracy:0.10300000011920929, confidence:0.9140710830688477, loss:9.407563209533691
epoch23: step0/4680
step 11500: accuracy:0.0949999988079071, confidence:0.9445458054542542, loss:9.831698417663574
epoch23: step500/4680
step 23000: accuracy:0.13300000131130219, confidence:0.8839362263679504, loss:7.922356128692627
epoch23: step1000/4680
step 34500: accuracy:0.07199999690055847, confidence:0.8415740728378296, loss:8.995135307312012
epoch23: step1500/4680
step 46000: accuracy:0.11999999731779099, confidence:0.7799685597419739, loss:9.513495445251465
epoch23: step2000/4680
step 57500: accuracy:0.15299999713897705, confidence:0.802404522895813, loss:8.081231117248535
epoch23: step2500/4680
step 69000: accuracy:0.0820000022649765, confidence:0.8220841884613037, loss:7.677587032318115
epoch23: step3000/4680
step 80500: accuracy:0.17000000178813934, confidence:0.8358438611030579, loss:7.77521276473999
epoch23: step3500/4680
step 92000: accuracy:0.11400000005960464, confidence:0.9660875797271729, loss:12.127639770507812
epoch23: step4000/4680
step 103500: accuracy:0.10599999874830246, confidence:0.91404128074646, loss:10.105182647705078
epoch23: step4500/4680
step 0: accuracy:0.1120000034570694, confidence:0.8990564346313477, loss:9.43456745147705
epoch24: step0/4680
step 12000: accuracy:0.1080000028014183, confidence:0.9436466097831726, loss:11.979751586914062
epoch24: step500/4680
step 24000: accuracy:0.12700000405311584, confidence:0.9182025790214539, loss:8.65656852722168
epoch24: step1000/4680
step 36000: accuracy:0.07900000363588333, confidence:0.8708789348602295, loss:9.495683670043945
epoch24: step1500/4680
step 48000: accuracy:0.1120000034570694, confidence:0.8124069571495056, loss:10.664957046508789
epoch24: step2000/4680
step 60000: accuracy:0.1420000046491623, confidence:0.851142406463623, loss:6.63134241104126
epoch24: step2500/4680
step 72000: accuracy:0.07100000232458115, confidence:0.8223602175712585, loss:7.622405052185059
epoch24: step3000/4680
step 84000: accuracy:0.16200000047683716, confidence:0.8331612348556519, loss:8.070822715759277
epoch24: step3500/4680
step 96000: accuracy:0.09399999678134918, confidence:0.9705377221107483, loss:12.969841003417969
epoch24: step4000/4680
step 108000: accuracy:0.0989999994635582, confidence:0.9317757487297058, loss:10.869807243347168
epoch24: step4500/4680
step 0: accuracy:0.10300000011920929, confidence:0.9314420819282532, loss:10.686999320983887
epoch25: step0/4680
step 12500: accuracy:0.11400000005960464, confidence:0.9285200834274292, loss:10.224818229675293
epoch25: step500/4680
step 25000: accuracy:0.12300000339746475, confidence:0.8578003644943237, loss:8.089201927185059
epoch25: step1000/4680
step 37500: accuracy:0.0729999989271164, confidence:0.8511753082275391, loss:9.216878890991211
epoch25: step1500/4680
step 50000: accuracy:0.12700000405311584, confidence:0.8281868696212769, loss:11.551010131835938
epoch25: step2000/4680
step 62500: accuracy:0.14800000190734863, confidence:0.807829737663269, loss:7.672065258026123
epoch25: step2500/4680
step 75000: accuracy:0.08500000089406967, confidence:0.7787896990776062, loss:7.67313289642334
epoch25: step3000/4680
step 87500: accuracy:0.18700000643730164, confidence:0.8351492881774902, loss:7.410895347595215
epoch25: step3500/4680
step 100000: accuracy:0.09799999743700027, confidence:0.9505342245101929, loss:11.89699935913086
epoch25: step4000/4680
step 112500: accuracy:0.11299999803304672, confidence:0.915959358215332, loss:10.029176712036133
epoch25: step4500/4680
step 0: accuracy:0.09300000220537186, confidence:0.9000607132911682, loss:9.895879745483398
epoch26: step0/4680
step 13000: accuracy:0.1080000028014183, confidence:0.9445750117301941, loss:10.66065502166748
epoch26: step500/4680
step 26000: accuracy:0.11500000208616257, confidence:0.8970029354095459, loss:8.35291862487793
epoch26: step1000/4680
step 39000: accuracy:0.06499999761581421, confidence:0.8533827066421509, loss:9.277682304382324
epoch26: step1500/4680
step 52000: accuracy:0.11400000005960464, confidence:0.7791054248809814, loss:9.4274320602417
epoch26: step2000/4680
step 65000: accuracy:0.13899999856948853, confidence:0.780802845954895, loss:8.002511978149414
epoch26: step2500/4680
step 78000: accuracy:0.0860000029206276, confidence:0.8237693309783936, loss:7.225818157196045
epoch26: step3000/4680
step 91000: accuracy:0.16300000250339508, confidence:0.8394221067428589, loss:7.927035808563232
epoch26: step3500/4680
step 104000: accuracy:0.11999999731779099, confidence:0.9634197950363159, loss:10.742493629455566
epoch26: step4000/4680
step 117000: accuracy:0.10700000077486038, confidence:0.9288669228553772, loss:9.77956771850586
epoch26: step4500/4680
step 0: accuracy:0.09600000083446503, confidence:0.9133737683296204, loss:9.187785148620605
epoch27: step0/4680
step 13500: accuracy:0.10499999672174454, confidence:0.9290644526481628, loss:10.343838691711426
epoch27: step500/4680
step 27000: accuracy:0.12399999797344208, confidence:0.847592294216156, loss:7.8067402839660645
epoch27: step1000/4680
step 40500: accuracy:0.052000001072883606, confidence:0.8290010094642639, loss:9.022953033447266
epoch27: step1500/4680
step 54000: accuracy:0.15800000727176666, confidence:0.8181120157241821, loss:11.281259536743164
epoch27: step2000/4680
step 67500: accuracy:0.11599999666213989, confidence:0.7827744483947754, loss:7.002315998077393
epoch27: step2500/4680
step 81000: accuracy:0.07199999690055847, confidence:0.8162499070167542, loss:7.7901153564453125
epoch27: step3000/4680
step 94500: accuracy:0.16200000047683716, confidence:0.8413729071617126, loss:8.367318153381348
epoch27: step3500/4680
step 108000: accuracy:0.11599999666213989, confidence:0.9978210926055908, loss:15.998025894165039
epoch27: step4000/4680
step 121500: accuracy:0.1469999998807907, confidence:0.8708962202072144, loss:8.221713066101074
epoch27: step4500/4680
step 0: accuracy:0.13099999725818634, confidence:0.8698264956474304, loss:8.20358943939209
epoch28: step0/4680
step 14000: accuracy:0.11599999666213989, confidence:0.917420506477356, loss:9.775995254516602
epoch28: step500/4680
step 28000: accuracy:0.10899999737739563, confidence:0.9364915490150452, loss:9.587225914001465
epoch28: step1000/4680
step 42000: accuracy:0.06199999898672104, confidence:0.8536324501037598, loss:9.138871192932129
epoch28: step1500/4680
step 56000: accuracy:0.1340000033378601, confidence:0.8329390287399292, loss:11.272006034851074
epoch28: step2000/4680
step 70000: accuracy:0.12800000607967377, confidence:0.8818548321723938, loss:7.420104026794434
epoch28: step2500/4680
step 84000: accuracy:0.09200000017881393, confidence:0.8450270891189575, loss:7.741777420043945
epoch28: step3000/4680
step 98000: accuracy:0.16500000655651093, confidence:0.8394330739974976, loss:8.155632019042969
epoch28: step3500/4680
step 112000: accuracy:0.10300000011920929, confidence:0.9565151333808899, loss:12.451193809509277
epoch28: step4000/4680
step 126000: accuracy:0.1120000034570694, confidence:0.924129843711853, loss:10.385018348693848
epoch28: step4500/4680
step 0: accuracy:0.1120000034570694, confidence:0.914355456829071, loss:9.856758117675781
epoch29: step0/4680
step 14500: accuracy:0.08299999684095383, confidence:0.9530678987503052, loss:11.736456871032715
epoch29: step500/4680
step 29000: accuracy:0.13600000739097595, confidence:0.8474699258804321, loss:7.4121198654174805
epoch29: step1000/4680
step 43500: accuracy:0.05299999937415123, confidence:0.8388561606407166, loss:8.627203941345215
epoch29: step1500/4680
step 58000: accuracy:0.13500000536441803, confidence:0.8178637027740479, loss:11.260838508605957
epoch29: step2000/4680
step 72500: accuracy:0.13500000536441803, confidence:0.8301882743835449, loss:7.451655387878418
epoch29: step2500/4680
step 87000: accuracy:0.08900000154972076, confidence:0.858554482460022, loss:8.645687103271484
epoch29: step3000/4680
step 101500: accuracy:0.1770000010728836, confidence:0.8365466594696045, loss:7.984239101409912
epoch29: step3500/4680
step 116000: accuracy:0.11900000274181366, confidence:0.9566310048103333, loss:11.523117065429688
epoch29: step4000/4680
step 130500: accuracy:0.10400000214576721, confidence:0.9259139895439148, loss:10.362872123718262
epoch29: step4500/4680
2018-06-15 17:37:24.197312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000a:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 17:37:24.197366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
2018-06-15 17:37:25.623608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
CHEKPOINT DIR: uniform
model has been loaded from fashion-mnist_classifier.pkl
---------
Variables: name (type shape) [size]
---------
Variable:0 (float32_ref 5x5x1x32) [800, bytes: 3200]
Variable_1:0 (float32_ref 32) [32, bytes: 128]
Variable_2:0 (float32_ref 5x5x32x64) [51200, bytes: 204800]
Variable_3:0 (float32_ref 64) [64, bytes: 256]
Variable_4:0 (float32_ref 3136x1024) [3211264, bytes: 12845056]
Variable_5:0 (float32_ref 1024) [1024, bytes: 4096]
Variable_6:0 (float32_ref 1024x10) [10240, bytes: 40960]
Variable_7:0 (float32_ref 10) [10, bytes: 40]
discriminator/d_conv1/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]
discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]
discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]
discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]
generator/g_fc1/Matrix:0 (float32_ref 74x1024) [75776, bytes: 303104]
generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]
generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]
generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]
classifier/c_fc1/Matrix:0 (float32_ref 1024x64) [65536, bytes: 262144]
classifier/c_fc1/bias:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/beta:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/gamma:0 (float32_ref 64) [64, bytes: 256]
classifier/c_fc2/Matrix:0 (float32_ref 64x12) [768, bytes: 3072]
classifier/c_fc2/bias:0 (float32_ref 12) [12, bytes: 48]
Total size of variables: 16552792
Total bytes of variables: 66211168
 [*] Reading checkpoints...
 [*] Success to read MultiModalInfoGAN.model-43721
 [*] Load SUCCESS


SAMPLES SIZE=4680,LABELS=299520


SAVED TRAINING SET generated_training_set_fashion-mnist_UniformSample_mu_0_sigma_0.15_czcc_rzcc_czrc_rzrc
 [*] Training finished!
 [*] Testing finished!
2018-06-15 17:38:15.488037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000a:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 17:38:15.488226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
model has been loaded from fashion-mnist_classifier.pkl
argmax:[7 7 7 ..., 7 7 7]
step 0: accuracy:0.00279999990016222, confidence:0.9966154098510742, loss:8.930298805236816
Assinging:8
[   0    0    0    0    0    0    0 9972    0   28]
argmax:[3 3 3 ..., 3 3 3]
step 0: accuracy:0.0, confidence:0.99849933385849, loss:12.398773193359375
Assinging:4
[    0     0     0 10000]
argmax:[4 4 4 ..., 4 4 4]
step 0: accuracy:0.0, confidence:0.7333278059959412, loss:12.49646282196045
Assinging:5
[   0    0   69    0 9436    0  495]
argmax:[7 7 7 ..., 7 7 7]
step 0: accuracy:0.0, confidence:0.9991472363471985, loss:12.881529808044434
Assinging:8
[    0     0     0     0     0     0     0 10000]
argmax:[8 8 8 ..., 8 8 8]
step 0: accuracy:0.0, confidence:0.9999984502792358, loss:18.03530502319336
Assinging:9
[    0     0     0     0     0     0     0     0 10000]
argmax:[1 1 1 ..., 1 1 1]
step 0: accuracy:0.0, confidence:0.9999997019767761, loss:22.120338439941406
Assinging:2
[    0 10000]
argmax:[0 0 0 ..., 0 0 0]
step 0: accuracy:0.0, confidence:0.9982784986495972, loss:19.026287078857422
Assinging:1
[9973    0    0    0    0    0   27]
argmax:[4 4 4 ..., 4 4 4]
step 0: accuracy:0.0, confidence:0.9923480749130249, loss:13.898530960083008
Assinging:5
[   0    0   17    8 9946    0   29]
argmax:[2 2 2 ..., 4 4 2]
step 0: accuracy:0.0, confidence:0.7929224967956543, loss:12.912821769714355
Assinging:3
[   0    0 8741    0 1259]
argmax:[9 9 9 ..., 9 9 9]
step 0: accuracy:1.0, confidence:0.9999990463256836, loss:8.733385925552284e-07
Assinging:10
[    0     0     0     0     0     0     0     0     0 10000]
2018-06-15 17:38:40.242980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000a:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 17:38:40.243154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
step 0: accuracy:0.13500000536441803, confidence:0.7976433038711548, loss:8.98600959777832
epoch0: step0/4680
step 0: accuracy:0.09200000017881393, confidence:1.0, loss:37.601322174072266
epoch0: step500/4680
step 0: accuracy:0.09399999678134918, confidence:0.9999957084655762, loss:17.115188598632812
epoch0: step1000/4680
step 0: accuracy:0.09000000357627869, confidence:0.9995513558387756, loss:9.755451202392578
epoch0: step1500/4680
step 0: accuracy:0.0860000029206276, confidence:0.9999901652336121, loss:13.117478370666504
epoch0: step2000/4680
step 0: accuracy:0.10199999809265137, confidence:0.999686598777771, loss:9.746280670166016
epoch0: step2500/4680
step 0: accuracy:0.11599999666213989, confidence:0.9994432926177979, loss:9.3677396774292
epoch0: step3000/4680
step 0: accuracy:0.0949999988079071, confidence:0.9634213447570801, loss:5.372450828552246
epoch0: step3500/4680
step 0: accuracy:0.09799999743700027, confidence:0.9936893582344055, loss:7.436273574829102
epoch0: step4000/4680
step 0: accuracy:0.09700000286102295, confidence:0.9958987236022949, loss:7.183286666870117
epoch0: step4500/4680
step 0: accuracy:0.0860000029206276, confidence:0.9973688125610352, loss:7.6622185707092285
epoch1: step0/4680
step 500: accuracy:0.10899999737739563, confidence:0.9998404383659363, loss:10.424927711486816
epoch1: step500/4680
step 1000: accuracy:0.11400000005960464, confidence:0.9994272589683533, loss:9.371838569641113
epoch1: step1000/4680
step 1500: accuracy:0.11400000005960464, confidence:0.9575468897819519, loss:5.295294284820557
epoch1: step1500/4680
step 2000: accuracy:0.11500000208616257, confidence:0.9968925714492798, loss:7.626521110534668
epoch1: step2000/4680
step 2500: accuracy:0.10599999874830246, confidence:0.9255130290985107, loss:4.71379280090332
epoch1: step2500/4680
step 3000: accuracy:0.11100000143051147, confidence:0.9744133949279785, loss:5.992001533508301
epoch1: step3000/4680
step 3500: accuracy:0.07000000029802322, confidence:0.9480589628219604, loss:5.3552961349487305
epoch1: step3500/4680
step 4000: accuracy:0.09000000357627869, confidence:0.9583081007003784, loss:5.521231174468994
epoch1: step4000/4680
step 4500: accuracy:0.09200000017881393, confidence:0.9575715065002441, loss:5.297447681427002
epoch1: step4500/4680
step 0: accuracy:0.10899999737739563, confidence:0.9858591556549072, loss:6.19649600982666
epoch2: step0/4680
step 1000: accuracy:0.0949999988079071, confidence:0.9506343603134155, loss:5.143709659576416
epoch2: step500/4680
step 2000: accuracy:0.10999999940395355, confidence:0.999847412109375, loss:11.094898223876953
epoch2: step1000/4680
step 3000: accuracy:0.10300000011920929, confidence:0.9787023067474365, loss:6.800918102264404
epoch2: step1500/4680
step 4000: accuracy:0.0989999994635582, confidence:0.861574113368988, loss:4.5073561668396
epoch2: step2000/4680
step 5000: accuracy:0.11599999666213989, confidence:0.6177371144294739, loss:3.317535161972046
epoch2: step2500/4680
step 6000: accuracy:0.12800000607967377, confidence:0.9907628297805786, loss:7.202660083770752
epoch2: step3000/4680
step 7000: accuracy:0.0949999988079071, confidence:0.9324784874916077, loss:5.051138877868652
epoch2: step3500/4680
step 8000: accuracy:0.08699999749660492, confidence:0.936265230178833, loss:5.48205041885376
epoch2: step4000/4680
step 9000: accuracy:0.0949999988079071, confidence:0.9222326278686523, loss:4.8602614402771
epoch2: step4500/4680
step 0: accuracy:0.11599999666213989, confidence:0.9735270738601685, loss:5.7111101150512695
epoch3: step0/4680
step 1500: accuracy:0.08299999684095383, confidence:0.9390041828155518, loss:5.110891342163086
epoch3: step500/4680
step 3000: accuracy:0.10899999737739563, confidence:0.9964596629142761, loss:9.182465553283691
epoch3: step1000/4680
step 4500: accuracy:0.10700000077486038, confidence:0.8649131059646606, loss:6.095714092254639
epoch3: step1500/4680
step 6000: accuracy:0.10199999809265137, confidence:0.9043861627578735, loss:5.737505912780762
epoch3: step2000/4680
step 7500: accuracy:0.10999999940395355, confidence:0.6047382354736328, loss:3.764338970184326
epoch3: step2500/4680
step 9000: accuracy:0.10300000011920929, confidence:0.9835622310638428, loss:7.213943004608154
epoch3: step3000/4680
step 10500: accuracy:0.09399999678134918, confidence:0.9700158834457397, loss:5.938860893249512
epoch3: step3500/4680
step 12000: accuracy:0.09200000017881393, confidence:0.9023715257644653, loss:4.9356465339660645
epoch3: step4000/4680
step 13500: accuracy:0.09799999743700027, confidence:0.87880539894104, loss:4.460583209991455
epoch3: step4500/4680
step 0: accuracy:0.09300000220537186, confidence:0.9469360113143921, loss:5.2356462478637695
epoch4: step0/4680
step 2000: accuracy:0.08299999684095383, confidence:0.9752001762390137, loss:6.189074993133545
epoch4: step500/4680
step 4000: accuracy:0.10000000149011612, confidence:0.9955085515975952, loss:8.798178672790527
epoch4: step1000/4680
step 6000: accuracy:0.08900000154972076, confidence:0.7397831082344055, loss:6.013731956481934
epoch4: step1500/4680
step 8000: accuracy:0.10300000011920929, confidence:0.8891124129295349, loss:5.8110480308532715
epoch4: step2000/4680
step 10000: accuracy:0.10599999874830246, confidence:0.6256062984466553, loss:3.967285633087158
epoch4: step2500/4680
step 12000: accuracy:0.11699999868869781, confidence:0.9864896535873413, loss:7.250487327575684
epoch4: step3000/4680
step 14000: accuracy:0.10999999940395355, confidence:0.9416982531547546, loss:5.205148696899414
epoch4: step3500/4680
step 16000: accuracy:0.09099999815225601, confidence:0.8849133849143982, loss:4.900963306427002
epoch4: step4000/4680
step 18000: accuracy:0.1080000028014183, confidence:0.786880373954773, loss:3.9016897678375244
epoch4: step4500/4680
step 0: accuracy:0.08500000089406967, confidence:0.8884268403053284, loss:4.666555881500244
epoch5: step0/4680
step 2500: accuracy:0.10100000351667404, confidence:0.8621484041213989, loss:4.882978916168213
epoch5: step500/4680
step 5000: accuracy:0.10700000077486038, confidence:0.9974316954612732, loss:10.376093864440918
epoch5: step1000/4680
step 7500: accuracy:0.0820000022649765, confidence:0.7636505365371704, loss:7.697976112365723
epoch5: step1500/4680
step 10000: accuracy:0.10300000011920929, confidence:0.988211989402771, loss:8.565688133239746
epoch5: step2000/4680
step 12500: accuracy:0.10499999672174454, confidence:0.7307858467102051, loss:4.522819519042969
epoch5: step2500/4680
step 15000: accuracy:0.10199999809265137, confidence:0.989547848701477, loss:7.751946926116943
epoch5: step3000/4680
step 17500: accuracy:0.09600000083446503, confidence:0.8777730464935303, loss:5.364409923553467
epoch5: step3500/4680
step 20000: accuracy:0.09700000286102295, confidence:0.9633510708808899, loss:5.986796855926514
epoch5: step4000/4680
step 22500: accuracy:0.0989999994635582, confidence:0.9484253525733948, loss:5.32391881942749
epoch5: step4500/4680
step 0: accuracy:0.11500000208616257, confidence:0.9782310724258423, loss:5.890902519226074
epoch6: step0/4680
step 3000: accuracy:0.0949999988079071, confidence:0.8404083847999573, loss:4.786303997039795
epoch6: step500/4680
step 6000: accuracy:0.10700000077486038, confidence:0.9851416945457458, loss:8.645222663879395
epoch6: step1000/4680
step 9000: accuracy:0.05700000002980232, confidence:0.7757028937339783, loss:7.448614597320557
epoch6: step1500/4680
step 12000: accuracy:0.11500000208616257, confidence:0.906506359577179, loss:7.0490593910217285
epoch6: step2000/4680
step 15000: accuracy:0.11299999803304672, confidence:0.8475818634033203, loss:6.138253688812256
epoch6: step2500/4680
step 18000: accuracy:0.125, confidence:0.9987165927886963, loss:9.317792892456055
epoch6: step3000/4680
step 21000: accuracy:0.09099999815225601, confidence:0.931155264377594, loss:5.737812519073486
epoch6: step3500/4680
step 24000: accuracy:0.10199999809265137, confidence:0.9819096922874451, loss:6.850363731384277
epoch6: step4000/4680
step 27000: accuracy:0.0820000022649765, confidence:0.927042543888092, loss:5.106084823608398
epoch6: step4500/4680
step 0: accuracy:0.11900000274181366, confidence:0.9638351798057556, loss:5.53776741027832
epoch7: step0/4680
step 3500: accuracy:0.09799999743700027, confidence:0.8146188855171204, loss:4.510308265686035
epoch7: step500/4680
step 7000: accuracy:0.09000000357627869, confidence:0.9377954006195068, loss:8.508387565612793
epoch7: step1000/4680
step 10500: accuracy:0.05400000140070915, confidence:0.7552314400672913, loss:7.2217535972595215
epoch7: step1500/4680
step 14000: accuracy:0.10999999940395355, confidence:0.9555445909500122, loss:8.171714782714844
epoch7: step2000/4680
step 17500: accuracy:0.08900000154972076, confidence:0.8301315903663635, loss:5.121257305145264
epoch7: step2500/4680
step 21000: accuracy:0.09399999678134918, confidence:0.9976118803024292, loss:10.515338897705078
epoch7: step3000/4680
step 24500: accuracy:0.10100000351667404, confidence:0.8963738083839417, loss:5.650053024291992
epoch7: step3500/4680
step 28000: accuracy:0.10100000351667404, confidence:0.9521461129188538, loss:6.337787628173828
epoch7: step4000/4680
step 31500: accuracy:0.0860000029206276, confidence:0.9160356521606445, loss:5.246924877166748
epoch7: step4500/4680
step 0: accuracy:0.0949999988079071, confidence:0.956040620803833, loss:5.7804365158081055
epoch8: step0/4680
step 4000: accuracy:0.09399999678134918, confidence:0.7500273585319519, loss:4.450590133666992
epoch8: step500/4680
step 8000: accuracy:0.08299999684095383, confidence:0.9684246778488159, loss:8.571368217468262
epoch8: step1000/4680
step 12000: accuracy:0.061000000685453415, confidence:0.7687877416610718, loss:7.063692092895508
epoch8: step1500/4680
step 16000: accuracy:0.09200000017881393, confidence:0.9634511470794678, loss:8.421177864074707
epoch8: step2000/4680
step 20000: accuracy:0.11400000005960464, confidence:0.8542096018791199, loss:5.435202598571777
epoch8: step2500/4680
step 24000: accuracy:0.1080000028014183, confidence:0.999405562877655, loss:11.533050537109375
epoch8: step3000/4680
step 28000: accuracy:0.10700000077486038, confidence:0.9035136699676514, loss:5.723180770874023
epoch8: step3500/4680
step 32000: accuracy:0.09399999678134918, confidence:0.9574329257011414, loss:6.596525192260742
epoch8: step4000/4680
step 36000: accuracy:0.10400000214576721, confidence:0.9239363670349121, loss:5.3353986740112305
epoch8: step4500/4680
step 0: accuracy:0.10700000077486038, confidence:0.9440175294876099, loss:5.53750467300415
epoch9: step0/4680
step 4500: accuracy:0.10599999874830246, confidence:0.778503954410553, loss:4.412929534912109
epoch9: step500/4680
step 9000: accuracy:0.10999999940395355, confidence:0.8678333759307861, loss:6.846987724304199
epoch9: step1000/4680
step 13500: accuracy:0.04600000008940697, confidence:0.6957411766052246, loss:5.979152202606201
epoch9: step1500/4680
step 18000: accuracy:0.10300000011920929, confidence:0.8451201915740967, loss:6.171672821044922
epoch9: step2000/4680
step 22500: accuracy:0.1080000028014183, confidence:0.8171384334564209, loss:6.153096675872803
epoch9: step2500/4680
step 27000: accuracy:0.09799999743700027, confidence:0.9995889067649841, loss:11.359519004821777
epoch9: step3000/4680
step 31500: accuracy:0.17299999296665192, confidence:0.8141460418701172, loss:5.741081714630127
epoch9: step3500/4680
step 36000: accuracy:0.0949999988079071, confidence:0.9618757963180542, loss:6.730697154998779
epoch9: step4000/4680
step 40500: accuracy:0.10700000077486038, confidence:0.9648391008377075, loss:6.1379618644714355
epoch9: step4500/4680
step 0: accuracy:0.10300000011920929, confidence:0.9788858890533447, loss:6.611583232879639
epoch10: step0/4680
step 5000: accuracy:0.10599999874830246, confidence:0.7651835680007935, loss:4.678142547607422
epoch10: step500/4680
step 10000: accuracy:0.12099999934434891, confidence:0.8855432271957397, loss:7.411596775054932
epoch10: step1000/4680
step 15000: accuracy:0.05700000002980232, confidence:0.6756855845451355, loss:6.159666538238525
epoch10: step1500/4680
step 20000: accuracy:0.11500000208616257, confidence:0.9340851902961731, loss:7.716333866119385
epoch10: step2000/4680
step 25000: accuracy:0.09799999743700027, confidence:0.8299611210823059, loss:5.121532440185547
epoch10: step2500/4680
step 30000: accuracy:0.1120000034570694, confidence:0.9977292418479919, loss:11.4884614944458
epoch10: step3000/4680
step 35000: accuracy:0.15800000727176666, confidence:0.7907041311264038, loss:5.681985378265381
epoch10: step3500/4680
step 40000: accuracy:0.10300000011920929, confidence:0.9406341314315796, loss:6.80919885635376
epoch10: step4000/4680
step 45000: accuracy:0.1080000028014183, confidence:0.9742024540901184, loss:6.717409610748291
epoch10: step4500/4680
step 0: accuracy:0.10199999809265137, confidence:0.9838319420814514, loss:7.331940650939941
epoch11: step0/4680
step 5500: accuracy:0.08399999886751175, confidence:0.7833518385887146, loss:5.184162616729736
epoch11: step500/4680
step 11000: accuracy:0.11599999666213989, confidence:0.9239867329597473, loss:8.05014705657959
epoch11: step1000/4680
step 16500: accuracy:0.10599999874830246, confidence:0.6786616444587708, loss:5.865877628326416
epoch11: step1500/4680
step 22000: accuracy:0.10400000214576721, confidence:0.9647589325904846, loss:8.428421974182129
epoch11: step2000/4680
step 27500: accuracy:0.11500000208616257, confidence:0.8006184101104736, loss:5.394941329956055
epoch11: step2500/4680
step 33000: accuracy:0.12300000339746475, confidence:0.9989330172538757, loss:10.847062110900879
epoch11: step3000/4680
step 38500: accuracy:0.15700000524520874, confidence:0.7559484243392944, loss:6.046502113342285
epoch11: step3500/4680
step 44000: accuracy:0.15399999916553497, confidence:0.8779268860816956, loss:6.07788610458374
epoch11: step4000/4680
step 49500: accuracy:0.10400000214576721, confidence:0.9737390875816345, loss:7.105521202087402
epoch11: step4500/4680
step 0: accuracy:0.0989999994635582, confidence:0.982232928276062, loss:7.4913716316223145
epoch12: step0/4680
step 6000: accuracy:0.08799999952316284, confidence:0.7822901606559753, loss:5.28032112121582
epoch12: step500/4680
step 12000: accuracy:0.08399999886751175, confidence:0.9608839750289917, loss:9.713517189025879
epoch12: step1000/4680
step 18000: accuracy:0.10100000351667404, confidence:0.6703407764434814, loss:5.867680072784424
epoch12: step1500/4680
step 24000: accuracy:0.09000000357627869, confidence:0.8688212037086487, loss:6.928610324859619
epoch12: step2000/4680
step 30000: accuracy:0.07900000363588333, confidence:0.7301154732704163, loss:5.274261951446533
epoch12: step2500/4680
step 36000: accuracy:0.11100000143051147, confidence:0.9989593029022217, loss:13.699459075927734
epoch12: step3000/4680
step 42000: accuracy:0.12099999934434891, confidence:0.8032302856445312, loss:6.0990166664123535
epoch12: step3500/4680
step 48000: accuracy:0.164000004529953, confidence:0.8778206706047058, loss:6.098966121673584
epoch12: step4000/4680
step 54000: accuracy:0.08699999749660492, confidence:0.9472236037254333, loss:6.834231376647949
epoch12: step4500/4680
step 0: accuracy:0.09799999743700027, confidence:0.9640911817550659, loss:7.0006937980651855
epoch13: step0/4680
step 6500: accuracy:0.0989999994635582, confidence:0.8191198706626892, loss:5.752711296081543
epoch13: step500/4680
step 13000: accuracy:0.10999999940395355, confidence:0.9801687598228455, loss:11.065820693969727
epoch13: step1000/4680
step 19500: accuracy:0.09700000286102295, confidence:0.6422923803329468, loss:5.596439838409424
epoch13: step1500/4680
step 26000: accuracy:0.0989999994635582, confidence:0.947515070438385, loss:8.083306312561035
epoch13: step2000/4680
step 32500: accuracy:0.10300000011920929, confidence:0.7027572393417358, loss:4.9644060134887695
epoch13: step2500/4680
step 39000: accuracy:0.1120000034570694, confidence:0.9923930764198303, loss:12.669879913330078
epoch13: step3000/4680
step 45500: accuracy:0.17100000381469727, confidence:0.7247716784477234, loss:6.2143635749816895
epoch13: step3500/4680
step 52000: accuracy:0.14499999582767487, confidence:0.8594345450401306, loss:6.14013671875
epoch13: step4000/4680
step 58500: accuracy:0.1550000011920929, confidence:0.8563359975814819, loss:5.419548511505127
epoch13: step4500/4680
step 0: accuracy:0.10700000077486038, confidence:0.896064817905426, loss:6.101457595825195
epoch14: step0/4680
step 7000: accuracy:0.08399999886751175, confidence:0.7772829532623291, loss:5.2552571296691895
epoch14: step500/4680
step 14000: accuracy:0.09799999743700027, confidence:0.9861892461776733, loss:11.964240074157715
epoch14: step1000/4680
step 21000: accuracy:0.09300000220537186, confidence:0.7090495824813843, loss:7.334200382232666
epoch14: step1500/4680
step 28000: accuracy:0.11699999868869781, confidence:0.9760565757751465, loss:9.721272468566895
epoch14: step2000/4680
step 35000: accuracy:0.0860000029206276, confidence:0.7584605813026428, loss:5.563955307006836
epoch14: step2500/4680
step 42000: accuracy:0.11500000208616257, confidence:0.9997460246086121, loss:14.967744827270508
epoch14: step3000/4680
step 49000: accuracy:0.08699999749660492, confidence:0.8441281914710999, loss:6.186176300048828
epoch14: step3500/4680
step 56000: accuracy:0.20399999618530273, confidence:0.8202168345451355, loss:5.466789722442627
epoch14: step4000/4680
step 63000: accuracy:0.125, confidence:0.8588593602180481, loss:5.867671489715576
epoch14: step4500/4680
step 0: accuracy:0.10999999940395355, confidence:0.9075688719749451, loss:6.177877426147461
epoch15: step0/4680
step 7500: accuracy:0.08799999952316284, confidence:0.79649418592453, loss:5.684016704559326
epoch15: step500/4680
step 15000: accuracy:0.10499999672174454, confidence:0.9541759490966797, loss:11.284296989440918
epoch15: step1000/4680
step 22500: accuracy:0.10999999940395355, confidence:0.8474326729774475, loss:8.389081954956055
epoch15: step1500/4680
step 30000: accuracy:0.09000000357627869, confidence:0.9964669346809387, loss:12.066938400268555
epoch15: step2000/4680
step 37500: accuracy:0.10599999874830246, confidence:0.8080198168754578, loss:5.903825283050537
epoch15: step2500/4680
step 45000: accuracy:0.10999999940395355, confidence:0.9974161386489868, loss:13.359419822692871
epoch15: step3000/4680
step 52500: accuracy:0.14499999582767487, confidence:0.6712194085121155, loss:6.186470985412598
epoch15: step3500/4680
step 60000: accuracy:0.1679999977350235, confidence:0.7735224962234497, loss:5.337563991546631
epoch15: step4000/4680
step 67500: accuracy:0.11599999666213989, confidence:0.8270052075386047, loss:5.796854019165039
epoch15: step4500/4680
step 0: accuracy:0.10300000011920929, confidence:0.8629416227340698, loss:6.17448616027832
epoch16: step0/4680
step 8000: accuracy:0.04899999871850014, confidence:0.7873890399932861, loss:5.765603542327881
epoch16: step500/4680
step 16000: accuracy:0.10000000149011612, confidence:0.909332811832428, loss:10.110947608947754
epoch16: step1000/4680
step 24000: accuracy:0.10700000077486038, confidence:0.7772959470748901, loss:8.426434516906738
epoch16: step1500/4680
step 32000: accuracy:0.050999999046325684, confidence:0.8018189668655396, loss:7.3644256591796875
epoch16: step2000/4680
step 40000: accuracy:0.10400000214576721, confidence:0.9326143264770508, loss:8.069397926330566
epoch16: step2500/4680
step 48000: accuracy:0.12399999797344208, confidence:0.9992291331291199, loss:12.70942497253418
epoch16: step3000/4680
step 56000: accuracy:0.11100000143051147, confidence:0.7384513020515442, loss:7.1179890632629395
epoch16: step3500/4680
step 64000: accuracy:0.20800000429153442, confidence:0.8099650144577026, loss:5.346704959869385
epoch16: step4000/4680
step 72000: accuracy:0.08900000154972076, confidence:0.9495517611503601, loss:7.054110527038574
epoch16: step4500/4680
step 0: accuracy:0.10999999940395355, confidence:0.9682955145835876, loss:7.5003886222839355
epoch17: step0/4680
step 8500: accuracy:0.08699999749660492, confidence:0.7873410582542419, loss:5.519988536834717
epoch17: step500/4680
step 17000: accuracy:0.10000000149011612, confidence:0.9477126002311707, loss:11.682458877563477
epoch17: step1000/4680
step 25500: accuracy:0.1080000028014183, confidence:0.6667082905769348, loss:6.459937572479248
epoch17: step1500/4680
step 34000: accuracy:0.07900000363588333, confidence:0.8655475378036499, loss:7.431258201599121
epoch17: step2000/4680
step 42500: accuracy:0.08399999886751175, confidence:0.7188218235969543, loss:5.359030246734619
epoch17: step2500/4680
step 51000: accuracy:0.10700000077486038, confidence:0.9998059868812561, loss:17.429407119750977
epoch17: step3000/4680
step 59500: accuracy:0.10199999809265137, confidence:0.9319920539855957, loss:7.352157115936279
epoch17: step3500/4680
step 68000: accuracy:0.1860000044107437, confidence:0.8022352457046509, loss:5.519144058227539
epoch17: step4000/4680
step 76500: accuracy:0.1120000034570694, confidence:0.8828867077827454, loss:6.097493648529053
epoch17: step4500/4680
step 0: accuracy:0.08900000154972076, confidence:0.9269108772277832, loss:6.8976263999938965
epoch18: step0/4680
step 9000: accuracy:0.09000000357627869, confidence:0.8106712698936462, loss:6.598917484283447
epoch18: step500/4680
step 18000: accuracy:0.10700000077486038, confidence:0.9272198677062988, loss:10.890742301940918
epoch18: step1000/4680
step 27000: accuracy:0.09399999678134918, confidence:0.698040246963501, loss:7.423000812530518
epoch18: step1500/4680
step 36000: accuracy:0.10700000077486038, confidence:0.9420275092124939, loss:8.824328422546387
epoch18: step2000/4680
step 45000: accuracy:0.09700000286102295, confidence:0.7327075004577637, loss:5.474503993988037
epoch18: step2500/4680
step 54000: accuracy:0.11299999803304672, confidence:0.9996935129165649, loss:16.702510833740234
epoch18: step3000/4680
step 63000: accuracy:0.10100000351667404, confidence:0.7537457942962646, loss:6.755001068115234
epoch18: step3500/4680
step 72000: accuracy:0.19099999964237213, confidence:0.8300781846046448, loss:6.033144474029541
epoch18: step4000/4680
step 81000: accuracy:0.09099999815225601, confidence:0.9180804491043091, loss:6.824853420257568
epoch18: step4500/4680
step 0: accuracy:0.08299999684095383, confidence:0.9439582228660583, loss:7.285873889923096
epoch19: step0/4680
step 9500: accuracy:0.08900000154972076, confidence:0.8367776274681091, loss:6.609477519989014
epoch19: step500/4680
step 19000: accuracy:0.08399999886751175, confidence:0.9255923628807068, loss:11.062457084655762
epoch19: step1000/4680
step 28500: accuracy:0.08299999684095383, confidence:0.721413254737854, loss:7.267494201660156
epoch19: step1500/4680
step 38000: accuracy:0.09000000357627869, confidence:0.8452209234237671, loss:7.6049957275390625
epoch19: step2000/4680
step 47500: accuracy:0.09000000357627869, confidence:0.7405408620834351, loss:5.590099334716797
epoch19: step2500/4680
step 57000: accuracy:0.11500000208616257, confidence:0.9999774098396301, loss:19.893104553222656
epoch19: step3000/4680
step 66500: accuracy:0.07900000363588333, confidence:0.958757221698761, loss:8.919417381286621
epoch19: step3500/4680
step 76000: accuracy:0.15299999713897705, confidence:0.8368999361991882, loss:5.991583347320557
epoch19: step4000/4680
step 85500: accuracy:0.10899999737739563, confidence:0.9198017716407776, loss:6.706648349761963
epoch19: step4500/4680
step 0: accuracy:0.10000000149011612, confidence:0.9419984817504883, loss:7.299241065979004
epoch20: step0/4680
step 10000: accuracy:0.11599999666213989, confidence:0.8590848445892334, loss:7.207693576812744
epoch20: step500/4680
step 20000: accuracy:0.09200000017881393, confidence:0.9233623147010803, loss:11.292250633239746
epoch20: step1000/4680
step 30000: accuracy:0.07900000363588333, confidence:0.7024413347244263, loss:6.6003007888793945
epoch20: step1500/4680
step 40000: accuracy:0.06800000369548798, confidence:0.9801258444786072, loss:10.25277328491211
epoch20: step2000/4680
step 50000: accuracy:0.10700000077486038, confidence:0.7662993669509888, loss:5.7072272300720215
epoch20: step2500/4680
step 60000: accuracy:0.09399999678134918, confidence:0.9990929961204529, loss:15.250495910644531
epoch20: step3000/4680
step 70000: accuracy:0.10100000351667404, confidence:0.900401771068573, loss:9.011241912841797
epoch20: step3500/4680
step 80000: accuracy:0.1850000023841858, confidence:0.7922321557998657, loss:5.609217166900635
epoch20: step4000/4680
step 90000: accuracy:0.11999999731779099, confidence:0.8870135545730591, loss:6.64505672454834
epoch20: step4500/4680
step 0: accuracy:0.08299999684095383, confidence:0.9157270193099976, loss:7.048120498657227
epoch21: step0/4680
step 10500: accuracy:0.07400000095367432, confidence:0.8243789076805115, loss:6.425760746002197
epoch21: step500/4680
step 21000: accuracy:0.11299999803304672, confidence:0.9180572628974915, loss:11.259698867797852
epoch21: step1000/4680
step 31500: accuracy:0.08100000023841858, confidence:0.7430368065834045, loss:7.9579668045043945
epoch21: step1500/4680
step 42000: accuracy:0.07900000363588333, confidence:0.808781623840332, loss:7.546450614929199
epoch21: step2000/4680
step 52500: accuracy:0.0989999994635582, confidence:0.7251327037811279, loss:5.646536827087402
epoch21: step2500/4680
step 63000: accuracy:0.12600000202655792, confidence:0.9999799132347107, loss:20.728517532348633
epoch21: step3000/4680
step 73500: accuracy:0.07800000160932541, confidence:0.9960893988609314, loss:11.078763961791992
epoch21: step3500/4680
step 84000: accuracy:0.16500000655651093, confidence:0.7572974562644958, loss:5.571457862854004
epoch21: step4000/4680
step 94500: accuracy:0.18199999630451202, confidence:0.7694612145423889, loss:5.436853408813477
epoch21: step4500/4680
step 0: accuracy:0.13899999856948853, confidence:0.8270925283432007, loss:5.984635353088379
epoch22: step0/4680
step 11000: accuracy:0.10599999874830246, confidence:0.8658546209335327, loss:7.410006999969482
epoch22: step500/4680
step 22000: accuracy:0.10300000011920929, confidence:0.9587280750274658, loss:12.840105056762695
epoch22: step1000/4680
step 33000: accuracy:0.06400000303983688, confidence:0.7365885972976685, loss:6.014074325561523
epoch22: step1500/4680
step 44000: accuracy:0.09399999678134918, confidence:0.9085229635238647, loss:7.826237678527832
epoch22: step2000/4680
step 55000: accuracy:0.10199999809265137, confidence:0.7714108824729919, loss:6.214200019836426
epoch22: step2500/4680
step 66000: accuracy:0.11299999803304672, confidence:0.9997928738594055, loss:18.278221130371094
epoch22: step3000/4680
step 77000: accuracy:0.12600000202655792, confidence:0.6882280707359314, loss:7.498823165893555
epoch22: step3500/4680
step 88000: accuracy:0.17399999499320984, confidence:0.6706762909889221, loss:4.803244590759277
epoch22: step4000/4680
step 99000: accuracy:0.14300000667572021, confidence:0.8040841817855835, loss:5.618106365203857
epoch22: step4500/4680
step 0: accuracy:0.09600000083446503, confidence:0.8706457614898682, loss:6.723140716552734
epoch23: step0/4680
step 11500: accuracy:0.11500000208616257, confidence:0.8470351099967957, loss:6.982397079467773
epoch23: step500/4680
step 23000: accuracy:0.10400000214576721, confidence:0.9465693831443787, loss:12.198308944702148
epoch23: step1000/4680
step 34500: accuracy:0.08299999684095383, confidence:0.7317863702774048, loss:7.2707390785217285
epoch23: step1500/4680
step 46000: accuracy:0.10499999672174454, confidence:0.9100961685180664, loss:8.570389747619629
epoch23: step2000/4680
step 57500: accuracy:0.09200000017881393, confidence:0.7798506021499634, loss:6.348055839538574
epoch23: step2500/4680
step 69000: accuracy:0.10999999940395355, confidence:0.9999582767486572, loss:21.405517578125
epoch23: step3000/4680
step 80500: accuracy:0.0860000029206276, confidence:0.9668858647346497, loss:10.723245620727539
epoch23: step3500/4680
step 92000: accuracy:0.14000000059604645, confidence:0.7971705198287964, loss:6.252793312072754
epoch23: step4000/4680
step 103500: accuracy:0.16300000250339508, confidence:0.7254985570907593, loss:5.499584674835205
epoch23: step4500/4680
step 0: accuracy:0.18000000715255737, confidence:0.7732325792312622, loss:5.816973686218262
epoch24: step0/4680
step 12000: accuracy:0.08500000089406967, confidence:0.892568826675415, loss:8.519071578979492
epoch24: step500/4680
step 24000: accuracy:0.125, confidence:0.9964872598648071, loss:15.376299858093262
epoch24: step1000/4680
step 36000: accuracy:0.06599999964237213, confidence:0.682172417640686, loss:6.732777118682861
epoch24: step1500/4680
step 48000: accuracy:0.0729999989271164, confidence:0.8793807029724121, loss:7.909661293029785
epoch24: step2000/4680
step 60000: accuracy:0.09799999743700027, confidence:0.7989279627799988, loss:6.332304000854492
epoch24: step2500/4680
step 72000: accuracy:0.0989999994635582, confidence:0.9999324679374695, loss:20.632469177246094
epoch24: step3000/4680
step 84000: accuracy:0.05700000002980232, confidence:0.7895802855491638, loss:7.5887298583984375
epoch24: step3500/4680
step 96000: accuracy:0.18400000035762787, confidence:0.6325179934501648, loss:4.614155292510986
epoch24: step4000/4680
step 108000: accuracy:0.11800000071525574, confidence:0.8521491885185242, loss:5.998188495635986
epoch24: step4500/4680
step 0: accuracy:0.0949999988079071, confidence:0.9239314794540405, loss:7.176208019256592
epoch25: step0/4680
step 12500: accuracy:0.12300000339746475, confidence:0.8686464428901672, loss:6.780196189880371
epoch25: step500/4680
step 25000: accuracy:0.0989999994635582, confidence:0.7840481996536255, loss:10.717768669128418
epoch25: step1000/4680
step 37500: accuracy:0.09399999678134918, confidence:0.7293243408203125, loss:7.7689666748046875
epoch25: step1500/4680
step 50000: accuracy:0.0820000022649765, confidence:0.8513273596763611, loss:7.387360572814941
epoch25: step2000/4680
step 62500: accuracy:0.09200000017881393, confidence:0.8337664604187012, loss:6.567872524261475
epoch25: step2500/4680
step 75000: accuracy:0.11999999731779099, confidence:0.9997262954711914, loss:14.467656135559082
epoch25: step3000/4680
step 87500: accuracy:0.11599999666213989, confidence:0.9393264055252075, loss:10.369942665100098
epoch25: step3500/4680
step 100000: accuracy:0.1889999955892563, confidence:0.7703123092651367, loss:5.713591575622559
epoch25: step4000/4680
step 112500: accuracy:0.1979999989271164, confidence:0.7436806559562683, loss:5.404904842376709
epoch25: step4500/4680
step 0: accuracy:0.1720000058412552, confidence:0.7983518242835999, loss:6.01220178604126
epoch26: step0/4680
step 13000: accuracy:0.10300000011920929, confidence:0.8963100910186768, loss:8.26331901550293
epoch26: step500/4680
step 26000: accuracy:0.0989999994635582, confidence:0.9847089648246765, loss:13.18161392211914
epoch26: step1000/4680
step 39000: accuracy:0.10899999737739563, confidence:0.7444722056388855, loss:6.713708400726318
epoch26: step1500/4680
step 52000: accuracy:0.07100000232458115, confidence:0.8030773401260376, loss:6.8532633781433105
epoch26: step2000/4680
step 65000: accuracy:0.11500000208616257, confidence:0.8231014609336853, loss:6.307445049285889
epoch26: step2500/4680
step 78000: accuracy:0.10499999672174454, confidence:0.9999988079071045, loss:22.45351219177246
epoch26: step3000/4680
step 91000: accuracy:0.1120000034570694, confidence:0.9985882639884949, loss:11.931536674499512
epoch26: step3500/4680
step 104000: accuracy:0.15199999511241913, confidence:0.6850959062576294, loss:5.218111991882324
epoch26: step4000/4680
step 117000: accuracy:0.08399999886751175, confidence:0.9127461910247803, loss:6.621031761169434
epoch26: step4500/4680
step 0: accuracy:0.0949999988079071, confidence:0.9579063653945923, loss:7.598267078399658
epoch27: step0/4680
step 13500: accuracy:0.11299999803304672, confidence:0.8583362698554993, loss:7.052964210510254
epoch27: step500/4680
step 27000: accuracy:0.12200000137090683, confidence:0.8362788558006287, loss:11.311538696289062
epoch27: step1000/4680
step 40500: accuracy:0.09700000286102295, confidence:0.7114538550376892, loss:7.057878494262695
epoch27: step1500/4680
step 54000: accuracy:0.10199999809265137, confidence:0.7883177995681763, loss:6.834017753601074
epoch27: step2000/4680
step 67500: accuracy:0.08299999684095383, confidence:0.7714670300483704, loss:6.134533882141113
epoch27: step2500/4680
step 81000: accuracy:0.1080000028014183, confidence:0.9999875426292419, loss:20.45606231689453
epoch27: step3000/4680
step 94500: accuracy:0.10300000011920929, confidence:0.9557104110717773, loss:10.841107368469238
epoch27: step3500/4680
step 108000: accuracy:0.14100000262260437, confidence:0.8320465683937073, loss:6.723040580749512
epoch27: step4000/4680
step 121500: accuracy:0.15000000596046448, confidence:0.8222408890724182, loss:6.072506904602051
epoch27: step4500/4680
step 0: accuracy:0.0949999988079071, confidence:0.8707724213600159, loss:6.74041748046875
epoch28: step0/4680
step 14000: accuracy:0.11599999666213989, confidence:0.8905594944953918, loss:8.456155776977539
epoch28: step500/4680
step 28000: accuracy:0.1080000028014183, confidence:0.9813764095306396, loss:14.675905227661133
epoch28: step1000/4680
step 42000: accuracy:0.10599999874830246, confidence:0.803617537021637, loss:7.423298358917236
epoch28: step1500/4680
step 56000: accuracy:0.06700000166893005, confidence:0.787980854511261, loss:7.321930885314941
epoch28: step2000/4680
step 70000: accuracy:0.10700000077486038, confidence:0.6838282942771912, loss:5.353446006774902
epoch28: step2500/4680
step 84000: accuracy:0.12099999934434891, confidence:0.9999890327453613, loss:22.064355850219727
epoch28: step3000/4680
step 98000: accuracy:0.09300000220537186, confidence:0.9882792830467224, loss:10.643776893615723
epoch28: step3500/4680
step 112000: accuracy:0.164000004529953, confidence:0.6026567220687866, loss:4.697555065155029
epoch28: step4000/4680
step 126000: accuracy:0.10300000011920929, confidence:0.9116008281707764, loss:6.430807113647461
epoch28: step4500/4680
step 0: accuracy:0.09099999815225601, confidence:0.9557968378067017, loss:7.3404221534729
epoch29: step0/4680
step 14500: accuracy:0.10199999809265137, confidence:0.8557168245315552, loss:6.8458757400512695
epoch29: step500/4680
step 29000: accuracy:0.12700000405311584, confidence:0.7824409008026123, loss:10.23799991607666
epoch29: step1000/4680
step 43500: accuracy:0.09700000286102295, confidence:0.7700632214546204, loss:6.5903191566467285
epoch29: step1500/4680
step 58000: accuracy:0.07699999958276749, confidence:0.8411082029342651, loss:6.963745594024658
epoch29: step2000/4680
step 72500: accuracy:0.08799999952316284, confidence:0.8515631556510925, loss:6.83404541015625
epoch29: step2500/4680
step 87000: accuracy:0.11699999868869781, confidence:0.999825656414032, loss:14.33447551727295
epoch29: step3000/4680
step 101500: accuracy:0.11599999666213989, confidence:0.8652828335762024, loss:12.277668952941895
epoch29: step3500/4680
step 116000: accuracy:0.11400000005960464, confidence:0.9187746047973633, loss:7.284541130065918
epoch29: step4000/4680
step 130500: accuracy:0.12999999523162842, confidence:0.7546374797821045, loss:5.4376115798950195
epoch29: step4500/4680
2018-06-15 17:47:11.223272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000a:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 17:47:11.223460: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
2018-06-15 17:47:12.675289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
CHEKPOINT DIR: uniform
model has been loaded from fashion-mnist_classifier.pkl
---------
Variables: name (type shape) [size]
---------
Variable:0 (float32_ref 5x5x1x32) [800, bytes: 3200]
Variable_1:0 (float32_ref 32) [32, bytes: 128]
Variable_2:0 (float32_ref 5x5x32x64) [51200, bytes: 204800]
Variable_3:0 (float32_ref 64) [64, bytes: 256]
Variable_4:0 (float32_ref 3136x1024) [3211264, bytes: 12845056]
Variable_5:0 (float32_ref 1024) [1024, bytes: 4096]
Variable_6:0 (float32_ref 1024x10) [10240, bytes: 40960]
Variable_7:0 (float32_ref 10) [10, bytes: 40]
discriminator/d_conv1/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]
discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]
discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]
discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]
generator/g_fc1/Matrix:0 (float32_ref 74x1024) [75776, bytes: 303104]
generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]
generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]
generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]
classifier/c_fc1/Matrix:0 (float32_ref 1024x64) [65536, bytes: 262144]
classifier/c_fc1/bias:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/beta:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/gamma:0 (float32_ref 64) [64, bytes: 256]
classifier/c_fc2/Matrix:0 (float32_ref 64x12) [768, bytes: 3072]
classifier/c_fc2/bias:0 (float32_ref 12) [12, bytes: 48]
Total size of variables: 16552792
Total bytes of variables: 66211168
 [*] Reading checkpoints...
 [*] Success to read MultiModalInfoGAN.model-43721
 [*] Load SUCCESS


SAMPLES SIZE=4680,LABELS=299520


SAVED TRAINING SET generated_training_set_fashion-mnist_UniformSample_mu_0_sigma_0.15_rzcc_rzrc_czcc_czrc
 [*] Training finished!
 [*] Testing finished!
2018-06-15 17:47:50.933305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000a:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 17:47:50.933489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
model has been loaded from fashion-mnist_classifier.pkl
argmax:[0 0 0 ..., 0 0 0]
step 0: accuracy:0.0, confidence:0.9807767868041992, loss:15.714167594909668
Assinging:1
[9619    0    0    0    0    0  381]
argmax:[1 1 1 ..., 1 1 1]
step 0: accuracy:0.0, confidence:0.998201847076416, loss:16.853164672851562
Assinging:2
[   0 9991    0    8    0    0    1]
argmax:[3 3 3 ..., 6 8 0]
step 0: accuracy:0.0, confidence:0.8672425746917725, loss:8.725852012634277
Assinging:4
[ 392    0   54 8895    4    0  569    0   86]
argmax:[7 7 7 ..., 9 9 9]
step 0: accuracy:0.08990000188350677, confidence:0.9162143468856812, loss:2.973595380783081
Assinging:8
[   0    0    0    0    0   10    0 9091    0  899]
argmax:[6 6 4 ..., 2 2 2]
step 0: accuracy:0.0, confidence:0.8948755860328674, loss:14.643941879272461
Assinging:7
[   0    0 1652    0 3319    0 5029]
argmax:[7 7 7 ..., 5 5 5]
step 0: accuracy:0.0, confidence:0.921552836894989, loss:8.067928314208984
Assinging:8
[   0    0    0    0    0 1512    0 8488]
argmax:[4 2 4 ..., 4 4 2]
step 0: accuracy:0.0, confidence:0.874945878982544, loss:9.981771469116211
Assinging:5
[   3    0 2279  955 6745    0   18]
argmax:[9 9 9 ..., 9 5 5]
step 0: accuracy:0.9043999910354614, confidence:0.9978193640708923, loss:1.4107974767684937
Assinging:10
[   0    0    0    0    0  956    0    0    0 9044]
argmax:[4 4 4 ..., 2 2 2]
step 0: accuracy:0.0, confidence:0.8876535296440125, loss:12.586216926574707
Assinging:3
[   0    0 4933    0 4848    0  219]
argmax:[8 8 8 ..., 8 8 8]
step 0: accuracy:0.0, confidence:0.9977681636810303, loss:14.745987892150879
Assinging:9
[   0    0    3    0    0    0    0    0 9997]
2018-06-15 17:48:08.004014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000a:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 17:48:08.004163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
step 0: accuracy:0.10700000077486038, confidence:0.7423022985458374, loss:7.78360652923584
epoch0: step0/4680
step 0: accuracy:0.10199999809265137, confidence:1.0, loss:50.11541748046875
epoch0: step500/4680
step 0: accuracy:0.08799999952316284, confidence:0.9999971389770508, loss:18.687559127807617
epoch0: step1000/4680
step 0: accuracy:0.09799999743700027, confidence:1.0, loss:22.928054809570312
epoch0: step1500/4680
step 0: accuracy:0.0860000029206276, confidence:0.9950100779533386, loss:7.624520301818848
epoch0: step2000/4680
step 0: accuracy:0.10700000077486038, confidence:0.983151912689209, loss:6.787873268127441
epoch0: step2500/4680
step 0: accuracy:0.10199999809265137, confidence:0.9921190738677979, loss:7.445725440979004
epoch0: step3000/4680
step 0: accuracy:0.10300000011920929, confidence:0.998982310295105, loss:11.188904762268066
epoch0: step3500/4680
step 0: accuracy:0.10199999809265137, confidence:0.9992720484733582, loss:9.884634971618652
epoch0: step4000/4680
step 0: accuracy:0.09700000286102295, confidence:0.98915034532547, loss:6.920895576477051
epoch0: step4500/4680
step 0: accuracy:0.0860000029206276, confidence:0.9916099905967712, loss:7.314785480499268
epoch1: step0/4680
step 500: accuracy:0.09000000357627869, confidence:0.9906154870986938, loss:7.2348103523254395
epoch1: step500/4680
step 1000: accuracy:0.1120000034570694, confidence:0.9999696016311646, loss:13.107312202453613
epoch1: step1000/4680
step 1500: accuracy:0.10100000351667404, confidence:0.9999191761016846, loss:12.47813892364502
epoch1: step1500/4680
step 2000: accuracy:0.11500000208616257, confidence:0.9975162148475647, loss:7.790118217468262
epoch1: step2000/4680
step 2500: accuracy:0.10499999672174454, confidence:0.9961927533149719, loss:8.140247344970703
epoch1: step2500/4680
step 3000: accuracy:0.09700000286102295, confidence:0.984653115272522, loss:6.455427646636963
epoch1: step3000/4680
step 3500: accuracy:0.09600000083446503, confidence:0.9757426977157593, loss:6.253334045410156
epoch1: step3500/4680
step 4000: accuracy:0.10100000351667404, confidence:0.9979221224784851, loss:8.282883644104004
epoch1: step4000/4680
step 4500: accuracy:0.09200000017881393, confidence:0.9968084692955017, loss:8.0614595413208
epoch1: step4500/4680
step 0: accuracy:0.10899999737739563, confidence:0.9974967241287231, loss:8.07681941986084
epoch2: step0/4680
step 1000: accuracy:0.11999999731779099, confidence:0.9818357229232788, loss:6.412782192230225
epoch2: step500/4680
step 2000: accuracy:0.11500000208616257, confidence:0.9993008971214294, loss:9.916976928710938
epoch2: step1000/4680
step 3000: accuracy:0.10000000149011612, confidence:0.9997861981391907, loss:11.557835578918457
epoch2: step1500/4680
step 4000: accuracy:0.0989999994635582, confidence:0.9984380006790161, loss:8.722722053527832
epoch2: step2000/4680
step 5000: accuracy:0.09799999743700027, confidence:0.9936540126800537, loss:7.33786153793335
epoch2: step2500/4680
step 6000: accuracy:0.10400000214576721, confidence:0.9511260986328125, loss:5.117982387542725
epoch2: step3000/4680
step 7000: accuracy:0.09200000017881393, confidence:0.9890449047088623, loss:7.923922538757324
epoch2: step3500/4680
step 8000: accuracy:0.08399999886751175, confidence:0.9966479539871216, loss:7.883070468902588
epoch2: step4000/4680
step 9000: accuracy:0.0949999988079071, confidence:0.9964067339897156, loss:7.659084320068359
epoch2: step4500/4680
step 0: accuracy:0.11599999666213989, confidence:0.9987022876739502, loss:8.341702461242676
epoch3: step0/4680
step 1500: accuracy:0.10400000214576721, confidence:0.9677518606185913, loss:5.66633415222168
epoch3: step500/4680
step 3000: accuracy:0.09200000017881393, confidence:0.9989839196205139, loss:9.295478820800781
epoch3: step1000/4680
step 4500: accuracy:0.09799999743700027, confidence:0.996536374092102, loss:8.038618087768555
epoch3: step1500/4680
step 6000: accuracy:0.10199999809265137, confidence:0.9850264191627502, loss:6.169477939605713
epoch3: step2000/4680
step 7500: accuracy:0.10899999737739563, confidence:0.9817036986351013, loss:6.332600116729736
epoch3: step2500/4680
step 9000: accuracy:0.11100000143051147, confidence:0.9488081932067871, loss:5.13913106918335
epoch3: step3000/4680
step 10500: accuracy:0.10400000214576721, confidence:0.9807260632514954, loss:7.29142951965332
epoch3: step3500/4680
step 12000: accuracy:0.09000000357627869, confidence:0.9905983805656433, loss:6.955298900604248
epoch3: step4000/4680
step 13500: accuracy:0.09799999743700027, confidence:0.9806970953941345, loss:6.5467047691345215
epoch3: step4500/4680
step 0: accuracy:0.09300000220537186, confidence:0.9895653128623962, loss:7.147692680358887
epoch4: step0/4680
step 2000: accuracy:0.08699999749660492, confidence:0.9628344178199768, loss:5.76157808303833
epoch4: step500/4680
step 4000: accuracy:0.12099999934434891, confidence:0.9947755932807922, loss:7.091181755065918
epoch4: step1000/4680
step 6000: accuracy:0.09000000357627869, confidence:0.9937169551849365, loss:7.406885623931885
epoch4: step1500/4680
step 8000: accuracy:0.10199999809265137, confidence:0.9912157654762268, loss:7.097511291503906
epoch4: step2000/4680
step 10000: accuracy:0.09200000017881393, confidence:0.9818690419197083, loss:6.506450653076172
epoch4: step2500/4680
step 12000: accuracy:0.10000000149011612, confidence:0.8757579922676086, loss:4.459094047546387
epoch4: step3000/4680
step 14000: accuracy:0.08399999886751175, confidence:0.9883962869644165, loss:7.845287322998047
epoch4: step3500/4680
step 16000: accuracy:0.10199999809265137, confidence:0.9928697347640991, loss:7.262466907501221
epoch4: step4000/4680
step 18000: accuracy:0.1080000028014183, confidence:0.9802019596099854, loss:6.466002464294434
epoch4: step4500/4680
step 0: accuracy:0.08500000089406967, confidence:0.9905589818954468, loss:7.406386852264404
epoch5: step0/4680
step 2500: accuracy:0.10599999874830246, confidence:0.9360874891281128, loss:5.295872211456299
epoch5: step500/4680
step 5000: accuracy:0.12600000202655792, confidence:0.995624840259552, loss:7.080106258392334
epoch5: step1000/4680
step 7500: accuracy:0.12700000405311584, confidence:0.9958057999610901, loss:7.321731090545654
epoch5: step1500/4680
step 10000: accuracy:0.10300000011920929, confidence:0.9395185708999634, loss:5.62960958480835
epoch5: step2000/4680
step 12500: accuracy:0.09300000220537186, confidence:0.9533047676086426, loss:6.704077243804932
epoch5: step2500/4680
step 15000: accuracy:0.11999999731779099, confidence:0.6989765167236328, loss:5.300064563751221
epoch5: step3000/4680
step 17500: accuracy:0.09799999743700027, confidence:0.9768527150154114, loss:6.517341613769531
epoch5: step3500/4680
step 20000: accuracy:0.0860000029206276, confidence:0.9796269536018372, loss:6.428022384643555
epoch5: step4000/4680
step 22500: accuracy:0.0989999994635582, confidence:0.9827714562416077, loss:6.740802764892578
epoch5: step4500/4680
step 0: accuracy:0.11500000208616257, confidence:0.9911840558052063, loss:7.139045238494873
epoch6: step0/4680
step 3000: accuracy:0.11699999868869781, confidence:0.9112260341644287, loss:4.770153522491455
epoch6: step500/4680
step 6000: accuracy:0.11500000208616257, confidence:0.9820827841758728, loss:5.890360355377197
epoch6: step1000/4680
step 9000: accuracy:0.10400000214576721, confidence:0.9869958162307739, loss:6.459090232849121
epoch6: step1500/4680
step 12000: accuracy:0.11500000208616257, confidence:0.9010571241378784, loss:5.940075397491455
epoch6: step2000/4680
step 15000: accuracy:0.08500000089406967, confidence:0.9843099117279053, loss:8.00797176361084
epoch6: step2500/4680
step 18000: accuracy:0.10100000351667404, confidence:0.804931104183197, loss:5.608627796173096
epoch6: step3000/4680
step 21000: accuracy:0.11100000143051147, confidence:0.9725784063339233, loss:5.9634904861450195
epoch6: step3500/4680
step 24000: accuracy:0.08900000154972076, confidence:0.9726563692092896, loss:5.905904293060303
epoch6: step4000/4680
step 27000: accuracy:0.0820000022649765, confidence:0.9879860877990723, loss:7.2974066734313965
epoch6: step4500/4680
step 0: accuracy:0.11900000274181366, confidence:0.9952144622802734, loss:7.758001804351807
epoch7: step0/4680
step 3500: accuracy:0.10400000214576721, confidence:0.8861055970191956, loss:4.7583184242248535
epoch7: step500/4680
step 7000: accuracy:0.1080000028014183, confidence:0.9849400520324707, loss:6.1374077796936035
epoch7: step1000/4680
step 10500: accuracy:0.10199999809265137, confidence:0.9851963520050049, loss:6.446347236633301
epoch7: step1500/4680
step 14000: accuracy:0.1080000028014183, confidence:0.8293787837028503, loss:5.316051006317139
epoch7: step2000/4680
step 17500: accuracy:0.08799999952316284, confidence:0.99139004945755, loss:9.939356803894043
epoch7: step2500/4680
step 21000: accuracy:0.09200000017881393, confidence:0.8791719079017639, loss:6.800734519958496
epoch7: step3000/4680
step 24500: accuracy:0.1120000034570694, confidence:0.9759849309921265, loss:6.239333152770996
epoch7: step3500/4680
step 28000: accuracy:0.09099999815225601, confidence:0.9676470160484314, loss:5.874270439147949
epoch7: step4000/4680
step 31500: accuracy:0.0860000029206276, confidence:0.9731671214103699, loss:6.520616054534912
epoch7: step4500/4680
step 0: accuracy:0.0949999988079071, confidence:0.9881749749183655, loss:7.212610244750977
epoch8: step0/4680
step 4000: accuracy:0.11400000005960464, confidence:0.8709452152252197, loss:4.897863388061523
epoch8: step500/4680
step 8000: accuracy:0.09799999743700027, confidence:0.9687238335609436, loss:5.7483320236206055
epoch8: step1000/4680
step 12000: accuracy:0.10700000077486038, confidence:0.9419342875480652, loss:5.12396764755249
epoch8: step1500/4680
step 16000: accuracy:0.15000000596046448, confidence:0.7013912200927734, loss:6.3683671951293945
epoch8: step2000/4680
step 20000: accuracy:0.09200000017881393, confidence:0.9918327927589417, loss:10.251167297363281
epoch8: step2500/4680
step 24000: accuracy:0.11100000143051147, confidence:0.9077478051185608, loss:7.458844184875488
epoch8: step3000/4680
step 28000: accuracy:0.10400000214576721, confidence:0.9809638261795044, loss:6.353281021118164
epoch8: step3500/4680
step 32000: accuracy:0.09600000083446503, confidence:0.96161949634552, loss:5.4697089195251465
epoch8: step4000/4680
step 36000: accuracy:0.10400000214576721, confidence:0.9491052627563477, loss:6.0020341873168945
epoch8: step4500/4680
step 0: accuracy:0.10700000077486038, confidence:0.9629603028297424, loss:6.2220306396484375
epoch9: step0/4680
step 4500: accuracy:0.08399999886751175, confidence:0.8478363156318665, loss:4.867282390594482
epoch9: step500/4680
step 9000: accuracy:0.11599999666213989, confidence:0.9474419355392456, loss:5.174544811248779
epoch9: step1000/4680
step 13500: accuracy:0.11400000005960464, confidence:0.9542794823646545, loss:5.5183539390563965
epoch9: step1500/4680
step 18000: accuracy:0.12999999523162842, confidence:0.7367690801620483, loss:5.877038955688477
epoch9: step2000/4680
step 22500: accuracy:0.10100000351667404, confidence:0.9942182898521423, loss:11.260746002197266
epoch9: step2500/4680
step 27000: accuracy:0.09700000286102295, confidence:0.9492585062980652, loss:8.39909839630127
epoch9: step3000/4680
step 31500: accuracy:0.07699999958276749, confidence:0.9755399227142334, loss:6.309690475463867
epoch9: step3500/4680
step 36000: accuracy:0.09000000357627869, confidence:0.9718453884124756, loss:6.300268650054932
epoch9: step4000/4680
step 40500: accuracy:0.10700000077486038, confidence:0.9917095899581909, loss:7.870264530181885
epoch9: step4500/4680
step 0: accuracy:0.10300000011920929, confidence:0.9930997490882874, loss:7.836267471313477
epoch10: step0/4680
step 5000: accuracy:0.10499999672174454, confidence:0.9099387526512146, loss:5.505461692810059
epoch10: step500/4680
step 10000: accuracy:0.11800000071525574, confidence:0.9692053198814392, loss:5.698529243469238
epoch10: step1000/4680
step 15000: accuracy:0.08100000023841858, confidence:0.906818687915802, loss:5.027778148651123
epoch10: step1500/4680
step 20000: accuracy:0.09700000286102295, confidence:0.9042689800262451, loss:7.610265254974365
epoch10: step2000/4680
step 25000: accuracy:0.08399999886751175, confidence:0.9795328974723816, loss:10.304183006286621
epoch10: step2500/4680
step 30000: accuracy:0.11599999666213989, confidence:0.8704882264137268, loss:7.858861446380615
epoch10: step3000/4680
step 35000: accuracy:0.09799999743700027, confidence:0.9903305768966675, loss:6.9613213539123535
epoch10: step3500/4680
step 40000: accuracy:0.0820000022649765, confidence:0.9744406342506409, loss:6.122448444366455
epoch10: step4000/4680
step 45000: accuracy:0.1080000028014183, confidence:0.9823574423789978, loss:6.897344589233398
epoch10: step4500/4680
step 0: accuracy:0.10199999809265137, confidence:0.9859819412231445, loss:7.21476936340332
epoch11: step0/4680
step 5500: accuracy:0.10899999737739563, confidence:0.7370263934135437, loss:4.033993244171143
epoch11: step500/4680
step 11000: accuracy:0.1080000028014183, confidence:0.9898026585578918, loss:6.866268157958984
epoch11: step1000/4680
step 16500: accuracy:0.10100000351667404, confidence:0.9503084421157837, loss:5.969237327575684
epoch11: step1500/4680
step 22000: accuracy:0.12600000202655792, confidence:0.7685397863388062, loss:6.3596601486206055
epoch11: step2000/4680
step 27500: accuracy:0.11100000143051147, confidence:0.9689884781837463, loss:10.485447883605957
epoch11: step2500/4680
step 33000: accuracy:0.11100000143051147, confidence:0.8569106459617615, loss:8.31296443939209
epoch11: step3000/4680
step 38500: accuracy:0.09099999815225601, confidence:0.9990566372871399, loss:9.103732109069824
epoch11: step3500/4680
step 44000: accuracy:0.09700000286102295, confidence:0.9931399822235107, loss:7.281445503234863
epoch11: step4000/4680
step 49500: accuracy:0.10400000214576721, confidence:0.9770005345344543, loss:7.130998134613037
epoch11: step4500/4680
step 0: accuracy:0.0989999994635582, confidence:0.9799149632453918, loss:7.222949028015137
epoch12: step0/4680
step 6000: accuracy:0.0989999994635582, confidence:0.841452419757843, loss:5.012053489685059
epoch12: step500/4680
step 12000: accuracy:0.12300000339746475, confidence:0.9317674040794373, loss:5.080655574798584
epoch12: step1000/4680
step 18000: accuracy:0.10999999940395355, confidence:0.8804401755332947, loss:5.0495991706848145
epoch12: step1500/4680
step 24000: accuracy:0.10899999737739563, confidence:0.977417528629303, loss:8.51500415802002
epoch12: step2000/4680
step 30000: accuracy:0.10899999737739563, confidence:0.9704877138137817, loss:10.072468757629395
epoch12: step2500/4680
step 36000: accuracy:0.11299999803304672, confidence:0.819546639919281, loss:7.863200664520264
epoch12: step3000/4680
step 42000: accuracy:0.10999999940395355, confidence:0.962303102016449, loss:5.918401718139648
epoch12: step3500/4680
step 48000: accuracy:0.09200000017881393, confidence:0.9270694851875305, loss:5.683916091918945
epoch12: step4000/4680
step 54000: accuracy:0.08699999749660492, confidence:0.9949541687965393, loss:8.802730560302734
epoch12: step4500/4680
step 0: accuracy:0.09799999743700027, confidence:0.9950839281082153, loss:8.546443939208984
epoch13: step0/4680
step 6500: accuracy:0.10199999809265137, confidence:0.7838500738143921, loss:4.447728633880615
epoch13: step500/4680
step 13000: accuracy:0.12200000137090683, confidence:0.9238418340682983, loss:4.942261219024658
epoch13: step1000/4680
step 19500: accuracy:0.11800000071525574, confidence:0.7859472036361694, loss:4.099158763885498
epoch13: step1500/4680
step 26000: accuracy:0.0860000029206276, confidence:0.9126686453819275, loss:7.242208003997803
epoch13: step2000/4680
step 32500: accuracy:0.10000000149011612, confidence:0.9068283438682556, loss:8.92349624633789
epoch13: step2500/4680
step 39000: accuracy:0.10899999737739563, confidence:0.8186995387077332, loss:7.726047039031982
epoch13: step3000/4680
step 45500: accuracy:0.09200000017881393, confidence:0.9962183833122253, loss:7.892440319061279
epoch13: step3500/4680
step 52000: accuracy:0.11299999803304672, confidence:0.914590060710907, loss:5.49636697769165
epoch13: step4000/4680
step 58500: accuracy:0.11800000071525574, confidence:0.9909272789955139, loss:7.706625461578369
epoch13: step4500/4680
step 0: accuracy:0.10000000149011612, confidence:0.9940618872642517, loss:8.286438941955566
epoch14: step0/4680
step 7000: accuracy:0.09300000220537186, confidence:0.7107430100440979, loss:4.046587944030762
epoch14: step500/4680
step 14000: accuracy:0.10700000077486038, confidence:0.9278276562690735, loss:5.288217544555664
epoch14: step1000/4680
step 21000: accuracy:0.10999999940395355, confidence:0.8232844471931458, loss:4.815572261810303
epoch14: step1500/4680
step 28000: accuracy:0.09099999815225601, confidence:0.9343094825744629, loss:6.979072093963623
epoch14: step2000/4680
step 35000: accuracy:0.12099999934434891, confidence:0.8108997941017151, loss:8.482508659362793
epoch14: step2500/4680
step 42000: accuracy:0.17299999296665192, confidence:0.7123575806617737, loss:7.019618988037109
epoch14: step3000/4680
step 49000: accuracy:0.0860000029206276, confidence:0.9925819039344788, loss:7.64571475982666
epoch14: step3500/4680
step 56000: accuracy:0.09000000357627869, confidence:0.9324606657028198, loss:5.624879837036133
epoch14: step4000/4680
step 63000: accuracy:0.07900000363588333, confidence:0.9753896594047546, loss:7.016381740570068
epoch14: step4500/4680
step 0: accuracy:0.10999999940395355, confidence:0.9860770106315613, loss:7.352115631103516
epoch15: step0/4680
step 7500: accuracy:0.0949999988079071, confidence:0.8349615931510925, loss:5.163257598876953
epoch15: step500/4680
step 15000: accuracy:0.11999999731779099, confidence:0.9333528280258179, loss:5.345263957977295
epoch15: step1000/4680
step 22500: accuracy:0.11800000071525574, confidence:0.8219741582870483, loss:4.793960094451904
epoch15: step1500/4680
step 30000: accuracy:0.10999999940395355, confidence:0.9290682077407837, loss:7.254729747772217
epoch15: step2000/4680
step 37500: accuracy:0.11800000071525574, confidence:0.7980526685714722, loss:7.333858013153076
epoch15: step2500/4680
step 45000: accuracy:0.16300000250339508, confidence:0.6928636431694031, loss:6.463832378387451
epoch15: step3000/4680
step 52500: accuracy:0.10599999874830246, confidence:0.9725309610366821, loss:6.401371955871582
epoch15: step3500/4680
step 60000: accuracy:0.09300000220537186, confidence:0.8978604674339294, loss:5.414358139038086
epoch15: step4000/4680
step 67500: accuracy:0.09399999678134918, confidence:0.990921139717102, loss:7.987588882446289
epoch15: step4500/4680
step 0: accuracy:0.09799999743700027, confidence:0.990420401096344, loss:7.774890899658203
epoch16: step0/4680
step 8000: accuracy:0.12800000607967377, confidence:0.7776502370834351, loss:4.337838649749756
epoch16: step500/4680
step 16000: accuracy:0.10599999874830246, confidence:0.9330439567565918, loss:5.567266941070557
epoch16: step1000/4680
step 24000: accuracy:0.12800000607967377, confidence:0.7921350598335266, loss:4.57468843460083
epoch16: step1500/4680
step 32000: accuracy:0.09399999678134918, confidence:0.9411829113960266, loss:7.39609432220459
epoch16: step2000/4680
step 40000: accuracy:0.11699999868869781, confidence:0.8129624128341675, loss:7.632066249847412
epoch16: step2500/4680
step 48000: accuracy:0.15800000727176666, confidence:0.7314121127128601, loss:6.537581443786621
epoch16: step3000/4680
step 56000: accuracy:0.10300000011920929, confidence:0.9910747408866882, loss:7.53289270401001
epoch16: step3500/4680
step 64000: accuracy:0.08299999684095383, confidence:0.7846496105194092, loss:4.848718643188477
epoch16: step4000/4680
step 72000: accuracy:0.08900000154972076, confidence:0.9756474494934082, loss:7.133401393890381
epoch16: step4500/4680
step 0: accuracy:0.10999999940395355, confidence:0.9752139449119568, loss:6.911011695861816
epoch17: step0/4680
step 8500: accuracy:0.10000000149011612, confidence:0.7396779656410217, loss:4.562117099761963
epoch17: step500/4680
step 17000: accuracy:0.12700000405311584, confidence:0.9425519704818726, loss:5.796586990356445
epoch17: step1000/4680
step 25500: accuracy:0.1120000034570694, confidence:0.8359665870666504, loss:5.0362443923950195
epoch17: step1500/4680
step 34000: accuracy:0.09700000286102295, confidence:0.9402190446853638, loss:8.681148529052734
epoch17: step2000/4680
step 42500: accuracy:0.10300000011920929, confidence:0.9010254740715027, loss:8.759051322937012
epoch17: step2500/4680
step 51000: accuracy:0.1340000033378601, confidence:0.7591342329978943, loss:7.237566947937012
epoch17: step3000/4680
step 59500: accuracy:0.10499999672174454, confidence:0.9629808068275452, loss:6.80988073348999
epoch17: step3500/4680
step 68000: accuracy:0.10999999940395355, confidence:0.6871150732040405, loss:5.383854389190674
epoch17: step4000/4680
step 76500: accuracy:0.0989999994635582, confidence:0.9921109080314636, loss:8.571174621582031
epoch17: step4500/4680
step 0: accuracy:0.08900000154972076, confidence:0.9918246865272522, loss:8.70907211303711
epoch18: step0/4680
step 9000: accuracy:0.11699999868869781, confidence:0.7324892282485962, loss:4.046698570251465
epoch18: step500/4680
step 18000: accuracy:0.11299999803304672, confidence:0.9460068345069885, loss:5.863471031188965
epoch18: step1000/4680
step 27000: accuracy:0.10100000351667404, confidence:0.7664594650268555, loss:4.598680019378662
epoch18: step1500/4680
step 36000: accuracy:0.0989999994635582, confidence:0.9370366930961609, loss:8.724726676940918
epoch18: step2000/4680
step 45000: accuracy:0.11699999868869781, confidence:0.8093270063400269, loss:8.557303428649902
epoch18: step2500/4680
step 54000: accuracy:0.15299999713897705, confidence:0.7574975490570068, loss:7.636096954345703
epoch18: step3000/4680
step 63000: accuracy:0.08100000023841858, confidence:0.9354350566864014, loss:6.887723922729492
epoch18: step3500/4680
step 72000: accuracy:0.11100000143051147, confidence:0.6476519107818604, loss:5.4935503005981445
epoch18: step4000/4680
step 81000: accuracy:0.09099999815225601, confidence:0.9866517186164856, loss:7.930265426635742
epoch18: step4500/4680
step 0: accuracy:0.08299999684095383, confidence:0.984691858291626, loss:7.728143215179443
epoch19: step0/4680
step 9500: accuracy:0.08699999749660492, confidence:0.8134974837303162, loss:5.020573616027832
epoch19: step500/4680
step 19000: accuracy:0.12700000405311584, confidence:0.9405926465988159, loss:5.610003471374512
epoch19: step1000/4680
step 28500: accuracy:0.10199999809265137, confidence:0.7798207998275757, loss:4.9289679527282715
epoch19: step1500/4680
step 38000: accuracy:0.10700000077486038, confidence:0.8801732063293457, loss:6.775872707366943
epoch19: step2000/4680
step 47500: accuracy:0.13300000131130219, confidence:0.7838695645332336, loss:8.65198802947998
epoch19: step2500/4680
step 57000: accuracy:0.1509999930858612, confidence:0.7393472194671631, loss:7.754453182220459
epoch19: step3000/4680
step 66500: accuracy:0.0989999994635582, confidence:0.987953782081604, loss:7.985940456390381
epoch19: step3500/4680
step 76000: accuracy:0.10499999672174454, confidence:0.676969587802887, loss:4.423977851867676
epoch19: step4000/4680
step 85500: accuracy:0.10899999737739563, confidence:0.9671263694763184, loss:6.627027988433838
epoch19: step4500/4680
step 0: accuracy:0.10000000149011612, confidence:0.9803122282028198, loss:7.185489654541016
epoch20: step0/4680
step 10000: accuracy:0.09000000357627869, confidence:0.8707245588302612, loss:5.543013572692871
epoch20: step500/4680
step 20000: accuracy:0.11699999868869781, confidence:0.9134865999221802, loss:5.522743225097656
epoch20: step1000/4680
step 30000: accuracy:0.13099999725818634, confidence:0.7596160173416138, loss:4.502291679382324
epoch20: step1500/4680
step 40000: accuracy:0.0949999988079071, confidence:0.9179087281227112, loss:7.49826192855835
epoch20: step2000/4680
step 50000: accuracy:0.11900000274181366, confidence:0.8050621747970581, loss:8.247211456298828
epoch20: step2500/4680
step 60000: accuracy:0.15700000524520874, confidence:0.7334136962890625, loss:7.211686611175537
epoch20: step3000/4680
step 70000: accuracy:0.10199999809265137, confidence:0.9774318933486938, loss:7.445267200469971
epoch20: step3500/4680
step 80000: accuracy:0.11500000208616257, confidence:0.6385654211044312, loss:4.2017083168029785
epoch20: step4000/4680
step 90000: accuracy:0.10599999874830246, confidence:0.9933344721794128, loss:7.930917263031006
epoch20: step4500/4680
step 0: accuracy:0.08299999684095383, confidence:0.9925923347473145, loss:7.8855977058410645
epoch21: step0/4680
step 10500: accuracy:0.10199999809265137, confidence:0.8886250257492065, loss:5.524975776672363
epoch21: step500/4680
step 21000: accuracy:0.11299999803304672, confidence:0.936671793460846, loss:5.826665878295898
epoch21: step1000/4680
step 31500: accuracy:0.10400000214576721, confidence:0.7714402079582214, loss:4.732517242431641
epoch21: step1500/4680
step 42000: accuracy:0.10999999940395355, confidence:0.9089313745498657, loss:7.045257091522217
epoch21: step2000/4680
step 52500: accuracy:0.10599999874830246, confidence:0.8070449233055115, loss:7.933124542236328
epoch21: step2500/4680
step 63000: accuracy:0.14499999582767487, confidence:0.7745243310928345, loss:6.897920608520508
epoch21: step3000/4680
step 73500: accuracy:0.11299999803304672, confidence:0.960118293762207, loss:7.165860652923584
epoch21: step3500/4680
step 84000: accuracy:0.10899999737739563, confidence:0.6086689829826355, loss:4.65089225769043
epoch21: step4000/4680
step 94500: accuracy:0.1120000034570694, confidence:0.9725214242935181, loss:6.35715913772583
epoch21: step4500/4680
step 0: accuracy:0.10499999672174454, confidence:0.9861817955970764, loss:7.1315202713012695
epoch22: step0/4680
step 11000: accuracy:0.09399999678134918, confidence:0.9533363580703735, loss:6.38125467300415
epoch22: step500/4680
step 22000: accuracy:0.11400000005960464, confidence:0.8985824584960938, loss:5.148313999176025
epoch22: step1000/4680
step 33000: accuracy:0.1469999998807907, confidence:0.737622082233429, loss:4.270018577575684
epoch22: step1500/4680
step 44000: accuracy:0.09200000017881393, confidence:0.8819423317909241, loss:6.599877834320068
epoch22: step2000/4680
step 55000: accuracy:0.10100000351667404, confidence:0.7913904786109924, loss:7.526160717010498
epoch22: step2500/4680
step 66000: accuracy:0.14300000667572021, confidence:0.7397261261940002, loss:6.635903358459473
epoch22: step3000/4680
step 77000: accuracy:0.10599999874830246, confidence:0.9679595828056335, loss:7.374701023101807
epoch22: step3500/4680
step 88000: accuracy:0.09200000017881393, confidence:0.6373172402381897, loss:4.812147617340088
epoch22: step4000/4680
step 99000: accuracy:0.09200000017881393, confidence:0.968228816986084, loss:7.188297271728516
epoch22: step4500/4680
step 0: accuracy:0.09300000220537186, confidence:0.9740290641784668, loss:7.4204325675964355
epoch23: step0/4680
step 11500: accuracy:0.11100000143051147, confidence:0.9439411759376526, loss:6.432562351226807
epoch23: step500/4680
step 23000: accuracy:0.1420000046491623, confidence:0.9153215885162354, loss:5.5405778884887695
epoch23: step1000/4680
step 34500: accuracy:0.12099999934434891, confidence:0.7801226377487183, loss:4.6676506996154785
epoch23: step1500/4680
step 46000: accuracy:0.11800000071525574, confidence:0.8919700980186462, loss:6.683959484100342
epoch23: step2000/4680
step 57500: accuracy:0.11699999868869781, confidence:0.841459333896637, loss:8.02497673034668
epoch23: step2500/4680
step 69000: accuracy:0.15000000596046448, confidence:0.8014143109321594, loss:7.052144527435303
epoch23: step3000/4680
step 80500: accuracy:0.09200000017881393, confidence:0.9901589751243591, loss:8.151776313781738
epoch23: step3500/4680
step 92000: accuracy:0.09200000017881393, confidence:0.6434606909751892, loss:4.95911169052124
epoch23: step4000/4680
step 103500: accuracy:0.09000000357627869, confidence:0.9530251026153564, loss:6.5902814865112305
epoch23: step4500/4680
step 0: accuracy:0.10000000149011612, confidence:0.9692914485931396, loss:6.958656311035156
epoch24: step0/4680
step 12000: accuracy:0.11500000208616257, confidence:0.8957993984222412, loss:5.976950645446777
epoch24: step500/4680
step 24000: accuracy:0.13199999928474426, confidence:0.9233358502388, loss:5.936257362365723
epoch24: step1000/4680
step 36000: accuracy:0.1120000034570694, confidence:0.8048931956291199, loss:5.130056858062744
epoch24: step1500/4680
step 48000: accuracy:0.10700000077486038, confidence:0.912592351436615, loss:7.451725482940674
epoch24: step2000/4680
step 60000: accuracy:0.13099999725818634, confidence:0.8433008790016174, loss:8.771224021911621
epoch24: step2500/4680
step 72000: accuracy:0.14499999582767487, confidence:0.7665524482727051, loss:7.750615119934082
epoch24: step3000/4680
step 84000: accuracy:0.0989999994635582, confidence:0.9894211888313293, loss:8.477534294128418
epoch24: step3500/4680
step 96000: accuracy:0.10300000011920929, confidence:0.6625069975852966, loss:6.648245334625244
epoch24: step4000/4680
step 108000: accuracy:0.10199999809265137, confidence:0.9547704458236694, loss:6.508155345916748
epoch24: step4500/4680
step 0: accuracy:0.09700000286102295, confidence:0.9640282988548279, loss:6.966867446899414
epoch25: step0/4680
step 12500: accuracy:0.10100000351667404, confidence:0.8145886659622192, loss:5.094635486602783
epoch25: step500/4680
step 25000: accuracy:0.11500000208616257, confidence:0.9359118938446045, loss:5.933182239532471
epoch25: step1000/4680
step 37500: accuracy:0.14300000667572021, confidence:0.7646077871322632, loss:4.560998916625977
epoch25: step1500/4680
step 50000: accuracy:0.10499999672174454, confidence:0.8983367085456848, loss:6.953071594238281
epoch25: step2000/4680
step 62500: accuracy:0.11599999666213989, confidence:0.8302045464515686, loss:7.49642276763916
epoch25: step2500/4680
step 75000: accuracy:0.1469999998807907, confidence:0.7832314968109131, loss:6.993097305297852
epoch25: step3000/4680
step 87500: accuracy:0.08799999952316284, confidence:0.9874613881111145, loss:8.582329750061035
epoch25: step3500/4680
step 100000: accuracy:0.09700000286102295, confidence:0.6576159000396729, loss:6.302516460418701
epoch25: step4000/4680
step 112500: accuracy:0.10400000214576721, confidence:0.9887427687644958, loss:8.026285171508789
epoch25: step4500/4680
step 0: accuracy:0.09700000286102295, confidence:0.9891865253448486, loss:7.945734024047852
epoch26: step0/4680
step 13000: accuracy:0.09099999815225601, confidence:0.9792943596839905, loss:7.519650459289551
epoch26: step500/4680
step 26000: accuracy:0.12200000137090683, confidence:0.8983073830604553, loss:5.634212493896484
epoch26: step1000/4680
step 39000: accuracy:0.12800000607967377, confidence:0.7759299278259277, loss:4.859188556671143
epoch26: step1500/4680
step 52000: accuracy:0.12099999934434891, confidence:0.8730528950691223, loss:6.396803855895996
epoch26: step2000/4680
step 65000: accuracy:0.125, confidence:0.8400614261627197, loss:7.46615743637085
epoch26: step2500/4680
step 78000: accuracy:0.14800000190734863, confidence:0.8087917566299438, loss:7.014009952545166
epoch26: step3000/4680
step 91000: accuracy:0.10100000351667404, confidence:0.9707964062690735, loss:8.027094841003418
epoch26: step3500/4680
step 104000: accuracy:0.09399999678134918, confidence:0.7368566989898682, loss:6.17889404296875
epoch26: step4000/4680
step 117000: accuracy:0.0949999988079071, confidence:0.9733900427818298, loss:7.5046234130859375
epoch26: step4500/4680
step 0: accuracy:0.10000000149011612, confidence:0.9771845936775208, loss:7.680665016174316
epoch27: step0/4680
step 13500: accuracy:0.10000000149011612, confidence:0.9746854901313782, loss:7.530790328979492
epoch27: step500/4680
step 27000: accuracy:0.13099999725818634, confidence:0.871624767780304, loss:5.422000885009766
epoch27: step1000/4680
step 40500: accuracy:0.13600000739097595, confidence:0.7859383821487427, loss:4.762383460998535
epoch27: step1500/4680
step 54000: accuracy:0.1120000034570694, confidence:0.8771441578865051, loss:7.086251735687256
epoch27: step2000/4680
step 67500: accuracy:0.10400000214576721, confidence:0.871178150177002, loss:7.860560417175293
epoch27: step2500/4680
step 81000: accuracy:0.12700000405311584, confidence:0.8527266979217529, loss:7.504145622253418
epoch27: step3000/4680
step 94500: accuracy:0.09000000357627869, confidence:0.9675031304359436, loss:8.433073043823242
epoch27: step3500/4680
step 108000: accuracy:0.07999999821186066, confidence:0.7131837606430054, loss:7.080072402954102
epoch27: step4000/4680
step 121500: accuracy:0.11699999868869781, confidence:0.9691951870918274, loss:6.93592643737793
epoch27: step4500/4680
step 0: accuracy:0.10700000077486038, confidence:0.968555748462677, loss:7.033019542694092
epoch28: step0/4680
step 14000: accuracy:0.09000000357627869, confidence:0.9420008659362793, loss:7.19876766204834
epoch28: step500/4680
step 28000: accuracy:0.11800000071525574, confidence:0.9086612462997437, loss:6.08526086807251
epoch28: step1000/4680
step 42000: accuracy:0.10499999672174454, confidence:0.7552056908607483, loss:4.70576286315918
epoch28: step1500/4680
step 56000: accuracy:0.12700000405311584, confidence:0.8710635900497437, loss:6.883407115936279
epoch28: step2000/4680
step 70000: accuracy:0.09799999743700027, confidence:0.85422682762146, loss:8.48624324798584
epoch28: step2500/4680
step 84000: accuracy:0.13099999725818634, confidence:0.8096290230751038, loss:7.205162525177002
epoch28: step3000/4680
step 98000: accuracy:0.09300000220537186, confidence:0.9862261414527893, loss:9.227619171142578
epoch28: step3500/4680
step 112000: accuracy:0.10400000214576721, confidence:0.6557649374008179, loss:5.932722568511963
epoch28: step4000/4680
step 126000: accuracy:0.10700000077486038, confidence:0.9431548714637756, loss:6.724114418029785
epoch28: step4500/4680
step 0: accuracy:0.09300000220537186, confidence:0.9535970687866211, loss:6.969481468200684
epoch29: step0/4680
step 14500: accuracy:0.08799999952316284, confidence:0.964396059513092, loss:7.41107177734375
epoch29: step500/4680
step 29000: accuracy:0.13899999856948853, confidence:0.8862865567207336, loss:5.667995929718018
epoch29: step1000/4680
step 43500: accuracy:0.15000000596046448, confidence:0.7051592469215393, loss:4.328444480895996
epoch29: step1500/4680
step 58000: accuracy:0.11599999666213989, confidence:0.8364572525024414, loss:6.242865085601807
epoch29: step2000/4680
step 72500: accuracy:0.1120000034570694, confidence:0.8609123826026917, loss:7.880670070648193
epoch29: step2500/4680
step 87000: accuracy:0.12700000405311584, confidence:0.7972304224967957, loss:7.0106916427612305
epoch29: step3000/4680
step 101500: accuracy:0.09099999815225601, confidence:0.9321100115776062, loss:7.718156337738037
epoch29: step3500/4680
step 116000: accuracy:0.09799999743700027, confidence:0.6524120569229126, loss:5.3310441970825195
epoch29: step4000/4680
step 130500: accuracy:0.09399999678134918, confidence:0.9743666052818298, loss:7.477693557739258
epoch29: step4500/4680
2018-06-15 17:56:36.134763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000a:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 17:56:36.134945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
2018-06-15 17:56:37.553843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
CHEKPOINT DIR: uniform
model has been loaded from fashion-mnist_classifier.pkl
---------
Variables: name (type shape) [size]
---------
Variable:0 (float32_ref 5x5x1x32) [800, bytes: 3200]
Variable_1:0 (float32_ref 32) [32, bytes: 128]
Variable_2:0 (float32_ref 5x5x32x64) [51200, bytes: 204800]
Variable_3:0 (float32_ref 64) [64, bytes: 256]
Variable_4:0 (float32_ref 3136x1024) [3211264, bytes: 12845056]
Variable_5:0 (float32_ref 1024) [1024, bytes: 4096]
Variable_6:0 (float32_ref 1024x10) [10240, bytes: 40960]
Variable_7:0 (float32_ref 10) [10, bytes: 40]
discriminator/d_conv1/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]
discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]
discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]
discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]
discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]
generator/g_fc1/Matrix:0 (float32_ref 74x1024) [75776, bytes: 303104]
generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]
generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]
generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]
generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]
generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]
generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]
generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]
generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]
classifier/c_fc1/Matrix:0 (float32_ref 1024x64) [65536, bytes: 262144]
classifier/c_fc1/bias:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/beta:0 (float32_ref 64) [64, bytes: 256]
classifier/c_bn1/gamma:0 (float32_ref 64) [64, bytes: 256]
classifier/c_fc2/Matrix:0 (float32_ref 64x12) [768, bytes: 3072]
classifier/c_fc2/bias:0 (float32_ref 12) [12, bytes: 48]
Total size of variables: 16552792
Total bytes of variables: 66211168
 [*] Reading checkpoints...
 [*] Success to read MultiModalInfoGAN.model-43721
 [*] Load SUCCESS


SAMPLES SIZE=4680,LABELS=299520


SAVED TRAINING SET generated_training_set_fashion-mnist_UniformSample_mu_0_sigma_0.15_czrc_czcc_rzcc_rzrc
 [*] Training finished!
 [*] Testing finished!
2018-06-15 17:57:18.153101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000a:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 17:57:18.153309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
model has been loaded from fashion-mnist_classifier.pkl
argmax:[4 4 4 ..., 6 6 6]
step 0: accuracy:0.0, confidence:0.7425218820571899, loss:10.56462287902832
Assinging:5
[   0    0  936    0 4563  234 4267]
argmax:[1 1 1 ..., 1 1 1]
step 0: accuracy:0.0, confidence:0.9996853470802307, loss:18.799694061279297
Assinging:2
[    0 10000]
argmax:[2 2 3 ..., 2 2 2]
step 0: accuracy:0.0, confidence:0.8626964688301086, loss:10.94996166229248
Assinging:3
[ 702    0 6958 1989  117    0  234]
argmax:[8 5 5 ..., 7 7 7]
step 0: accuracy:0.0, confidence:0.8979995250701904, loss:6.534582614898682
Assinging:8
[   0    0    0    0    0 4212    0 5671  117]
argmax:[9 9 9 ..., 9 9 9]
step 0: accuracy:0.9765999913215637, confidence:0.9588537812232971, loss:0.06044063717126846
Assinging:10
[   0    0    0    0    0    0    0  234    0 9766]
argmax:[4 4 4 ..., 4 4 4]
step 0: accuracy:0.0, confidence:0.7177537083625793, loss:11.704702377319336
Assinging:5
[   0    0  234    0 4150    0 2106    0 3510]
argmax:[3 3 3 ..., 3 3 3]
step 0: accuracy:0.0, confidence:0.9216770529747009, loss:14.689303398132324
Assinging:4
[ 936    0  117 7426    0    0    0    0 1521]
argmax:[0 0 0 ..., 0 0 0]
step 0: accuracy:0.0, confidence:0.9577694535255432, loss:16.625940322875977
Assinging:1
[8128    0    0    0    0    0 1872]
argmax:[3 3 3 ..., 3 3 3]
step 0: accuracy:0.0, confidence:0.9612778425216675, loss:13.25690746307373
Assinging:4
[ 117    0    0 5320    0    0  819    0 3744]
argmax:[7 7 5 ..., 7 7 7]
step 0: accuracy:0.0, confidence:0.9746702909469604, loss:11.080684661865234
Assinging:8
[   0    0    0    0    0 1755    0 8245]
2018-06-15 17:57:32.869280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.4805
pciBusID: 000a:01:00.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2018-06-15 17:57:32.911699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 000a:01:00.0, compute capability: 6.0)
/usr/lib/python3/dist-packages/numpy/core/machar.py:127: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:129: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:138: RuntimeWarning: invalid value encountered in subtract
  itemp = int_conv(temp-a)
/usr/lib/python3/dist-packages/numpy/core/machar.py:162: RuntimeWarning: overflow encountered in add
  a = a + a
/usr/lib/python3/dist-packages/numpy/core/machar.py:164: RuntimeWarning: invalid value encountered in subtract
  temp1 = temp - a
/usr/lib/python3/dist-packages/numpy/core/machar.py:171: RuntimeWarning: invalid value encountered in subtract
  if any(temp-a != zero):
step 0: accuracy:0.10700000077486038, confidence:0.8749623894691467, loss:11.321263313293457
epoch0: step0/4680
step 0: accuracy:0.08799999952316284, confidence:1.0, loss:34.9332389831543
epoch0: step500/4680
step 0: accuracy:0.08799999952316284, confidence:1.0, loss:24.883237838745117
epoch0: step1000/4680
step 0: accuracy:0.10400000214576721, confidence:0.9998557567596436, loss:10.536698341369629
epoch0: step1500/4680
step 0: accuracy:0.0860000029206276, confidence:0.9995958805084229, loss:9.852784156799316
epoch0: step2000/4680
step 0: accuracy:0.10400000214576721, confidence:0.9976446032524109, loss:9.011631965637207
epoch0: step2500/4680
step 0: accuracy:0.07900000363588333, confidence:0.9995387196540833, loss:10.35351848602295
epoch0: step3000/4680
step 0: accuracy:0.11599999666213989, confidence:0.9998733997344971, loss:11.013751029968262
epoch0: step3500/4680
step 0: accuracy:0.10100000351667404, confidence:0.9995214343070984, loss:10.685663223266602
epoch0: step4000/4680
step 0: accuracy:0.0860000029206276, confidence:0.9977165460586548, loss:8.824152946472168
epoch0: step4500/4680
step 0: accuracy:0.09399999678134918, confidence:0.9974591135978699, loss:8.733539581298828
epoch1: step0/4680
step 500: accuracy:0.10400000214576721, confidence:0.99826979637146, loss:9.009051322937012
epoch1: step500/4680
step 1000: accuracy:0.1120000034570694, confidence:0.999587893486023, loss:10.62524700164795
epoch1: step1000/4680
step 1500: accuracy:0.10300000011920929, confidence:0.9996413588523865, loss:10.282154083251953
epoch1: step1500/4680
step 2000: accuracy:0.11500000208616257, confidence:0.999093770980835, loss:8.751788139343262
epoch1: step2000/4680
step 2500: accuracy:0.10300000011920929, confidence:0.9989832639694214, loss:9.412911415100098
epoch1: step2500/4680
step 3000: accuracy:0.10400000214576721, confidence:0.9948175549507141, loss:7.709117412567139
epoch1: step3000/4680
step 3500: accuracy:0.08900000154972076, confidence:0.9998840689659119, loss:11.625678062438965
epoch1: step3500/4680
step 4000: accuracy:0.10400000214576721, confidence:0.9973787069320679, loss:8.161092758178711
epoch1: step4000/4680
step 4500: accuracy:0.1080000028014183, confidence:0.9999645948410034, loss:12.321009635925293
epoch1: step4500/4680
step 0: accuracy:0.09099999815225601, confidence:0.9998769760131836, loss:11.094815254211426
epoch2: step0/4680
step 1000: accuracy:0.10499999672174454, confidence:0.9983822703361511, loss:9.708037376403809
epoch2: step500/4680
step 2000: accuracy:0.11500000208616257, confidence:0.9993147253990173, loss:10.442520141601562
epoch2: step1000/4680
step 3000: accuracy:0.10999999940395355, confidence:0.9984036684036255, loss:8.317842483520508
epoch2: step1500/4680
step 4000: accuracy:0.0989999994635582, confidence:0.9996871948242188, loss:9.804062843322754
epoch2: step2000/4680
step 5000: accuracy:0.10499999672174454, confidence:0.9984590411186218, loss:8.593144416809082
epoch2: step2500/4680
step 6000: accuracy:0.0949999988079071, confidence:0.985960066318512, loss:6.963212013244629
epoch2: step3000/4680
step 7000: accuracy:0.10000000149011612, confidence:0.9946691989898682, loss:7.4732136726379395
epoch2: step3500/4680
step 8000: accuracy:0.10199999809265137, confidence:0.9983257055282593, loss:8.535616874694824
epoch2: step4000/4680
step 9000: accuracy:0.10499999672174454, confidence:0.9998617172241211, loss:10.824857711791992
epoch2: step4500/4680
step 0: accuracy:0.09000000357627869, confidence:0.9997410178184509, loss:10.332252502441406
epoch3: step0/4680
step 1500: accuracy:0.10000000149011612, confidence:0.9953354597091675, loss:8.163290977478027
epoch3: step500/4680
step 3000: accuracy:0.09200000017881393, confidence:0.9934455752372742, loss:7.806797981262207
epoch3: step1000/4680
step 4500: accuracy:0.08699999749660492, confidence:0.9970646500587463, loss:8.252264022827148
epoch3: step1500/4680
step 6000: accuracy:0.10199999809265137, confidence:0.9998713135719299, loss:10.671685218811035
epoch3: step2000/4680
step 7500: accuracy:0.09300000220537186, confidence:0.9945685267448425, loss:7.3494038581848145
epoch3: step2500/4680
step 9000: accuracy:0.0989999994635582, confidence:0.9976195096969604, loss:8.183305740356445
epoch3: step3000/4680
step 10500: accuracy:0.09700000286102295, confidence:0.9993627667427063, loss:9.489789962768555
epoch3: step3500/4680
step 12000: accuracy:0.11299999803304672, confidence:0.9957349300384521, loss:7.298182010650635
epoch3: step4000/4680
step 13500: accuracy:0.10100000351667404, confidence:0.9998177289962769, loss:10.592031478881836
epoch3: step4500/4680
step 0: accuracy:0.10700000077486038, confidence:0.9996151328086853, loss:9.634027481079102
epoch4: step0/4680
step 2000: accuracy:0.10999999940395355, confidence:0.9965358972549438, loss:8.044179916381836
epoch4: step500/4680
step 4000: accuracy:0.12099999934434891, confidence:0.98853600025177, loss:7.010524272918701
epoch4: step1000/4680
step 6000: accuracy:0.09000000357627869, confidence:0.9878419637680054, loss:6.887462615966797
epoch4: step1500/4680
step 8000: accuracy:0.10199999809265137, confidence:0.9975265264511108, loss:7.882425785064697
epoch4: step2000/4680
step 10000: accuracy:0.09799999743700027, confidence:0.9893847703933716, loss:6.652730941772461
epoch4: step2500/4680
step 12000: accuracy:0.09600000083446503, confidence:0.994288444519043, loss:7.775001525878906
epoch4: step3000/4680
step 14000: accuracy:0.1080000028014183, confidence:0.998647928237915, loss:8.6812162399292
epoch4: step3500/4680
step 16000: accuracy:0.09399999678134918, confidence:0.9828245043754578, loss:6.204777240753174
epoch4: step4000/4680
step 18000: accuracy:0.09799999743700027, confidence:0.9980887174606323, loss:8.364850044250488
epoch4: step4500/4680
step 0: accuracy:0.08699999749660492, confidence:0.9976027607917786, loss:8.119953155517578
epoch5: step0/4680
step 2500: accuracy:0.09799999743700027, confidence:0.9248450994491577, loss:5.320258617401123
epoch5: step500/4680
step 5000: accuracy:0.12600000202655792, confidence:0.9841147661209106, loss:6.68134069442749
epoch5: step1000/4680
step 7500: accuracy:0.10499999672174454, confidence:0.9948304295539856, loss:7.34173059463501
epoch5: step1500/4680
step 10000: accuracy:0.10300000011920929, confidence:0.9959483742713928, loss:7.435735702514648
epoch5: step2000/4680
step 12500: accuracy:0.10199999809265137, confidence:0.8940651416778564, loss:4.486401557922363
epoch5: step2500/4680
step 15000: accuracy:0.0860000029206276, confidence:0.975246250629425, loss:6.131045341491699
epoch5: step3000/4680
step 17500: accuracy:0.10400000214576721, confidence:0.9985302686691284, loss:8.731766700744629
epoch5: step3500/4680
step 20000: accuracy:0.09200000017881393, confidence:0.9926436543464661, loss:7.1380157470703125
epoch5: step4000/4680
step 22500: accuracy:0.10999999940395355, confidence:0.9923893213272095, loss:7.230793476104736
epoch5: step4500/4680
step 0: accuracy:0.1080000028014183, confidence:0.9928692579269409, loss:7.18779182434082
epoch6: step0/4680
step 3000: accuracy:0.07400000095367432, confidence:0.8772293329238892, loss:6.0138068199157715
epoch6: step500/4680
step 6000: accuracy:0.11599999666213989, confidence:0.9294354915618896, loss:5.84672737121582
epoch6: step1000/4680
step 9000: accuracy:0.11299999803304672, confidence:0.9977350831031799, loss:8.45272159576416
epoch6: step1500/4680
step 12000: accuracy:0.11500000208616257, confidence:0.999197781085968, loss:9.142277717590332
epoch6: step2000/4680
step 15000: accuracy:0.0949999988079071, confidence:0.9285855889320374, loss:4.917891502380371
epoch6: step2500/4680
step 18000: accuracy:0.09399999678134918, confidence:0.9292726516723633, loss:5.08302116394043
epoch6: step3000/4680
step 21000: accuracy:0.1080000028014183, confidence:0.9966372847557068, loss:8.03537654876709
epoch6: step3500/4680
step 24000: accuracy:0.10199999809265137, confidence:0.9622129201889038, loss:5.4627814292907715
epoch6: step4000/4680
step 27000: accuracy:0.10599999874830246, confidence:0.9971151351928711, loss:8.134225845336914
epoch6: step4500/4680
step 0: accuracy:0.10899999737739563, confidence:0.9961650371551514, loss:7.741701602935791
epoch7: step0/4680
step 3500: accuracy:0.08799999952316284, confidence:0.8907437324523926, loss:6.249549865722656
epoch7: step500/4680
step 7000: accuracy:0.10899999737739563, confidence:0.8994973301887512, loss:5.613858699798584
epoch7: step1000/4680
step 10500: accuracy:0.09200000017881393, confidence:0.9976235032081604, loss:8.928681373596191
epoch7: step1500/4680
step 14000: accuracy:0.10999999940395355, confidence:0.998927116394043, loss:9.03718090057373
epoch7: step2000/4680
step 17500: accuracy:0.08299999684095383, confidence:0.9358574151992798, loss:5.015915393829346
epoch7: step2500/4680
step 21000: accuracy:0.10899999737739563, confidence:0.7585330009460449, loss:4.020675182342529
epoch7: step3000/4680
step 24500: accuracy:0.11299999803304672, confidence:0.996177613735199, loss:7.8131914138793945
epoch7: step3500/4680
step 28000: accuracy:0.10300000011920929, confidence:0.9899266362190247, loss:6.566773414611816
epoch7: step4000/4680
step 31500: accuracy:0.0989999994635582, confidence:0.9979749321937561, loss:8.545984268188477
epoch7: step4500/4680
step 0: accuracy:0.09000000357627869, confidence:0.9974449276924133, loss:8.317102432250977
epoch8: step0/4680
step 4000: accuracy:0.05400000140070915, confidence:0.9228008985519409, loss:7.669208526611328
epoch8: step500/4680
step 8000: accuracy:0.10000000149011612, confidence:0.9045786261558533, loss:5.676534175872803
epoch8: step1000/4680
step 12000: accuracy:0.10000000149011612, confidence:0.993171751499176, loss:8.367176055908203
epoch8: step1500/4680
step 16000: accuracy:0.09200000017881393, confidence:0.9912766218185425, loss:7.486133098602295
epoch8: step2000/4680
step 20000: accuracy:0.07900000363588333, confidence:0.9569563865661621, loss:5.611598014831543
epoch8: step2500/4680
step 24000: accuracy:0.0820000022649765, confidence:0.758854866027832, loss:4.251551628112793
epoch8: step3000/4680
step 28000: accuracy:0.08699999749660492, confidence:0.9754355549812317, loss:6.46906042098999
epoch8: step3500/4680
step 32000: accuracy:0.09200000017881393, confidence:0.9291900396347046, loss:4.986311912536621
epoch8: step4000/4680
step 36000: accuracy:0.09000000357627869, confidence:0.9921616315841675, loss:7.858211517333984
epoch8: step4500/4680
step 0: accuracy:0.0989999994635582, confidence:0.9911707043647766, loss:7.478381633758545
epoch9: step0/4680
step 4500: accuracy:0.0860000029206276, confidence:0.9536479115486145, loss:8.060626029968262
epoch9: step500/4680
step 9000: accuracy:0.11800000071525574, confidence:0.8983030319213867, loss:5.44143533706665
epoch9: step1000/4680
step 13500: accuracy:0.09399999678134918, confidence:0.9639362692832947, loss:8.12282943725586
epoch9: step1500/4680
step 18000: accuracy:0.11699999868869781, confidence:0.9800029397010803, loss:6.692249298095703
epoch9: step2000/4680
step 22500: accuracy:0.09099999815225601, confidence:0.9865437150001526, loss:6.711593151092529
epoch9: step2500/4680
step 27000: accuracy:0.09300000220537186, confidence:0.7797095775604248, loss:4.440199375152588
epoch9: step3000/4680
step 31500: accuracy:0.10599999874830246, confidence:0.9705824255943298, loss:6.13190221786499
epoch9: step3500/4680
step 36000: accuracy:0.11800000071525574, confidence:0.8173746466636658, loss:4.14111852645874
epoch9: step4000/4680
step 40500: accuracy:0.1120000034570694, confidence:0.9869145750999451, loss:7.552157878875732
epoch9: step4500/4680
step 0: accuracy:0.10100000351667404, confidence:0.985156774520874, loss:7.455272674560547
epoch10: step0/4680
step 5000: accuracy:0.0560000017285347, confidence:0.9307719469070435, loss:8.773128509521484
epoch10: step500/4680
step 10000: accuracy:0.12200000137090683, confidence:0.8979291915893555, loss:5.901930809020996
epoch10: step1000/4680
step 15000: accuracy:0.11400000005960464, confidence:0.9793846607208252, loss:8.367921829223633
epoch10: step1500/4680
step 20000: accuracy:0.11500000208616257, confidence:0.9955311417579651, loss:8.16285228729248
epoch10: step2000/4680
step 25000: accuracy:0.09099999815225601, confidence:0.9773545861244202, loss:6.172397613525391
epoch10: step2500/4680
step 30000: accuracy:0.09799999743700027, confidence:0.7054765820503235, loss:4.088939666748047
epoch10: step3000/4680
step 35000: accuracy:0.1120000034570694, confidence:0.9768070578575134, loss:6.296896457672119
epoch10: step3500/4680
step 40000: accuracy:0.10400000214576721, confidence:0.7600473761558533, loss:4.075606822967529
epoch10: step4000/4680
step 45000: accuracy:0.09099999815225601, confidence:0.9706380367279053, loss:6.889669418334961
epoch10: step4500/4680
step 0: accuracy:0.11500000208616257, confidence:0.9680328369140625, loss:6.616535663604736
epoch11: step0/4680
step 5500: accuracy:0.039000000804662704, confidence:0.8451435565948486, loss:7.326284408569336
epoch11: step500/4680
step 11000: accuracy:0.10899999737739563, confidence:0.8849847912788391, loss:6.741073131561279
epoch11: step1000/4680
step 16500: accuracy:0.10700000077486038, confidence:0.926419198513031, loss:7.763284206390381
epoch11: step1500/4680
step 22000: accuracy:0.10400000214576721, confidence:0.9917536377906799, loss:8.031649589538574
epoch11: step2000/4680
step 27500: accuracy:0.0820000022649765, confidence:0.9936806559562683, loss:7.598753452301025
epoch11: step2500/4680
step 33000: accuracy:0.0989999994635582, confidence:0.6299759745597839, loss:3.8080697059631348
epoch11: step3000/4680
step 38500: accuracy:0.08399999886751175, confidence:0.9711669683456421, loss:6.4876251220703125
epoch11: step3500/4680
step 44000: accuracy:0.09099999815225601, confidence:0.7085368633270264, loss:3.8822031021118164
epoch11: step4000/4680
step 49500: accuracy:0.10499999672174454, confidence:0.974015474319458, loss:7.004116058349609
epoch11: step4500/4680
step 0: accuracy:0.10499999672174454, confidence:0.9708192944526672, loss:6.887457847595215
epoch12: step0/4680
step 6000: accuracy:0.08799999952316284, confidence:0.8545690774917603, loss:6.413743019104004
epoch12: step500/4680
step 12000: accuracy:0.12600000202655792, confidence:0.9175645709037781, loss:8.226938247680664
epoch12: step1000/4680
step 18000: accuracy:0.09600000083446503, confidence:0.9983405470848083, loss:10.82573413848877
epoch12: step1500/4680
step 24000: accuracy:0.09099999815225601, confidence:0.9805645942687988, loss:6.729443073272705
epoch12: step2000/4680
step 30000: accuracy:0.0989999994635582, confidence:0.9842657446861267, loss:6.836791515350342
epoch12: step2500/4680
step 36000: accuracy:0.11500000208616257, confidence:0.7377916574478149, loss:4.436675548553467
epoch12: step3000/4680
step 42000: accuracy:0.08900000154972076, confidence:0.9797016382217407, loss:6.942636013031006
epoch12: step3500/4680
step 48000: accuracy:0.11599999666213989, confidence:0.7042171359062195, loss:3.78082537651062
epoch12: step4000/4680
step 54000: accuracy:0.10199999809265137, confidence:0.9698578715324402, loss:6.899785041809082
epoch12: step4500/4680
step 0: accuracy:0.09600000083446503, confidence:0.9643704891204834, loss:6.824613094329834
epoch13: step0/4680
step 6500: accuracy:0.0860000029206276, confidence:0.7379966974258423, loss:5.425711154937744
epoch13: step500/4680
step 13000: accuracy:0.13099999725818634, confidence:0.8694183826446533, loss:7.686532497406006
epoch13: step1000/4680
step 19500: accuracy:0.09300000220537186, confidence:0.9878692030906677, loss:10.964771270751953
epoch13: step1500/4680
step 26000: accuracy:0.0989999994635582, confidence:0.9976826310157776, loss:8.718742370605469
epoch13: step2000/4680
step 32500: accuracy:0.0989999994635582, confidence:0.981935977935791, loss:6.683020114898682
epoch13: step2500/4680
step 39000: accuracy:0.13099999725818634, confidence:0.718019425868988, loss:4.272212028503418
epoch13: step3000/4680
step 45500: accuracy:0.09000000357627869, confidence:0.9853181838989258, loss:7.704880237579346
epoch13: step3500/4680
step 52000: accuracy:0.11699999868869781, confidence:0.7068759799003601, loss:3.8358287811279297
epoch13: step4000/4680
step 58500: accuracy:0.10100000351667404, confidence:0.9753747582435608, loss:7.346745491027832
epoch13: step4500/4680
step 0: accuracy:0.10899999737739563, confidence:0.9657353758811951, loss:6.939981937408447
epoch14: step0/4680
step 7000: accuracy:0.05900000035762787, confidence:0.7656068205833435, loss:5.334784984588623
epoch14: step500/4680
step 14000: accuracy:0.11299999803304672, confidence:0.9301029443740845, loss:9.039473533630371
epoch14: step1000/4680
step 21000: accuracy:0.09399999678134918, confidence:0.999055802822113, loss:12.104552268981934
epoch14: step1500/4680
step 28000: accuracy:0.11699999868869781, confidence:0.9883447885513306, loss:7.502993106842041
epoch14: step2000/4680
step 35000: accuracy:0.10599999874830246, confidence:0.9806340932846069, loss:6.924446105957031
epoch14: step2500/4680
step 42000: accuracy:0.12099999934434891, confidence:0.6867340803146362, loss:4.2100958824157715
epoch14: step3000/4680
step 49000: accuracy:0.11500000208616257, confidence:0.9766342639923096, loss:7.035238265991211
epoch14: step3500/4680
step 56000: accuracy:0.0989999994635582, confidence:0.7150202989578247, loss:4.046660423278809
epoch14: step4000/4680
step 63000: accuracy:0.09300000220537186, confidence:0.9572666883468628, loss:7.12606143951416
epoch14: step4500/4680
step 0: accuracy:0.11299999803304672, confidence:0.9558230042457581, loss:6.603665351867676
epoch15: step0/4680
step 7500: accuracy:0.07400000095367432, confidence:0.7106706500053406, loss:5.296388626098633
epoch15: step500/4680
step 15000: accuracy:0.12999999523162842, confidence:0.8629293441772461, loss:6.812349796295166
epoch15: step1000/4680
step 22500: accuracy:0.09700000286102295, confidence:0.9794468283653259, loss:10.873479843139648
epoch15: step1500/4680
step 30000: accuracy:0.09000000357627869, confidence:0.994215726852417, loss:8.297663688659668
epoch15: step2000/4680
step 37500: accuracy:0.09799999743700027, confidence:0.9939708113670349, loss:7.92860221862793
epoch15: step2500/4680
step 45000: accuracy:0.10400000214576721, confidence:0.7309499979019165, loss:4.469535827636719
epoch15: step3000/4680
step 52500: accuracy:0.10700000077486038, confidence:0.9597069025039673, loss:6.632721900939941
epoch15: step3500/4680
step 60000: accuracy:0.1289999932050705, confidence:0.6826175451278687, loss:4.1136579513549805
epoch15: step4000/4680
step 67500: accuracy:0.10300000011920929, confidence:0.9812130331993103, loss:7.936561584472656
epoch15: step4500/4680
step 0: accuracy:0.11800000071525574, confidence:0.9802830815315247, loss:7.549572944641113
epoch16: step0/4680
step 8000: accuracy:0.07400000095367432, confidence:0.7166627049446106, loss:5.145113945007324
epoch16: step500/4680
step 16000: accuracy:0.10599999874830246, confidence:0.8728306889533997, loss:7.615018367767334
epoch16: step1000/4680
step 24000: accuracy:0.0989999994635582, confidence:0.9969309568405151, loss:10.972013473510742
epoch16: step1500/4680
step 32000: accuracy:0.08799999952316284, confidence:0.9841196537017822, loss:7.6476054191589355
epoch16: step2000/4680
step 40000: accuracy:0.10899999737739563, confidence:0.9886836409568787, loss:7.548022270202637
epoch16: step2500/4680
step 48000: accuracy:0.1120000034570694, confidence:0.748403787612915, loss:4.681744575500488
epoch16: step3000/4680
step 56000: accuracy:0.10100000351667404, confidence:0.9829584956169128, loss:7.661665916442871
epoch16: step3500/4680
step 64000: accuracy:0.11699999868869781, confidence:0.6956759095191956, loss:4.01399564743042
epoch16: step4000/4680
step 72000: accuracy:0.10100000351667404, confidence:0.9611412882804871, loss:7.392782688140869
epoch16: step4500/4680
step 0: accuracy:0.08500000089406967, confidence:0.9612752795219421, loss:7.292013645172119
epoch17: step0/4680
step 8500: accuracy:0.05000000074505806, confidence:0.6775803565979004, loss:5.0753254890441895
epoch17: step500/4680
step 17000: accuracy:0.13699999451637268, confidence:0.8593629002571106, loss:6.836540222167969
epoch17: step1000/4680
step 25500: accuracy:0.10000000149011612, confidence:0.9929758906364441, loss:11.07846450805664
epoch17: step1500/4680
step 34000: accuracy:0.09399999678134918, confidence:0.9908344745635986, loss:8.032658576965332
epoch17: step2000/4680
step 42500: accuracy:0.09000000357627869, confidence:0.9849318861961365, loss:7.4260945320129395
epoch17: step2500/4680
step 51000: accuracy:0.10199999809265137, confidence:0.7277026772499084, loss:4.777263641357422
epoch17: step3000/4680
step 59500: accuracy:0.11599999666213989, confidence:0.9884597659111023, loss:8.022931098937988
epoch17: step3500/4680
step 68000: accuracy:0.1459999978542328, confidence:0.7400445342063904, loss:4.357367992401123
epoch17: step4000/4680
step 76500: accuracy:0.10199999809265137, confidence:0.9650223255157471, loss:7.392552375793457
epoch17: step4500/4680
step 0: accuracy:0.11999999731779099, confidence:0.9592156410217285, loss:7.268720626831055
epoch18: step0/4680
step 9000: accuracy:0.05700000002980232, confidence:0.7311276793479919, loss:5.507867336273193
epoch18: step500/4680
step 18000: accuracy:0.10999999940395355, confidence:0.898950457572937, loss:8.04699993133545
epoch18: step1000/4680
step 27000: accuracy:0.1120000034570694, confidence:0.9976809620857239, loss:10.909586906433105
epoch18: step1500/4680
step 36000: accuracy:0.11100000143051147, confidence:0.9863674640655518, loss:8.207517623901367
epoch18: step2000/4680
step 45000: accuracy:0.11500000208616257, confidence:0.9676397442817688, loss:6.565606594085693
epoch18: step2500/4680
step 54000: accuracy:0.0949999988079071, confidence:0.6820357441902161, loss:4.456991195678711
epoch18: step3000/4680
step 63000: accuracy:0.10300000011920929, confidence:0.9727407693862915, loss:7.432433128356934
epoch18: step3500/4680
step 72000: accuracy:0.1509999930858612, confidence:0.677490234375, loss:4.014432907104492
epoch18: step4000/4680
step 81000: accuracy:0.10999999940395355, confidence:0.9688678979873657, loss:7.674947261810303
epoch18: step4500/4680
step 0: accuracy:0.11100000143051147, confidence:0.9677027463912964, loss:7.293504238128662
epoch19: step0/4680
step 9500: accuracy:0.0729999989271164, confidence:0.6887924075126648, loss:5.0634284019470215
epoch19: step500/4680
step 19000: accuracy:0.1289999932050705, confidence:0.8836098313331604, loss:7.378612518310547
epoch19: step1000/4680
step 28500: accuracy:0.0989999994635582, confidence:0.9990195631980896, loss:12.849456787109375
epoch19: step1500/4680
step 38000: accuracy:0.10000000149011612, confidence:0.9754314422607422, loss:7.4029035568237305
epoch19: step2000/4680
step 47500: accuracy:0.11699999868869781, confidence:0.9523364901542664, loss:6.41012716293335
epoch19: step2500/4680
step 57000: accuracy:0.11900000274181366, confidence:0.724428653717041, loss:4.58627986907959
epoch19: step3000/4680
step 66500: accuracy:0.08900000154972076, confidence:0.9701358079910278, loss:7.493220329284668
epoch19: step3500/4680
step 76000: accuracy:0.12999999523162842, confidence:0.7008124589920044, loss:4.407419204711914
epoch19: step4000/4680
step 85500: accuracy:0.10400000214576721, confidence:0.9733631610870361, loss:8.00339412689209
epoch19: step4500/4680
step 0: accuracy:0.10999999940395355, confidence:0.9789073467254639, loss:7.869359493255615
epoch20: step0/4680
step 10000: accuracy:0.09000000357627869, confidence:0.7149649262428284, loss:5.301383018493652
epoch20: step500/4680
step 20000: accuracy:0.11699999868869781, confidence:0.8848568201065063, loss:7.405045032501221
epoch20: step1000/4680
step 30000: accuracy:0.08500000089406967, confidence:0.9891320466995239, loss:10.014261245727539
epoch20: step1500/4680
step 40000: accuracy:0.07000000029802322, confidence:0.9797171354293823, loss:8.230289459228516
epoch20: step2000/4680
step 50000: accuracy:0.09600000083446503, confidence:0.9501232504844666, loss:6.480503082275391
epoch20: step2500/4680
step 60000: accuracy:0.13600000739097595, confidence:0.7023807168006897, loss:4.582943916320801
epoch20: step3000/4680
step 70000: accuracy:0.10700000077486038, confidence:0.9758133292198181, loss:7.732478618621826
epoch20: step3500/4680
step 80000: accuracy:0.125, confidence:0.6813076138496399, loss:4.270318031311035
epoch20: step4000/4680
step 90000: accuracy:0.09799999743700027, confidence:0.9615049362182617, loss:7.672319412231445
epoch20: step4500/4680
step 0: accuracy:0.10000000149011612, confidence:0.960092306137085, loss:7.516044616699219
epoch21: step0/4680
step 10500: accuracy:0.06700000166893005, confidence:0.712718665599823, loss:5.477004051208496
epoch21: step500/4680
step 21000: accuracy:0.11900000274181366, confidence:0.8898171186447144, loss:7.778783321380615
epoch21: step1000/4680
step 31500: accuracy:0.09200000017881393, confidence:0.9947531223297119, loss:11.044510841369629
epoch21: step1500/4680
step 42000: accuracy:0.10899999737739563, confidence:0.965893030166626, loss:7.297501087188721
epoch21: step2000/4680
step 52500: accuracy:0.10000000149011612, confidence:0.9697861671447754, loss:7.352095127105713
epoch21: step2500/4680
step 63000: accuracy:0.12700000405311584, confidence:0.7371523976325989, loss:4.791395664215088
epoch21: step3000/4680
step 73500: accuracy:0.10000000149011612, confidence:0.9791941046714783, loss:8.195516586303711
epoch21: step3500/4680
step 84000: accuracy:0.1289999932050705, confidence:0.7162779569625854, loss:4.627540588378906
epoch21: step4000/4680
step 94500: accuracy:0.09700000286102295, confidence:0.9674598574638367, loss:8.078639030456543
epoch21: step4500/4680
step 0: accuracy:0.11699999868869781, confidence:0.9602634310722351, loss:7.611608505249023
epoch22: step0/4680
step 11000: accuracy:0.08500000089406967, confidence:0.7005661129951477, loss:5.065245151519775
epoch22: step500/4680
step 22000: accuracy:0.10999999940395355, confidence:0.8979715704917908, loss:8.162262916564941
epoch22: step1000/4680
step 33000: accuracy:0.09300000220537186, confidence:0.9965870380401611, loss:11.108040809631348
epoch22: step1500/4680
step 44000: accuracy:0.1120000034570694, confidence:0.9839279651641846, loss:8.40839958190918
epoch22: step2000/4680
step 55000: accuracy:0.10400000214576721, confidence:0.9545422792434692, loss:6.8860554695129395
epoch22: step2500/4680
step 66000: accuracy:0.12200000137090683, confidence:0.6997359991073608, loss:4.611515522003174
epoch22: step3000/4680
step 77000: accuracy:0.10499999672174454, confidence:0.9850343465805054, loss:8.73988151550293
epoch22: step3500/4680
step 88000: accuracy:0.14000000059604645, confidence:0.7206593751907349, loss:4.747145175933838
epoch22: step4000/4680
step 99000: accuracy:0.10999999940395355, confidence:0.9563922882080078, loss:7.459700584411621
epoch22: step4500/4680
step 0: accuracy:0.11800000071525574, confidence:0.9604665637016296, loss:7.383909702301025
epoch23: step0/4680
step 11500: accuracy:0.08299999684095383, confidence:0.7226589918136597, loss:5.569955348968506
epoch23: step500/4680
step 23000: accuracy:0.13600000739097595, confidence:0.9075434803962708, loss:8.379392623901367
epoch23: step1000/4680
step 34500: accuracy:0.08900000154972076, confidence:0.9962910413742065, loss:11.5993070602417
epoch23: step1500/4680
step 46000: accuracy:0.10499999672174454, confidence:0.9846357107162476, loss:8.787039756774902
epoch23: step2000/4680
step 57500: accuracy:0.09799999743700027, confidence:0.9771062135696411, loss:7.694150924682617
epoch23: step2500/4680
step 69000: accuracy:0.10000000149011612, confidence:0.7118972539901733, loss:4.731983661651611
epoch23: step3000/4680
step 80500: accuracy:0.10199999809265137, confidence:0.983681857585907, loss:8.556995391845703
epoch23: step3500/4680
step 92000: accuracy:0.1509999930858612, confidence:0.7533353567123413, loss:5.071601390838623
epoch23: step4000/4680
step 103500: accuracy:0.10400000214576721, confidence:0.9667620658874512, loss:8.063024520874023
epoch23: step4500/4680
step 0: accuracy:0.09799999743700027, confidence:0.9711448550224304, loss:8.022683143615723
epoch24: step0/4680
step 12000: accuracy:0.05299999937415123, confidence:0.721983790397644, loss:5.6971635818481445
epoch24: step500/4680
step 24000: accuracy:0.12399999797344208, confidence:0.9000828266143799, loss:8.590459823608398
epoch24: step1000/4680
step 36000: accuracy:0.10999999940395355, confidence:0.9956634640693665, loss:10.773785591125488
epoch24: step1500/4680
step 48000: accuracy:0.10400000214576721, confidence:0.956479012966156, loss:7.5519537925720215
epoch24: step2000/4680
step 60000: accuracy:0.07800000160932541, confidence:0.9502175450325012, loss:7.066746711730957
epoch24: step2500/4680
step 72000: accuracy:0.11500000208616257, confidence:0.6901692748069763, loss:4.660880088806152
epoch24: step3000/4680
step 84000: accuracy:0.09799999743700027, confidence:0.9798687696456909, loss:8.880992889404297
epoch24: step3500/4680
step 96000: accuracy:0.15700000524520874, confidence:0.7366423606872559, loss:4.889287948608398
epoch24: step4000/4680
step 108000: accuracy:0.10599999874830246, confidence:0.9663405418395996, loss:7.905729293823242
epoch24: step4500/4680
step 0: accuracy:0.10999999940395355, confidence:0.9589388370513916, loss:7.751430034637451
epoch25: step0/4680
step 12500: accuracy:0.0820000022649765, confidence:0.7015262842178345, loss:5.135400772094727
epoch25: step500/4680
step 25000: accuracy:0.11400000005960464, confidence:0.898230791091919, loss:8.169751167297363
epoch25: step1000/4680
step 37500: accuracy:0.10599999874830246, confidence:0.9975336790084839, loss:11.837740898132324
epoch25: step1500/4680
step 50000: accuracy:0.10000000149011612, confidence:0.9711529016494751, loss:8.065332412719727
epoch25: step2000/4680
step 62500: accuracy:0.09200000017881393, confidence:0.9441391229629517, loss:7.140270709991455
epoch25: step2500/4680
step 75000: accuracy:0.09799999743700027, confidence:0.719833493232727, loss:4.886434555053711
epoch25: step3000/4680
step 87500: accuracy:0.0949999988079071, confidence:0.9801101088523865, loss:9.002784729003906
epoch25: step3500/4680
step 100000: accuracy:0.13500000536441803, confidence:0.748664915561676, loss:5.376321315765381
epoch25: step4000/4680
step 112500: accuracy:0.10700000077486038, confidence:0.9628031253814697, loss:8.138680458068848
epoch25: step4500/4680
step 0: accuracy:0.10599999874830246, confidence:0.9616709351539612, loss:8.009966850280762
epoch26: step0/4680
step 13000: accuracy:0.0689999982714653, confidence:0.7040885090827942, loss:5.253588676452637
epoch26: step500/4680
step 26000: accuracy:0.11999999731779099, confidence:0.9023558497428894, loss:8.646565437316895
epoch26: step1000/4680
step 39000: accuracy:0.10400000214576721, confidence:0.9947212934494019, loss:10.631746292114258
epoch26: step1500/4680
step 52000: accuracy:0.10300000011920929, confidence:0.9759684801101685, loss:7.985213756561279
epoch26: step2000/4680
step 65000: accuracy:0.08799999952316284, confidence:0.9312387704849243, loss:6.935703277587891
epoch26: step2500/4680
step 78000: accuracy:0.10199999809265137, confidence:0.7142684459686279, loss:4.769242286682129
epoch26: step3000/4680
step 91000: accuracy:0.09000000357627869, confidence:0.9795845150947571, loss:9.064532279968262
epoch26: step3500/4680
step 104000: accuracy:0.14900000393390656, confidence:0.75426185131073, loss:5.366534233093262
epoch26: step4000/4680
step 117000: accuracy:0.10000000149011612, confidence:0.971245288848877, loss:8.248652458190918
epoch26: step4500/4680
step 0: accuracy:0.1289999932050705, confidence:0.9695718884468079, loss:7.888577461242676
epoch27: step0/4680
step 13500: accuracy:0.09099999815225601, confidence:0.7499107718467712, loss:5.631822109222412
epoch27: step500/4680
step 27000: accuracy:0.11500000208616257, confidence:0.908869206905365, loss:8.241362571716309
epoch27: step1000/4680
step 40500: accuracy:0.0989999994635582, confidence:0.9959918856620789, loss:11.085392951965332
epoch27: step1500/4680
step 54000: accuracy:0.11599999666213989, confidence:0.9854013919830322, loss:9.212102890014648
epoch27: step2000/4680
step 67500: accuracy:0.0949999988079071, confidence:0.9235442876815796, loss:6.858809471130371
epoch27: step2500/4680
step 81000: accuracy:0.11999999731779099, confidence:0.7224661111831665, loss:4.838812351226807
epoch27: step3000/4680
step 94500: accuracy:0.1080000028014183, confidence:0.9840379357337952, loss:9.474709510803223
epoch27: step3500/4680
step 108000: accuracy:0.16500000655651093, confidence:0.771052360534668, loss:5.326313495635986
epoch27: step4000/4680
step 121500: accuracy:0.10999999940395355, confidence:0.9721258282661438, loss:8.442840576171875
epoch27: step4500/4680
step 0: accuracy:0.10599999874830246, confidence:0.9710813164710999, loss:8.315659523010254
epoch28: step0/4680
step 14000: accuracy:0.09399999678134918, confidence:0.7389079928398132, loss:5.533468246459961
epoch28: step500/4680
step 28000: accuracy:0.1080000028014183, confidence:0.9201995134353638, loss:9.263256072998047
epoch28: step1000/4680
step 42000: accuracy:0.09399999678134918, confidence:0.9953039288520813, loss:10.642468452453613
epoch28: step1500/4680
step 56000: accuracy:0.0860000029206276, confidence:0.9753298163414001, loss:9.008533477783203
epoch28: step2000/4680
step 70000: accuracy:0.10599999874830246, confidence:0.9171172976493835, loss:6.653103828430176
epoch28: step2500/4680
step 84000: accuracy:0.11100000143051147, confidence:0.7464944124221802, loss:4.954588890075684
epoch28: step3000/4680
step 98000: accuracy:0.09099999815225601, confidence:0.9714374542236328, loss:8.9456148147583
epoch28: step3500/4680
step 112000: accuracy:0.164000004529953, confidence:0.7435458302497864, loss:5.202791690826416
epoch28: step4000/4680
step 126000: accuracy:0.10100000351667404, confidence:0.9627238512039185, loss:8.131040573120117
epoch28: step4500/4680
step 0: accuracy:0.0989999994635582, confidence:0.9555116891860962, loss:7.936939716339111
epoch29: step0/4680
step 14500: accuracy:0.08399999886751175, confidence:0.7463856935501099, loss:5.802940368652344
epoch29: step500/4680
step 29000: accuracy:0.1340000033378601, confidence:0.8991940021514893, loss:8.127490997314453
epoch29: step1000/4680
step 43500: accuracy:0.10000000149011612, confidence:0.9988771677017212, loss:12.90187931060791
epoch29: step1500/4680
step 58000: accuracy:0.12200000137090683, confidence:0.9873771071434021, loss:9.087071418762207
epoch29: step2000/4680
step 72500: accuracy:0.10199999809265137, confidence:0.9273635745048523, loss:7.137260913848877
epoch29: step2500/4680
step 87000: accuracy:0.10999999940395355, confidence:0.7575660347938538, loss:5.178365230560303
epoch29: step3000/4680
step 101500: accuracy:0.10000000149011612, confidence:0.9679263830184937, loss:8.845362663269043
epoch29: step3500/4680
step 116000: accuracy:0.13300000131130219, confidence:0.8302977681159973, loss:6.859979629516602
epoch29: step4000/4680
step 130500: accuracy:0.10499999672174454, confidence:0.9717042446136475, loss:8.715225219726562
epoch29: step4500/4680
